<!-- ****************************************************************************************
  Author: Christian Wolf
  Begin: 19.4.2005
  ******************************************************************************************* -->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
	<title>Christian Wolf's Publications</title>
  	<link rel=stylesheet href="../main.css" type="text/css">
	<link rel=stylesheet href="publications.css" type="text/css">
	<script type="text/javascript" language="JavaScript" src="publications.js"></script>
	<link rel="shortcut icon" href="../graphics/favicon.ico">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
</head>

<body  onload="startPage()">


<!-- ****************************************************************************************
  The nav menu box
  ******************************************************************************************* -->

<div class="navmenunew">
	<ul>
		<li class="first"><a href="../index.html">Main</a></li>
		<li> <a href="../teaching/index.html">Teaching</a></li>
		<li> <a href="../publications/index_bydate.html">Publications</a></li>
		<li> <a href="../publications/pres.html">Talks</a></li>        
	</ul>
</div>

<h1>Publications</h1>


<div class="navmenunew">
	<ul>
	<li class="first" style="color: gray;">By Type</li>
	<li><a href="index_bydate.html">by Date</a></li>
	<li><a href="http://dblp.uni-trier.de/pers/hd/w/Wolf_0001:Christian">DBLP</a> </li>
	<li><a href="http://scholar.google.com/citations?user=idYS1AIAAAAJ&hl=en">Google citations page</a></li>
	</ul>
</div>

<!-- ****************************************************************************************
  ARXIV
  ******************************************************************************************* -->



<h2>ArXiv pre-prints (a subset of papers currently under review)</h2>

<dl>

	<dl>
	
  
	<dt><img src="../graphics/bib_arxiv.gif"></dt>
	<dd id="arxiv2022iros" class="bib-arxive">
		<span class="t-author">						
			<a href="https://fr.linkedin.com/in/quentin-possama%C3%AF-425420142">Quentin Possamaï<a>,
			<a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny<a>,
			<a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri<a>,
			<a href="https://sites.google.com/site/laurentbako">Laurent Bako<a>,
      and 
      <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Learning to estimate UAV created turbulence from scene structure observed by onboard cameras.
		</span>
		<span class="t-medium">
			pre-print arxiv:2203.14726, 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2022iros');">
			Abstract</a>
			</li>												
			<li><a href="https://arxiv.org/abs/2203.14726">ArXiv-Link</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			Controlling UAV flights precisely requires a realistic dynamic model and accurate state estimates from onboard sensors like UAV, GPS and visual observations. Obtaining a precise dynamic model is extremely difficult, as important aerodynamic effects are hard to model, in particular ground effect and other turbulences. While machine learning has been used in the past to estimate UAV created turbulence, this was restricted to flat grounds or diffuse in-flight air turbulences, both without taking into account obstacles. In this work we address the complex problem of estimating in-flight turbulences caused by obstacles, in particular the complex structures in cluttered environments. We learn a mapping from control input and images captured by onboard cameras to turbulence. In a large-scale setting, we train a model over a large number of different simulated photo-realistic environments loaded into the this simulator augmented with a dynamic UAV model and an analytic ground effect model. We transfer the model from simulation to a real environment and evaluate on real UAV flights from the EuRoC-MAV dataset, showing that the model is capable of good sim2real generalization performance. The dataset will be made publicly available upon acceptance.
		    </p>
		</div>
  </dd>
  
	<dt><img src="../graphics/bib_arxiv.gif"></dt>
	<dd id="arxiv2022vqabottleneck" class="bib-arxive">
		<span class="t-author">
			<a href="https://scholar.google.fr/citations?user=NAI5mi4AAAAJ&hl=fr">Pierre Marza</a>,
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec</a>,
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a> and
      <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    An experimental study of the vision-bottleneck in VQA.
		</span>
		<span class="t-medium">
			pre-print arxiv:2202.06858, 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2022vqabottleneck');">
			Abstract</a>
			</li>												
			<li><a href="https://arxiv.org/abs/2202.06858">ArXiv-Link</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			As in many tasks combining vision and language, both modalities play a crucial role in Visual Question Answering (VQA). To properly solve the task, a given model should both understand the content of the proposed image and the nature of the question. While the fusion between modalities, which is another obviously important part of the problem, has been highly studied, the vision part has received less attention in recent work. Current state-of-the-art methods for VQA mainly rely on off-the-shelf object detectors delivering a set of object bounding boxes and embeddings, which are then combined with question word embeddings through a reasoning module. In this paper, we propose an in-depth study of the vision-bottleneck in VQA, experimenting with both the quantity and quality of visual objects extracted from images. We also study the impact of two methods to incorporate the information about objects necessary for answering a question, in the reasoning module directly, and earlier in the object selection stage. This work highlights the importance of vision in the context of VQA, and the interest of tailoring vision methods used in VQA to the task at hand.
		  </p>
		</div>
  </dd>

	<dt><img src="../graphics/bib_arxiv.gif"></dt>
	<dd id="arxiv2022vice" class="bib-arxive">
		<span class="t-author">						
			<a href="https://fr.linkedin.com/in/quentin-possama%C3%AF-425420142">Quentin Possamaï<a>,
			<a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny<a>,
			<a href="https://fr.linkedin.com/in/guillaume-bono-4ba97767">Guillaume Bono<a>,	
			<a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri<a>,
			<a href="https://sites.google.com/site/laurentbako">Laurent Bako<a>,
      and 
      <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    MoCap-less Quantitative Evaluation of Ego-Pose Estimation Without Ground Truth Measurements.
		</span>
		<span class="t-medium">
			pre-print arxiv:2202.00403, 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2022vice');">
			Abstract</a>
			</li>												
			<li><a href="https://arxiv.org/abs/2202.00403">ArXiv-Link</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			The emergence of data-driven approaches for control and planning in robotics have highlighted the need for developing experimental robotic platforms for data collection. However, their implementation is often complex and expensive, in particular for flying and terrestrial robots where the precise estimation of the position requires motion capture devices (MoCap) or Lidar. In order to simplify the use of a robotic platform dedicated to research on a wide range of indoor and outdoor environments, we present a data validation tool for ego-pose estimation that does not require any equipment other than the on-board camera. The method and tool allow a rapid, visual and quantitative evaluation of the quality of ego-pose sensors and are sensitive to different sources of flaws in the acquisition chain, ranging from desynchronization of the sensor flows to misevaluation of the geometric parameters of the robotic platform. Using computer vision, the information from the sensors is used to calculate the motion of a semantic scene point through its projection to the 2D image space of the on-board camera. The deviations of these keypoints from references created with a semi-automatic tool allow rapid and simple quality assessment of the data collected on the platform. To demonstrate the performance of our method, we evaluate it on two challenging standard UAV datasets as well as one dataset taken from a terrestrial robot.
		    </p>
		</div>
  </dd>
	
  	
	<dt><img src="../graphics/bib_arxiv.gif"></dt>
	<dd id="arxiv2021uda" class="bib-arxive">
		<span class="t-author">			
			<a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii<a>,
			<a href="https://www.assemsadek.com">Assem Sadek<a> and
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Universal Domain Adaptation in Ordinal Regression.
		</span>
		<span class="t-medium">
			pre-print arXiv:2106.11576, 2021.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2021uda');">
			Abstract</a>
			</li>												
			<li><a href="https://arxiv.org/abs/2106.11576">ArXiv-Link</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			We address the problem of universal domain adaptation (UDA) in ordinal regression (OR), which attempts to solve classification problems in which labels are not independent, but follow a natural order. We show that the UDA techniques developed for classification and based on the clustering assumption, under-perform in OR settings. We propose a method that complements the OR classifier with an auxiliary task of order learning, which plays the double role of discriminating between common and private instances, and expanding class labels to the private target images via ranking. Combined with adversarial domain discrimination, our model is able to address the closed set, partial and open set configurations. We evaluate our method on three face age estimation datasets, and show that it outperforms the baseline methods.
		    </p>
		</div>
  	</dd>

  	<dt><img src="../graphics/bib_arxiv.gif"></dt>
	<dd id="arxiv2020semantic" class="bib-arxive">
		<span class="t-author">
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec</a>,
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a>,
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Estimating semantic structure for the VQA answer space.
		</span>
		<span class="t-medium">
			pre-print arXiv:2006.05726, 2020.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2020semantic');">
			Abstract</a>
			</li>			
			<li><a href="https://arxiv.org/abs/2006.05726">Arxiv-Version</a></li>	
      		</ul>
		<div class="t-abstract">
			<p>
			Since its appearance, Visual Question Answering (VQA, i.e. answering a question posed over an image), has always been treated as a classification problem over a set of predefined answers. Despite its convenience, this classification approach poorly reflects the semantics of the problem limiting the answering to a choice between independent proposals, without taking into account the similarity between them (e.g. equally penalizing for answering \say{cat} or \say{German shepherd} instead of \say{dog}). We address this issue by proposing (1) two measures of proximity between VQA classes, and (2) a corresponding loss which takes into account the estimated proximity. This significantly improves the generalization of VQA models by reducing their language bias. In particular, we show that our approach is completely model-agnostic since it allows consistent improvements with three different VQA models. Finally, by combining our method with a language bias reduction approach, we report SOTA-level performance on the challenging VQAv2-CP dataset.
		    </p>
		</div>
  	</dd>

</dl>



<!-- ****************************************************************************************
  BOOK CHAPTERS  ******************************************************************************************* -->

<h2>Book Chapters</h2>

<dl>

  <dt class="journal">BC1</dt>
	<dd id="bc2019" class="bib-journal">
		<span class="t-author">
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a>,
			<a href="https://scholar.google.fr/citations?user=GhnwS48AAAAJ&hl=fr">Alaeddine Mihoub</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~frederic.elisei/">Fréderic Elisei</a>.
	  	</span>
	  	<span class="t-title">
		    Gaze and face-to-face interaction: from multimodal data to behavioral models.
		</span>
		<span class="t-medium">
			Book chapter in "Eye-tracking in Interaction: Studies on the role of eye gaze in dialogue (Advances in Interaction Studies) ", Geert Brône & Bert Oben, ed., 2018.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('bc2019');">
			Abstract</a>
			</li>
          <li> <a href="../papers/bchapter_intstudies2018.pdf">PDF (Same content)</a></li>
          <li><a href="https://benjamins.com/catalog/ais.10">Publisher-Link</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			This chapter describes experimental and modeling work aiming at describing gaze patterns that are mutually exchanged by interlocutors during situated and task-directed face-to-face two-ways interactions. We will show that these gaze patterns (incl. blinking rate) are significantly influenced by the cognitive states of the interlocutors (speaking, listening, thinking, etc.), their respective roles in the conversation (e.g. instruction giver, respondent) as well as their social relationship (e.g. colleague, supervisor).
		   </p><p>
			This chapter provides insights into the (micro-)coordination of gaze with other components of attention management as well as methodologies for capturing and modeling behavioral regularities observed in experimental data. A particular emphasis is put on statistical models, which are able to learn behaviors in a data-driven way.
			</p><p>
			We will introduce several statistical models of multimodal behaviors that can be trained on such multimodal signals and generate behaviors given perceptual cues. We will notably compare performances and properties of models which explicitly model the temporal structure of studied signals, and which relate them to internal cognitive states. In particular we study Semi-Hidden Markov Models and Dynamic Bayesian Networks and compare them to classifiers without sequential models (Support Vector Machines and Decision Trees).
			</p><p>
			We will further show that the gaze of conversational agents (virtual talking heads, speaking robots) may have a strong impact on communication efficiency. One of the conclusions we draw from these experiments is that multimodal behavioral models able to generate co-verbal gaze patterns should be designed with great care in order not to increase cognitive load. Experiments involving an impoverished or irrelevant control of the gaze of artificial agents (virtual talking heads and humanoid robots) have demonstrated its negative impact on communication (Garau, Slater, Bee, & Sasse, 2001).
		    </p>
		</div>
  	</dd>
</dl>

<!-- ****************************************************************************************
  INTERNATIONAL JOURNALS
  ******************************************************************************************* -->

<h2>Articles in international journals</h2>

<dl>
	<dt class="journal">J24</dt>
	<dd id="vis2021visqa" class="bib-journal">
		<span class="t-author">
			<a href="https://theo-jaunet.github.io/">Théo Jaunet<a>,			
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec</a>,
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a>,
			<a href="https://romain.vuillemot.net/">Romain Vuillemot<a> and
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    VisQA: X-raying Vision and Language Reasoning in Transformers.
		</span>
		<span class="t-medium">
			In IEEE Transactions on Visualization and Computer Graphics (Proceedings of VIS 2021).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('vis2021visqa');">
			Abstract</a>
			</li>						
			<li><a href="https://arxiv.org/abs/2104.00926">ArXiv-Link</a></li>	
			<li><a href="https://visqa.liris.cnrs.fr">Interactive Visualization tool</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models -- attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a direct impact on the design and training of a neural model for VQA, improving model performance as a consequence. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.
		    </p>
		</div>
  	</dd>

	<dt class="journal">J23</dt>
	<dd id="eurovis2020" class="bib-journal">
		<span class="t-author">
			<a href="https://theo-jaunet.github.io/">Théo Jaunet<a>,
			<a href="https://romain.vuillemot.net/">Romain Vuillemot<a> and
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    DRLViz: Understanding Decisions and Memory in Deep Reinforcement Learning.
		</span>
		<span class="t-medium">
			In Computer Graphics Forum (Proceedings of Eurovis), 2020.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('eurovis2020');">
			Abstract</a>
			</li>
			<li><a href="https://arxiv.org/abs/1909.02982">Arxiv-Version</a></li>
			<li><a href="https://theo-jaunet.github.io/DRLViz/">Project Page</a></li>
			<li><a href="https://sical.github.io/drlviz/">Demo</a></li>	
      		</ul>
		<div class="t-abstract">
			<p>
			We present DRLViz, a visual analytics interface to interpret the internal memory of an agent (e.g. a robot) trained using deep reinforcement learning. This memory is composed of large temporal vectors updated when the agent moves in an environment and is not trivial to understand. It is often referred to as a black box as only inputs (images) and outputs (actions) are intelligible for humans. Using DRLViz, experts are assisted to interpret using memory reduction interactions, to investigate parts of the memory role when errors have been made, and ultimately to improve the agent training process. We report on several examples of use of DRLViz, in the context of video games simulators (ViZDoom) for a navigation scenario with item gathering tasks. We also report on experts evaluation using DRLViz, and applicability of DRLViz to other scenarios and navigation problems beyond simulation games, as well as its contribution to black box models interpret-ability and explain-ability in the field of visual analytics.
		    </p>
		</div>
  	</dd>
  	
	<dt class="journal">J22</dt>
	<dd id="tectonics2019" class="bib-journal">
		<span class="t-author">
			<a href="https://fr.linkedin.com/in/tgillooly">Tom Gillooly</a>, 
			<a href="https://ens.academia.edu/NicolasColtice">Nicolas Coltice</a> and
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    An anticipation experiment for plate tectonics.
		</span>
		<span class="t-medium">
			In "Tectonics", 38(11):3916-3938, 2019.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('tectonics2019');">
			Abstract</a>
			</li>
			<li>
			<a href="https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018TC005427">Publisher-link</a>
		</li>
      		</ul>
		<div class="t-abstract">
			<p>
			Although plate tectonics has pushed the frontiers of geosciences in the past 50 years, it has legitimate limitations and among them we focus on both the absence of dynamics in the theory, and the difficulty of reconstructing tectonics when data is sparse. In this manuscript, we propose an anticipation experiment, proposing a singular outlook on plate tectonics in the digital era. We hypothesize that mantle convection models producing self-consistently plate-like behavior will capture the essence of the self-organisation of plate boundaries. Such models exist today in a preliminary fashion and we use them here to build a database of mid-ocean ridge and trench configurations. To extract knowledge from it we develop a machine learning framework based on Generative Adversarial Networks (GANs) that learns the regularities of the self-organisation in order to fill gaps of observations when working on reconstructing a plate configuration. The user provides the distribution of known ridges and trenches, the location of the region where observations lack, and our digital architecture proposes a horizontal divergence map from which missing plate boundaries are extracted. Our framework is able to prolongate and interpolate plate boundaries within an unresolved region, but fails to retrieve a plate boundary that would be completely contained in it. The attempt we make is certainly too early because geodynamic models need improvement and a larger amount of geodynamic model outputs, as independent as possible, is required. However, this work suggests applying such an approach to expand the capabilities of plate tectonics is within reach.
		   </p>
		</div>
  	</dd>

  	<dt class="journal">J21</dt>
	<dd id="nsrp2019" class="bib-journal">
		<span class="t-author">
			<a href="http://ozgurerkent.gforge.inria.fr">Pejman Rasti</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="https://www.researchgate.net/profile/Hugo_Dorez">Hugo Dorez</a>,
			<a href="https://www.researchgate.net/profile/Sablong_Raphael">Raphael Sablong</a>,
			<a href="https://www.researchgate.net/scientific-contributions/2079272489_Driffa_Moussata">Driffa Moussata</a>,
			Salma Samiei,
			<a href="https://scholar.google.fr/citations?user=33IO_m4AAAAJ&hl=fr">David Rousseau</a>.
	  	</span>
	  	<span class="t-title">
		    Machine Learning-Based Classification of the Health State of Mice Colon in Cancer Study from Confocal Laser Endomicroscopy.
		</span>
		<span class="t-medium">
			In "Nature Scientific Reports 9", 2019.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('nsrp2019');">
			Abstract</a>
			</li>			
			<li><a href="https://www.nature.com/articles/s41598-019-56583-9">Nature-Link</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			In this article, we address the problem of the classification of the health state of the colon's wall of mice, possibly injured by cancer with machine learning approaches. This problem is essential for translational research on cancer and is a priori challenging since the amount of data is usually limited in all preclinical studies for practical and ethical reasons. Three states considered including cancer, health, and inflammatory on tissues. Fully automated machine learning-based methods are proposed, including deep learning, transfer learning, and shallow learning with SVM. These methods addressed different training strategies corresponding to clinical questions such as the automatic clinical state prediction on unseen data using a pre-trained model, or in an alternative setting, real-time estimation of the clinical state of individual tissue samples during the examination. Experimental results show the best performance of 99.93% correct recognition rate obtained for the second strategy as well as the performance of 98.49% which were achieved for the more difficult first case.
		   </p>
		</div>
  	</dd>

	<dt class="journal">J20</dt>
	<dd id="unmanned2019" class="bib-journal">
		<span class="t-author">
			<a href="http://ozgurerkent.gforge.inria.fr">Ozgur Erkent</a>,
            <a href="../index.html">Christian Wolf</a> and
            <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>.
	  	</span>
	  	<span class="t-title">
		    End-to-End Learning of Semantic Grid Estimation Deep Neural Network with Occupancy Grids.
		</span>
		<span class="t-medium">
			In "Unmanned Systems", 07(03):171-181, 2019.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('unmanned2019');">
			Abstract</a>
			</li>
			<li> <a href="../papers/unmanned_systems_2019.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			We propose semantic grid, a spatial 2D map of the environment around an autonomous vehicle consisting of cells which represent the semantic information of the corresponding region such as car, road, vegetation, bikes, etc. It consists of an integration of an occupancy grid, which computes the grid states with a Bayesian filter approach, and semantic segmentation information from monocular RGB images, which is obtained with a deep neural network. The network fuses the information and can be trained in an end-to-end manner. The output of the neural network is refined with a conditional random field. The proposed method is tested in various datasets (KITTI dataset, Inria-Chroma dataset and SYNTHIA) and different deep neural network architectures are compared.
		   </p>
		</div>
  	</dd>

	<dt class="journal">J19</dt>
  	<dd id="ijdar2018" class="bib-arxive">
		<span class="t-author">
			<a href="https://liris.cnrs.fr/membres?idn=bmoysset">Bastien Moysset</a>,
			<a href="https://fr.linkedin.com/pub/christopher-kermorvant/2/58b/871">Christopher Kermorvant</a>,
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		 	Learning to detect, localize and recognize many text objects in document images from few examples.
		</span>
		<span class="t-medium">
			In International Journal on Document Analysis and Recognition (IJDAR), 21(3):161–175, 2018.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('ijdar2018');">
			Abstract</a>
			</li>
			<li><a href="https://arxiv.org/abs/1611.05664">Arxiv-Version</a> (similar version)</li>
      		</ul>
		<div class="t-abstract">
The current trend in object detection and localization is to
learn predictions with high capacity deep neural networks
trained on a very large amount of annotated data and using
a high amount of processing power.  In this work, we pro-
pose a new neural model which directly predicts bounding
box coordinates.  The particularity of our contribution lies
in the local computations of predictions with a new form of
local parameter sharing which keeps the overall amount of
trainable parameters low. Key components of the model are
spatial 2D-LSTM recurrent layers which convey contextual
information between the regions of the image.
We show that this model is more powerful than the state
of the art in applications where training data is not as abun-
dant as in the classical configuration of natural images and
Imagenet/Pascal VOC tasks.  We particularly target the de-
tection of text in document images,  but our method is not
limited to this setting.  The proposed model also facilitates
the detection of many objects in a single image and can deal
with inputs of variable sizes without resizing.
		</div>
  	</dd>

    <dt class="journal">J18</dt>
    <dd id="iet2018" class="bib-arxive">
        <span class="t-author">
            <a href="http://personel.gsu.edu.tr/en/en-emre-dogan">Emre Dogan</a>,
            <a href="http://www.goneneren.com/">Gonen Eren</a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="https://liris.cnrs.fr/membres?id=2207&onglet=publis">Eric Lombardi</a>,
            <a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>.
        </span>
        <span class="t-title">
            Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint selection.
        </span>
        <span class="t-medium">
            In IET Computer Vision, 12(4):403–411, 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('iet2018');">
            Abstract</a>
            </li>
            <li><a href="../papers/ietcv_emre.pdf">PDF</a></li>
            <li><a href="http://digital-library.theiet.org/content/journals/10.1049/iet-cvi.2017.0146">IET-Link</a></li>
            <li><a href="http://arxiv.org/abs/1709.08527">Arxiv-Version</a></li>
            <li><a href="https://github.com/emredog/mvposeestim">Source-code</a></li>
            </ul>
        <div class="t-abstract">
                We propose a new method for human pose estimation which leverages information
                from multiple views to impose a strong prior on articulated pose. The novelty
                of the method concerns the types of coherence modelled. Consistency is
                maximised over the different views through different terms modelling classical
                geometric information (coherence of the resulting poses) as well as appearance
                information which is modelled as latent variables in the global energy
                function. Moreover, adequacy of each view is assessed and their contributions
                are adjusted accordingly. Experiments on the HumanEva and UMPM datasets show
                that the proposed method significantly decreases the estimation error compared
                to single-view results.
        </div>
    </dd>

	<dt class="journal">J17</dt>
  	<dd id="cviu2017" class="bib-arxive">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="https://fr.linkedin.com/pub/florian-nebout/17/367/265">Florian Nebout</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>.

	  	</span>
	  	<span class="t-title">
		 	Hand Pose Estimation through Weakly-Supervised Learning of a Rich Intermediate Representation.
		</span>
		<span class="t-medium">
			In Computer Vision and Image Understanding (CVIU) 167:56-67, 2017.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cviu2017');">
			Abstract</a>
			</li>
			<li><a href="../papers/cviu2017.pdf">PDF</a></li>
			<li><a href="http://www.sciencedirect.com/science/article/pii/S1077314217301686">Sciencedirect-Link</a></li>
			<li><a href="http://arxiv.org/abs/1511.06728">Arxiv-Version</a></li>
			<li><a href="https://www.youtube.com/watch?v=7GMiExWKM8c">Video/youtube</a></li>
			<li><a href="../papers/pres-cviu2017-sfu-workshop2018.pdf">Presentation</a></li>
      	</ul>
		<div class="t-abstract">
			We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.
		</div>
  	</dd>

    <dt class="journal">J16</dt>
    <dd id="siggraphasia2017" class="bib-conference">
        <span class="t-author">
            <a href="http://liris.cnrs.fr/eric.guerin/">Eric Guerin</a>,
            <a href="http://liris.cnrs.fr/~egalin/">Eric Galin</a>,
            <a href="https://liris.cnrs.fr/julie.digne/">Julie Digne</a>,
            <a href="http://liris.cnrs.fr/adrien.peytavie/">Adrien Peytavie</a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="http://hpcg.purdue.edu/bbenes/">Bedrich Benes</a>,
            <a href="https://fr.linkedin.com/in/benoit-martinez-5a79623">Benoit Martinez</a>.
        </span>
        <span class="t-title">
            Interactive Example-Based Terrain Authoring with Conditional Generative Adversarial Networks.
        </span>
        <span class="t-medium">
            In Transactions on Graphics (SIGGRAPH Asia), 2017.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('siggraphasia2017');">
            Abstract</a>
            </li>
            <li><a href="../papers/tog2017.pdf">PDF</a></li>
            <li><a href="https://www.youtube.com/watch?v=5w685udM838">video/youtube</a></li>
            </ul>
        <div class="t-abstract">
            <p>
            Authoring virtual terrains presents a challenge and there is a strong need for authoring tools
            able to create realistic terrains with simple user-inputs and with high user control.
            We propose an example-based authoring pipeline that uses a set of terrain synthesizers dedicated to specific tasks.
            </p>
            <p>
            Each terrain synthesizer is a Conditional Generative Adversarial Network
            trained by using real-world terrains and their sketched counterparts.
            The training sets are built automatically with a view that the terrain synthesizers learn the generation from features that are easy to sketch. During the authoring process, the artist first creates a rough sketch of the main terrain features, such as rivers, valleys and ridges, and the algorithm automatically synthesizes a terrain corresponding to the sketch using the learned features of the training samples.
            Moreover, an erosion synthesizer can also generate terrain evolution by erosion at a very low computational cost.
            Our framework allows for an easy terrain authoring and provides a high level of realism for a minimum sketch cost.
            We show various examples of terrain synthesis created by experienced as well as inexperienced users who are able to
            design a vast variety of complex terrains in a very short time.
            </p>
        </div>
    </dd>

    <dt class="journal">J15</dt>
    <dd id="neuro2017" class="bib-journal">
        <span class="t-author">
            <a href="">Damien Fourure</a>,
            <a href="http://home.heeere.com/">Remi Emonet</a>,
            <a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>,
            <a href="http://perso.univ-st-etienne.fr/muda8804/">Damien Muselet</a>,
            <a href="https://nneverova.github.io">Natalia Neverova</a>,
            <a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp">Alain Trémeau</a>,
            <a href="../index.html">Christian Wolf</a>.
        </span>
        <span class="t-title">
            Multi-task, Multi-domain Learning: application to semantic segmentation and pose regression.
        </span>
        <span class="t-medium">
            In Neurocomputing, 2017.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('neuro2017');">
            Abstract</a>
            </li>
            <li><a href="../papers/neurocomputing2017.pdf">PDF</a></li>
            <li><a href="http://www.sciencedirect.com/science/article/pii/S0925231217306847#">ScienceDirect-Link</a></li>
            </ul>
        <div class="t-abstract">
            We present an approach that leverages multiple datasets annotated for different tasks (e.g., classification with different labelsets) to improve the predictive accuracy on each individual dataset.
            Domain adaptation techniques can correct dataset bias but they are not applicable when the tasks differ, and they need to be complemented to handle multi-task settings.
            We propose a new selective loss function that can be integrated into deep neural networks to exploit training data coming from multiple datasets annotated for related but possibly different label sets.
            We show that the gradient-reversal approach for domain adaptation can be used in this setup to additionally handle domain shifts.
            We also propose an auto-context approach that further captures existing correlations across tasks.
            Thorough experiments on two types of applications (semantic segmentation and hand pose estimation) show the relevance of our approach in different contexts.
        </div>
    </dd>

	<dt class="journal">J14</dt>
	<dd id="pami2016" class="bib-journal">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a> and
			<a href="https://fr.linkedin.com/pub/florian-nebout/17/367/265">Florian Nebout</a>.
	  	</span>
	  	<span class="t-title">
		    ModDrop: adaptive multi-modal gesture recognition.
		</span>
		<span class="t-medium">
			In IEEE Transactions on Pattern Analysis and Machine Intelligence - PAMI 38(8):1692-1706, 2016.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('pami2016');">
			Abstract</a>
			</li>
			<li><a href="../papers/pami2015.pdf">PDF</a></li>
			<li><a href="http://arxiv.org/abs/1501.00102">Arxiv-Version</a></li>
			<li><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=7169562">IEEE-Link</a></li>
			<li><a href="https://www.youtube.com/watch?v=yuT8i8eiUXU">video/youtube</a></li>
			<li><a href="https://github.com/nneverova/deepgestures_lasagne">Theano/Lasagne code<a></li>
      		</ul>
		<div class="t-abstract">
			We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed "ModDrop") for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.

		</div>
  	</dd>

  	<dt class="journal">J13</dt>
	<dd id="access2016" class="bib-journal">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			Griffin Lacey, Lex Fridman, Deepak Chandra, Brandon Barbello and
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>.

	  	</span>
	  	<span class="t-title">
		 	Learning Human Identity from Motion Patterns.
		</span>
		<span class="t-medium">
			In IEEE Access (4):1810-1820, 2016.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('access2016');">
			Abstract</a>
			</li>
			<li><a href="../papers/ieeeaccess2016.pdf">PDF</a></li>
			<li><a href="http://arxiv.org/abs/1511.03908">Arxiv-Version</a></li>
			<li><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7458136">IEEE-Link</a></li>
      		</ul>
		<div class="t-abstract">
			We present a large-scale study, exploring the capability of temporal deep neural networks in interpreting natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind dataset of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We (1) compare several neural architectures for efficient learning of temporal multi-modal data representations, (2) propose an optimized shift-invariant dense convolutional mechanism (DCWRNN) and (3) incorporate the discriminatively-trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate, that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems.

		</div>
  	</dd>

  	<dt class="journal">J12</dt>
  	<dd id="prl2016" class="bib-journal">
		<span class="t-author">
			<a href="https://scholar.google.fr/citations?user=GhnwS48AAAAJ&hl=fr">Alaeddine Mihoub</a>,
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~frederic.elisei/">Fréderic Elisei</a>.
	  	</span>
	  	<span class="t-title">
		    Graphical models for social behavior modeling in face-to face interaction.
		</span>
		<span class="t-medium">
			In Pattern Recognition Letters (75):82-89, 2016.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('prl2016');">
			Abstract</a>
			</li>
			<li><a href="../papers/prl2016.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			The goal of this paper is to model the coverbal behavior of a subject involved in face-to-face social interactions. For this end, we present a multimodal behavioral model based on a Dynamic Bayesian Network (DBN). The model was inferred from multimodal data of interacting dyads in a specific scenario designed to foster mutual attention and multimodal deixis of objects and places in a collaborative task. The challenge for this behavioral model is to generate coverbal actions (gaze, hand gestures) for the subject given his verbal productions, the current phase of the interaction and the perceived actions of the partner. In our work, the structure of the DBN was learned from data, which revealed an interesting causality graph describing precisely how verbal and coverbal human behaviors are coordinated during the studied interactions. Using this structure, DBN exhibits better performances compared to classical baseline models such as Hidden Markov Models (HMMs) and Hidden Semi-Markov Models (HSMMs). We outperform the baseline in both measures of performance, i.e. interaction unit recognition and behavior generation. DBN also reproduces more faithfully the coordination patterns between modalities observed in ground truth compared to the baseline models.
		</div>
  	</dd>

  	<dt class="journal">J11</dt>
	<dd id="jmiv2015" class="bib-journal">
		<span class="t-author">
			<a href="http://scholar.google.com/citations?user=CCCoMqcAAAAJ&hl=en">Oya Celiktutan</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a> and
			<a href="https://liris.cnrs.fr/membres?id=2207&onglet=publis">Eric Lombardi</a>.
	  	</span>
	  	<span class="t-title">
			Fast Exact Hyper-Graph Matching with Dynamic Programming for Spatio-Temporal Data.
		</span>
		In
		<span class="t-medium">
		Journal on Mathematical Imaging and Vision, pp. 1-21, 2015.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('jmiv2015');">Abstract</a></li>
		<li><a href="../papers/jmiv2014-oya.pdf">PDF</a></li>
		<li><a href="http://arxiv.org/abs/1505.00581">Arxiv paper on the GPU implementation only</a>
      		</ul>
		<div class="t-abstract">
		<p>
		Graphs and hyper-graphs are frequently used to recognize complex and often non-rigid patterns in computer vision, either through graph matching or point-set matching with graphs. Most formulations resort to the minimization of a difficult energy function containing geometric or structural terms, frequently coupled with data attached terms involving appearance information. Traditional methods solve the minimization problem approximately, for instance re- sorting to spectral techniques. In this paper, we deal with the spatio-temporal data, for a concrete example, human actions in video sequences. In this context, we first make three realistic assumptions: (i) causality of human movements; (ii) sequential nature of human movements; and (iii) one-to-one mapping of time instants. We show that, under these assumptions, the correspondence problem can be decomposed into a set of subproblems such that each subproblem can be solved recursively in terms of the others, and hence an efficient exact minimization algorithm can be derived using dynamic programming approach. Secondly, we propose a special graphical structure which is elongated in time. We argue that, instead of approximately solving the original problem, a solution can be obtained by exactly solving an approximated problem. An exact minimization algorithm is derived for this structure and successfully applied to action recognition in two settings: video data and Kinect coordinate data.
		</p>
		</div>
  	</dd>

  	<dt class="journal">J10</dt>
  	<dd id="jmui2015" class="bib-journal">
		<span class="t-author">
			<a href="http://www.gipsa-lab.grenoble-inp.fr/page_pro.php?vid=1752">Alaeddine Mihoub</a>,
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~frederic.elisei/">Fréderic Elisei</a>.
	  	</span>
	  	<span class="t-title">
		    Learning multimodal behavioral models for face-to-face social interaction.
		</span>
		In
		<span class="t-medium">
			Journal on Multimodal User Interfaces, (9):3, pp 195-210, 2015.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('jmui2015');">
			Abstract</a>
			</li>
			<li><a href="../papers/jmiv2014-oya.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			The aim of this paper is to model multimodal perception-action loops of human behavior in face-to-face interactions. The long-term goal of this research is to give artificial agents social skills to engage believable interactions with human interlocutors. To this end, we propose trainable behavioral models that generate optimal actions given others’ perceived actions and joint goals. We first compare sequential models - in particular Discrete Hidden Markov Models (DHMMs) - with standard classifiers (SVMs and Decision Trees). We propose a modification of the initialization of the DHMMs in order to better capture the recurrent structure of the sensory-motor states. We show that the explicit state duration modeling by Hidden Semi Markov Models (HSMMs) improves prediction performance. We applied these models to parallel speech and gaze data collected from interacting dyads. The challenge was to predict the gaze of one subject given the gaze of the interlocutor and the voice activity of both. For both HMMs and HSMMs the Short-Time Viterbi concept is used for incremental decoding and generation. For the proposed models we evaluated objectively many properties in order to go beyond pure classification performance. Results show that while Incremental Discrete HMMs (IDHMMs) were more efficient than classic classifiers, the Incremental Discrete HSMMs (IDHSMMs) give best performance. This result emphasizes the relevance of state duration modeling.
		</div>
  	</dd>

  	<dt class="journal">J9</dt>
	<dd id="cviu2014" class="bib-journal">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>,
			<a href="https://liris.cnrs.fr/membres?id=2207&onglet=publis">Eric Lombardi</a>,
			<a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
			<a href="http://scholar.google.com/citations?user=CCCoMqcAAAAJ&hl=en">Oya Celiktutan</a>,
			<a href="http://dblp.uni-trier.de/pers/hd/j/Jiu:Mingyuan">Mingyuan Jiu</a>,
			<a href="http://personel.gsu.edu.tr/en/en-emre-dogan">Emre Dogan</a>,
			<a href="http://www.goneneren.com/">Gonen Eren</a>,
			<a href="http://liris.cnrs.fr/moez.baccouche/">Moez Baccouche</a>,
			<a href="http://perso.ec-lyon.fr/emmanuel.dellandrea/">Emmanuel Dellandréa</a>,
			<a href="http://perso.ec-lyon.fr/charles-edmond.bichot/">Charles-Edmond Bichot</a>,
			<a href="https://liris.cnrs.fr/membres?id=3701&onglet=publis">Christophe Garcia</a>,
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>.
	  	</span>
	  	<span class="t-title">
			Evaluation of video activity localizations integrating quality and quantity measurements.
		</span>
			In
		<span class="t-medium">
			Computer Vision and Image Understanding
		</span>
		(127):14-30, 2014.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('cviu2014');">Abstract</a></li>
		<li><a href="../papers/cviu2014.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>Evaluating the performance of computer vision algorithms is classically done by reporting classification error or accuracy, if the problem at hand is the classification of an object in an image, the recognition of an activity in a video or the categorization and labeling of the image or video. If in addition the detection of an item in an image or a video, and/or its localization are required, frequently used metrics are Recall and Precision, as well as ROC curves. These metrics give quantitative performance values which are easy to understand and to interpret even by non-experts. However, an inherent problem is the dependency of quantitative performance measures on the quality constraints that we need impose on the detection algorithm. In particular, an important quality parameter  of these measures is the spatial or spatio-temporal overlap between a ground-truth item and a detected item, and this needs to be taken into account when interpreting the results.
		</p>
		<p>
		We propose a new performance metric addressing and unifying the qualitative and quantitative aspects of the performance measures. The performance of a detection and recognition algorithm is illustrated intuitively by performance graphs which present quantitative performance values, like Recall, Precision and F-Score, depending on quality constraints of the  detection. In order to compare the performance of different computer vision algorithms, a representative single performance measure is computed from the graphs, by integrating out all quality parameters. The evaluation method can be applied to different types of activity detection and recognition algorithms. The performance metric has been tested on several activity recognition algorithms participating in the ICPR 2012 HARL competition.
		</p>
		</div>
  	</dd>


	<dt class="journal">J8</dt>
	<dd id="prl2013" class="bib-journal">
		<span class="t-author">
			<a href="http://dblp.uni-trier.de/pers/hd/j/Jiu:Mingyuan">Mingyuan Jiu,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a> and
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>.
	  	</span>
	  	<span class="t-title">
			Human body part estimation from depth images via spatially-constrained deep learning.
		</span>
		In
		<span class="t-medium">
			Pattern Recognition Letters 50(1):122-129, 2014.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('prl2013');">Abstract</a></li>
		<li><a href="../papers/prl2013-mingyuan.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		Object recognition, human pose estimation and scene recognition are applications which are
		frequently solved through a decomposition into a collection of parts. The resulting local
		representation has significant advantages, especially in the case of occlusions and when
		the subject is non-rigid. Detection and recognition require modelling the appearance of the
		different object parts as well as their spatial layout. This representation has been particularly
		successful in body part estimation from depth images.
		Integrating the spatial layout of parts may require the minimization of complex energy functions.
		This is prohibitive in most real world applications and therefore often omitted. However,
		ignoring the spatial layout puts all the burden on the classifier, whose only available
		information is local appearance. We propose a new method to integrate spatial layout into
		parts classification without costly pairwise terms during testing.
		Spatial relationships are exploited in the training algorithm, but not during testing.
		As with competing methods, the proposed method classifies pixels independently, which
		makes real-time processing possible. We show that training a classifier with spatial
		relationships increases generalization performance when compared to classical training
		minimizing classification error on the training set. We present an application to
		human body part estimation from depth images.
		</p>
		</div>
  	</dd>

  	<dt class="journal">J7</dt>
	<dd id="cc2012" class="bib-journal">
		<span class="t-author">
			<a href="http://dblp.uni-trier.de/pers/hd/j/Jiu:Mingyuan">Mingyuan Jiu,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/christophe.garcia">Christophe Garcia</a> and
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>.
	  	</span>
	  	<span class="t-title">
			Supervised learning and codebook optimization for bag of words models.
		</span>
		In
		<span class="t-medium">
			Cognitive Computation, Springer Verlag,
		</span>
		 (4):409-419, 2012.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('cc2012');">Abstract</a></li>
		<li><a href="../papers/cogncomp2012.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		In this paper, we present a novel approach for supervised codebook learning and optimization for bag of words models. This type of models is frequently used in visual recognition tasks like object class recognition or human action recognition. An entity is represented as a histogram of codewords, which are traditionally clustered with unsupervised methods like \textit{k}-means or random forests, and then classified in a supervised way. We propose a new supervised method for joint codebook creation and class learning, which learns the cluster centers of the codebook in a goal-directed way using the class labels of the training set. As a result, the codebook is highly correlated to the recognition problem, leading to a more discriminative codebook. We propose two different learning algorithms, one based on error backpropagation and one based on cluster label reassignment. We apply the proposed method to human action recognition from video sequences and evaluate it on the KTH dataset, reporting very promising results. The proposed technique allows to improve the discriminative power of an unsupervised learned codebook, or to keep the discriminative power while decreasing the size of the learned codebook, thus decreasing the computational complexity due to the nearest neighbor search.
		</p>
		</div>
  	</dd>

  	<dt class="journal">J6</dt>
	<dd id="tvc2012" class="bib-journal">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>.
	  	</span>
	  	<span class="t-title">
			Combinatorial Mesh Optimization,
		</span>
		In
		<span class="t-medium">
			The Visual Computer,
		</span>
		 28(5):511-525, 2012.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('tvc2012');">Abstract</a></li>
        	<li><a href="../papers/visualcomputer2011.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		A new mesh optimization framework for 3D triangular surface meshes is presented, which formulates the task as an energy minimization problem in the same spirit as in Hoppe et al. [1]. The desired mesh properties are controlled through a global energy function including data attached terms measuring the fidelity to the original mesh, shape potentials favoring high quality triangles and connectivity as well as budget terms controlling the sampling density. The optimization algorithm modifies mesh connectivity as well as the vertex positions. Solutions for the vertex repositioning step are obtained by a discrete graph cut algorithm examining global combinations of local candidates. Results on various 3D meshes compare favorably to recent state-of-the-art algorithms. Applications consist in optimizing triangular meshes and in simplifying meshes, while maintaining high mesh quality. Targeted areas are the improvement of the accuracy of numerical simulations, the convergence of numerical schemes, improvements of mesh rendering (normal field smoothness) or improvements of the geometric prediction in mesh compression techniques.
		</p>
		</div>
  	</dd>

  	<dt class="journal">J5</dt>
	<dd id="pami2009" class="bib-journal">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
			Document Ink bleed-through removal with two hidden Markov random fields and a single observation field.
		</span>
		<span class="t-medium">
			In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 32(3):431-447, 2010.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('pami2009');">Abstract</a></li>
        	<li><a href="../papers/pami2009.pdf">PDF</a></li>
		<li><a href="../software/mrfrestoration/index.html">Software</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		We present a new method for blind document bleed through removal based on separate Markov Random Field (MRF) regularization for the recto and for the verso side, where separate priors are derived from the full graph. The segmentation algorithm is based on Bayesian Maximum a Posteriori (MAP) estimation. The advantages of this separate approach are the adaptation of the prior to the contents creation process (e.g. superimposing two hand written pages), and the improvement of the estimation of the recto pixels through an estimation of the verso pixels covered by recto pixels; Moreover, the formulation as a binary labeling problem with two hidden labels per pixels naturally leads to an efficient optimization method based on the minimum cut/maximum flow in a graph. The proposed method is evaluated on scanned document images from the 18th century, showing an improvement of character recognition results compared to other restoration methods.
		</p>
		</div>
  	</dd>

  	<dt class="journal">J4</dt>
	<dd id="neuro2009" class="bib-journal">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and G&eacute;rald Gavin.
	  	</span>
	  	<span class="t-title">
			Inference and parameter estimation on hierarchical belief networks for image segmentation.
		</span>
		<span class="t-medium">
			In Neurocomputing 73(4-6):563-569, 2010.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('neuro2009');">Abstract</a></li>
        	<li><a href="../papers/neurocomputing2009.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		We introduce a new causal hierarchical belief network for image segmentation. Contrary to classical tree structured (or pyramidal) models, the factor graph of the network contains cycles. Each level of the hierarchical structure features the same number of sites as the base level and each site on a given level has several neighbors on the parent level. Compared to tree structured models, the (spatial) random process on the base level of the model is stationary which avoids known drawbacks, namely visual artifacts in the segmented image.
		We propose different parameterizations of the conditional probability distributions governing the transitions between the image levels. A parametric distribution depending on a single parameter allows the design of a fast inference algorithm on graph cuts, whereas for arbitrary distributions, we propose inference with loopy belief propagation. The method is evaluated on scanned documents, showing an improvement of character recognition results compared to other methods.
		</p>
		</div>
  	</dd>

  	<dt class="journal">J3</dt>
	<dd id="ijdar2006" class="bib-journal">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
       			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms,
		</span>
			In
		<span class="t-medium">
			International Journal on Document Analysis and Recognition
		</span>, 8(4):280-296, 2006.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('ijdar2006');">	Abstract</a></li>
        	<li><a href="../papers/tr-liris-2005-wolfjolion.pdf">Similar-Content-PDF</a></li>
        	<li><a href="javascript:;" onClick="displayBibtexOf('ijdar2006');">BibTeX</a></li>
        	<li><a href="../software/deteval/index.html">Software</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		Evaluation of object detection algorithms is a non-trivial task: a detection result is usually
		evaluated by comparing the bounding box of the detected object with the bounding box of the
		ground truth object. The commonly used precision and recall measures are computed from the overlap
		area of these two rectangles. However, these measures have several drawbacks: they don't give
		intuitive information about the proportion of the correctly detected objects and the number of
		false alarms, and they cannot be accumulated across multiple images without creating ambiguity
		in their interpretation. Furthermore, quantitative and qualitative evaluation is often mixed
		resulting in ambiguous measures.
		</p><p>
		In this paper we propose a new approach which tackles these problems. The performance of a
		detection algorithm is illustrated intuitively by performance graphs which present object level
		precision and recall depending on constraints on detection quality. In order to compare
		different detection algorithms, a representative single performance value is computed from the
		graphs. The influence of the test database on the detection performance is illustrated by
		performance/generality graphs. The evaluation method can be applied to different types of object
		detection algorithms. It has been tested on different text detection algorithms, among which are
		the participants of the ICDAR 2003 text detection competition.
		</p>
		</div>
		<div class="t-bibtex">
		<PRE>
@Article{WolfIJDAR2006,
  Author         = {C. Wolf and J.-M. Jolion},
  Title          = {Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms},
  Journal        = {International Journal on Document Analysis and Recognition},
  year           = {2006},
  volume     = {8},
  number     = {4},
  pages      = {280-296}
}
		</PRE>
		</div>
  	</dd>

  	<dt class="journal">J2</dt>
	<dd id="ijdar2004simon" class="bib-journal">
		<span class="t-author">
			S.M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, R. Young,
      			K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto, H. Miyao, J. Zhu, W. Ou,
			<a href="../index.html">C. Wolf</a>,
			<a href="http://rfv.insa-lyon.fr/~jolion">J.-M. Jolion</a>,
			L. Todoran, M. Worring,
      		et X. Lin.
	  	</span>
	  	<span class="t-title">
			ICDAR 2003 Robust Reading Competitions: Entries, Results and
			Future Directions
		</span>
		<span class="t-medium">
			International Journal on Document Analysis and Recognition (IJDAR),
		</span>
			7(2-3):105-122, 2005
			(Special Issue on Camera-based Text and Document Recognition)
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('ijdar2004simon');">
			Abstract</a>
		</li>
      		</ul>
		<div class="t-abstract">
		This paper describes the robust reading competitions for ICDAR 2003. With the rapid
		growth in research over the last few years on recognizing text in natural scenes,
		there is an urgent need to establish some common benchmark datasets, and
		gain a clear understanding of the current state of the art. We use the term robust
		reading to refer to text images that are beyond the capabilities of current
		commercial OCR packages. We chose to break down the robust reading problem into
		three sub-problems, and run competitions for each stage, and also a competition for
		the best overall system. The sub-problems we chose were text locating, character
		recognition and word recognition. By breaking down the problem in this way, we hoped
		to gain a better understanding of the state of the art in each of the sub-problems.
		Furthermore, our methodology involved storing detailed results of applying each
		algorithm to each image in the data sets, allowing researchers to study in depth the
		strengths and weaknesses of each algorithm. The text locating contest was the only
		one to have any entries. We give a brief description of each entry, and present the
		results of this contest, showing cases where the leading entries succeed and fail.
		We also describe an algorithm for combining the outputs of the individual text
		locaters, and show how the combination scheme improves on any of the individual
		systems.
		</div>
  	</dd>

  	<dt class="journal">J1</dt>
	<dd id="paa2003" class="bib-journal">
		<span class="t-author">
			<a href="../index.html">Christian
       		Wolf</a> and <a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Extraction and Recognition of Artificial Text in Multimedia Documents.
		</span>
		<span class="t-medium"> Pattern Analysis and Applications,
		</span>
		6(4):309-326, 2003.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('paa2003');">	Abstract</a></li>
        	<li><a href="../papers/tr-rfv-2002-01.pdf">Similar-Content-PDF</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('paa2003');">BibTeX</a></li>
        	<li><a href="http://telesun.insa-lyon.fr/%7Ewolf/demos/textdetect.html">Demo</a></li>
      		</ul>
		<div class="t-abstract">
		The systems currently available for content based image and video retrieval
		work without semantic knowledge, i.e. they use image processing methods to
		extract low level features of the data. The similarity obtained by these
		approaches does not always correspond to the similarity a human user would expect.
		A way to include more semantic knowledge into the
		indexing process is to use the text included in the images and video sequences.
		It is rich in information but easy to use, e.g. by key word based queries. In
		this paper we present an algorithm to localize artificial text in images and
		videos using a measure of accumulated gradients and morphological processing.
		The quality of the localized text is improved by robust multiple frame integration.
		A new technique for the binarization of the text boxes based on a criterion
		maximizing local contrast is proposed. Finally, detection and OCR results for a
		commercial OCR are presented, justifying the choice of the binarization technique
		</div>
		<div class="t-bibtex">
		<PRE>
@Article{WolfPAA03,
  Author         = {C. Wolf and J.-M. Jolion},
  Title          = {Extraction and {R}ecognition of {A}rtificial {T}ext in {M}ultimedia {D}ocuments},
  Journal        = {Pattern {A}nalysis and {A}pplications},
  year           = {2003},
  volume     = {6},
  number     = {4},
  pages      = {309-326}
}
		</PRE>
		</div>
  	</dd>
</dl>

<!-- ****************************************************************************************

  ******************************************************************************************* -->

<h2>Scientific competitions</h2>

<ul>

	<li class="bib-empty">
  		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>,
			Florian Nebout.
	  	</span>
		Ranked 1st of 17 in the
		<span class="t-title">
		"ChaLearn 2014 Looking at People: Gesture Recognition" Competition,
		in conjunction with ECCV 2014
		</span>
		<ul class="reflink-box">
			<li class="first"><a href="../papers/eccv2014ws.pdf">ECCV Workshop Paper</a></li>
			<li><a href="../papers/eccv2014ws-pres.pdf">Presentation-PDF</a></li>
			<li><a href="http://gesture.chalearn.org/challenge-results">Competition Results</a></li>
		</ul>
  	</li>

	<li class="bib-empty">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.istc.cnr.it/people/giulio-paci">Giulio Paci</a>,
			<a href="http://www.istc.cnr.it/people/giacomo-sommavilla">Giacomo Sommavilla</a>
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>,
			Florian Nebout.
	  	</span>
		Ranked 6th of 20 in the ChaLearn 2013
		<a href="../papers/iccv2013ws.pdf">Gesture Recognition Competition</a>.
  	</li>

	<li class="bib-empty">
		Ranked 5th of 43 in the ICDAR 2009
		<a href="http://www.cvc.uab.es/icdar2009/papers/3725b375.pdf">document image binarisation contest</a>.
  	</li>

	<li class="bib-empty">
		Our <a href="../software/deteval/index.html">DetEval algorithm and software</a> has been used to evaluate the entries of the ImageEval 2007 text detection competition.
  	</li>

	<li class="bib-empty">
		Our <a href="../software/deteval/index.html">DetEval algorithm and software</a> has been used to evaluate the entries of the ICDAR 2003 robust reading competition.
  	</li>

	<li class="bib-empty">
		Participation in the ICDAR 2003 robust reading competition.
	</li>

	<li class="bib-empty">
		Participation in the TREC 2002 Video TREC (with David Doermann of University of Maryland).
	</li>
</ul>

<!-- ****************************************************************************************
  INVITED TALKS
  ******************************************************************************************* -->


<h2>Invited talks at conferences</h2>

<p>
Invited talks/seminars <a href="pres.html">[ARE HERE]</a>.
</p>

<dl>
	<dt class="conference">-</dt>
	<dd id="mmsys2018" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Learning human motion: gestures, activities, pose, identity.
		</span>
		Invited talk at
		<span class="t-medium">
		      MMSYS
		</span>
		Bielefeld, 2018.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('mmsys2018');">Abstract</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		    This talk is devoted to (deep) learning methods advancing automatic analysis and interpreting of human motion from different perspectives and based on various sources of information, such as images, video, depth, mocap data, audio and inertial sensors. We propose several models and associated training algorithms for supervised classification and semi-supervised and weakly-supervised feature learning, as well as modelling of temporal dependencies, and show their efficiency on a set of fundamental tasks, including detection, classification, parameter estimation and user verification.
		</p><p>
		Advances in several applications will be shown, including (i) gesture spotting and recognition based on multi-scale and multi-modal deep learning from visual signals; (ii) human activity recognition using models of visual attention; (iii) hand pose estimation through deep regression from depth images, based on semi-supervised and weakly-supervised learning; (iv) mobile biometrics, in particular the automatic authentification of smartphone users through learning from data acquired from inertiel sensors.
		</p>
		</div>
  	</dd>

	<dt class="conference">c21</dt>
	<dd id="ipta2012" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>,
	  	</span>
	  	<span class="t-title">
			Action recognition in videos,
		</span>
		Invited talk at
		<span class="t-medium">
		      International Conference on Image Processing Theory, Tools and Applications,
		</span>
		Istanbul, 2012.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('ipta2012');">Abstract</a></li>
		<li><a href="../papers/ipta2012.pdf">PDF</a></li>
		<li><a href="../papers/ipta2012-pres.pdf">PDF (Presentation)</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		    Activity recognition in video sequences is a difficult problem due to the complex characteristics of human articulated motion and its large variations. It requires motion estimation, which involves the separation of motion and visual appearance information, the suppression of irrelevant background clutter and background motion, the separation of motion belonging to different people, and the creation of models describing actions. In this talk we will briefly describe the different frameworks for action recognition, based on background subtraction and on space-time interest points, and we will focus and structured and on semi-structured models. These models attempt to bridge the gap between the rich descriptive power of fully structured models constructed from sets of local features and the convenience and the power of machine learning algorithms, which are mostly based on unstructured features embedded in vector spaces. Semi-structured models proceed by translating structured information into unstructured information, while structured models keep a full representation. As an example we will deal with graphs and graph matching algorithms. Hierarchical representations and parts based models will be investigated, which allow to decompose complex activities into smaller parts of less sophisticated elementary actions or elementary descriptors.
		</p>
		</div>
  	</dd>

</dl>

<!-- ****************************************************************************************
  INTERNATINAL CONFERENCES
  ******************************************************************************************* -->


<h2>Articles at conferences with international audience (refereed)</h2>

<dl>

	<dt class="conference">c74</dt>
	<dd id="arxiv2022cdc" class="bib-conference">
		<span class="t-author">						
			<a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny<a>,
			<a href="https://fr.linkedin.com/in/quentin-possama%C3%AF-425420142">Quentin Possamaï<a>,
			<a href="https://sites.google.com/site/laurentbako">Laurent Bako<a>,
			<a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri<a>			
      and 
      <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Learning Reduced Nonlinear State-Space Models: an Output-Error Based Canonical Approach
		</span>
		<span class="t-medium">
			To appear in Control and Decision Conference (CDC), 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cdc2022');">
			Abstract</a>
			</li>												
			<li><a href="https://arxiv.org/abs/2206.04791">Arxiv-Version</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			The identification of a nonlinear dynamic model is an open topic in control theory, especially from sparse inputoutput measurements. A fundamental challenge of this problem is that very few to zero prior knowledge is available on both the state and the nonlinear system model. To cope with this challenge, we investigate the effectiveness of deep learning in the modeling of dynamic systems with nonlinear behavior by advocating an approach which relies on three main ingredients: (i) we show that under some structural conditions on the tobe-identified model, the state can be expressed in function of a sequence of the past inputs and outputs; (ii) this relation which we call the state map can be modelled by resorting to the welldocumented approximation power of deep neural networks; (iii) taking then advantage of existing learning schemes, a statespace model can be finally identified. After the formulation and analysis of the approach, we show its ability to identify three different nonlinear systems. The performances are evaluated in terms of open-loop prediction on test data generated in simulation as well as a real world data-set of unmanned aerial vehicle flight measurements.
		    </p>
		</div>
  </dd>

	<dt class="conference">c73</dt>
	<dd id="iros2022" class="bib-conference">
		<span class="t-author">			
			<a href="https://scholar.google.fr/citations?user=NAI5mi4AAAAJ&hl=fr">Pierre Marza</a> and
			<a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetitia Matignon</a> and
			<a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a> and
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation.
		</span>
		<span class="t-medium">
			To appear in International Conference on Intelligent Robots and Systems (IROS), 2022
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2021bmvc');">
			Abstract</a>
			</li>						
			<li><a href="../papers/arxivmultion2021.pdf">PDF</a></li>							
			<li><a href="https://arxiv.org/abs/2107.06011">ArXiv-Version</a></li>				
			<li><a href="http://multion-challenge.cs.sfu.ca">Multi-ON@CVPR 2021 (1st ranked)</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object affordances. In classical Reinforcement Learning (RL) setups, this capacity is learned from reward alone. We introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input.
		    </p>
		</div>
  	</dd>
  	
	<dt class="conference">c72</dt>
	<dd id="iclr2022" class="bib-conference">
		<span class="t-author">
			<a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny<a>,
			<a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
			<a href="https://nneverova.github.io/">Natalia Neverova<a>,
            <a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri<a>,
            <a href="http://www.cs.sfu.ca/~mori">Greg Mori</a>,
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space.
		</span>
		<span class="t-medium">
			In International Conference on Learning Representations (ICLR), 2022 (oral presentation, 1.6% acceptance rate).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('iclr2022');">Abstract</a>
			</li>
			<li><a href="https://openreview.net/forum?id=1L0C5ROtFp">Openreview-Link</a></li>			
			<li><a href="https://filteredcophy.github.io">Project + Benchmark dataset</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.
		    </p>
		</div>
  </dd>

  <dt class="conference">c71</dt>
	<dd id="icra2022assem" class="bib-arxive">
		<span class="t-author">						
			<a href="https://www.assemsadek.com/sadek">Assem Sadek<a>,
			<a href="https://fr.linkedin.com/in/guillaume-bono-4ba97767">Guillaume Bono<a>,	
			<a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii<a> and
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    An in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments.
		</span>
		<span class="t-medium">
			In International Conference on Robotics and Automation (ICRA), 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icra2022assem');">
			Abstract</a>
			</li>												
			<li><a href="https://arxiv.org/abs/2111.14666">ArXiv-Version</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			Visual navigation by mobile robots is classically tackled through SLAM plus optimal planning, and more recently through end-to-end training of policies implemented as deep networks. While the former are often limited to waypoint planning, but have proven their efficiency even on real physical environments, the latter solutions are most frequently employed in simulation, but have been shown to be able learn more complex visual reasoning, involving complex semantical regularities. Navigation by real robots in physical environments is still an open problem. End-to-end training approaches have been thoroughly tested in simulation only, with experiments involving real robots being restricted to rare performance evaluations in simplified laboratory conditions. In this work we present an in-depth study of the performance and reasoning capacities of real physical agents, trained in simulation and deployed to two different physical environments. Beyond benchmarking, we provide insights into the generalization capabilities of different agents training in different conditions. We visualize sensor usage and the importance of the different types of signals. We show, that for the PointGoal task, an agent pre-trained on wide variety of tasks and fine-tuned on a simulated version of the target environment can reach competitive performance without modelling any sim2real transfer, i.e. by deploying the trained agent directly from simulation to a real physical robot.
		    </p>
		</div>
  </dd>
	
	<dt class="conference">c70</dt>	
	<dd id="arxiv2021graphrl" class="bib-conference">
		<span class="t-author">
			<a href="https://github.com/edbeeching">Edward Beeching</a>,			
			Maxim Peter, Philippe Marcotte,
			<a href="http://perso.citi-lab.fr/jdibangoy">Jilles Dibangoye</a>,
			<a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>,
			Joshua Romoff and
			<a href="../index.html">Christian Wolf</a>.
			
	  	</span>
	  	<span class="t-title">
		  	Graph augmented Deep Reinforcement Learning in the GameRLand3D environment.
		</span>
		<span class="t-medium">
			To appear in AAAI Workshop on Reinforcement Learning in Games, 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2021graphrl');">
			Abstract</a>
			</li>				
			<li><a href="https://arxiv.org/abs/2112.11731">Arxiv-Version</a></li>						
      		</ul>
		<div class="t-abstract">
			<p>
			We address planning and navigation in challenging 3D video games featuring maps with disconnected regions reachable by agents using special actions. In this setting, classical symbolic planners are not applicable or diffi- cult to adapt. We introduce a hybrid technique combin- ing a low level policy trained with reinforcement learn- ing and a graph based high level classical planner. In addition to providing human-interpretable paths, the ap- proach improves the generalization performance of an end-to-end approach in unseen maps, where it achieves a 20% absolute increase in success rate over a recurrent end-to-end agent on a point to point navigation task in yet unseen large-scale maps of size 1km×1km. In an in- depth experimental study, we quantify the limitations of end-to-end Deep RL approaches in vast environments and we also introduce “GameRLand3D”, a new bench- mark and soon to be released environment built with the Unity engine able to generate complex procedural 3D maps for navigation tasks. An overview video is available <a href="https://www.youtube.com/watch?v=H0WAHvEeVyc">here</a>.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c69</dt>	
	<dd id="arxiv2021edwardgodot" class="bib-conference">
		<span class="t-author">
			<a href="https://github.com/edbeeching">Edward Beeching</a>,			
			<a href="http://perso.citi-lab.fr/jdibangoy">Jilles Dibangoye</a>,
			<a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a> and
			<a href="../index.html">Christian Wolf</a>.
			
	  	</span>
	  	<span class="t-title">
		  	Godot Reinforcement Learning Agents.
		</span>
		<span class="t-medium">
			To appear in AAAI Workshop on Reinforcement Learning in Games, 2022.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2021edwardgodot');">
			Abstract</a>
			</li>					
			<li><a href="https://arxiv.org/abs/2112.03636">Arxiv-Version</a></li>	
      		</ul>
		<div class="t-abstract">
			<p>
			We present Godot Reinforcement Learning (RL) Agents, an open-source interface for developing environments and agents in the Godot Game Engine. The Godot RL Agents interface allows the design, creation and learning of agent behaviors in challenging 2D and 3D environments with various on-policy and off-policy Deep RL algorithms. We provide a standard Gym interface, with wrappers for learning in the Ray RLlib and Stable Baselines RL frameworks. This allows users access to over 20 state of the art on-policy, off-policy and multi-agent RL algorithms. The framework is a versatile tool that allows researchers and game designers the ability to create environments with discrete, continuous and mixed action spaces. The interface is relatively performant, with 12k interactions per second on a high end laptop computer, when parallized on 4 CPU cores. An overview video is available <a href="https://www.youtube.com/watch?v=g1MlZSFqIj4">here</a>.
		    </p>
		</div>
  	</dd>
  	
	<dt class="conference">c68</dt>
	<dd id="neurips2021" class="bib-conference">
		<span class="t-author">			
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec*</a>,
			<a href="../index.html">Christian Wolf*</a>.
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a> and
			<a href="">Madiha Nadri<a>.            
	  	</span>
	  	<span class="t-title">
		    Supervising the Transfer of Reasoning Patterns in VQA.
		</span>
		<span class="t-medium">
			In Neural Information Processing Systems (NeurIPS), 2021 (*=equal contribution).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('neurips2021');">
			Abstract</a>
			</li>									
			<li><a href="../papers/arxiv2021supervisedtransfer.pdf">PDF</a></li>
			<li><a href="https://arxiv.org/abs/2106.05597">ArXiv-Link</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			Methods for Visual Question Anwering (VQA) are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. It has been recently shown that better reasoning patterns emerge in attention layers of a state-of-the-art VQA model when they are trained on perfect (oracle) visual inputs. This provides evidence that deep neural networks can learn to reason when training conditions are favorable enough. However, transferring this learned knowledge to deployable models is a challenge, as much of it is lost during the transfer. We propose a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. We also demonstrate the effectiveness of this approach experimentally on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training.
		    </p>
		</div>
  	</dd>

  	<dt class="conference">c67</dt>
	<dd id="arxiv2021sim2real" class="bib-arxive">
		<span class="t-author">
			<a href="https://theo-jaunet.github.io/">Théo Jaunet<a>,	
			<a href="https://fr.linkedin.com/in/guillaume-bono-4ba97767">Guillaume Bono<a>,	
			<a href="https://romain.vuillemot.net/">Romain Vuillemot<a> and
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Sim2RealViz: Visualizing the Sim2Real Gap in Robot Ego-Pose Estimation.
		</span>
		<span class="t-medium">
			NeurIPS XAI Workshop on eXplainable AI approaches for debugging and diagnosis, 2021 (oral).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2021sim2real');">
			Abstract</a>
			</li>						
			<li><a href="https://arxiv.org/abs/2109.11801">ArXiv-Link</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			The Robotics community has started to heavily rely on increasingly realistic 3D simulators for large-scale training of robots on massive amounts of data. But once robots are deployed in the real world, the simulation gap, as well as changes in the real world (e.g. lights, objects displacements) lead to errors. In this paper, we introduce Sim2RealViz, a visual analytics tool to assist experts in understanding and reducing this gap for robot ego-pose estimation tasks, i.e. the estimation of a robot's position using trained models. Sim2RealViz displays details of a given model and the performance of its instances in both simulation and real-world. Experts can identify environment differences that impact model predictions at a given location and explore through direct interactions with the model hypothesis to fix it. We detail the design of the tool, and case studies related to the exploit of the regression to the mean bias and how it can be addressed, and how models are perturbed by the vanish of landmarks such as bikes.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c66</dt>
	<dd id="cdc2021" class="bib-conference">
		<span class="t-author">
			<a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny</a>,
			<a href="https://sites.google.com/site/vincentandrieu/">Vincent Andrieu</a>,
			<a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri</a>,
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Deep KKL: Data-driven Output Prediction for Non-Linear Systems.
		</span>
		<span class="t-medium">
			In Control and Decision Conference (CDC), 2021.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cdc2021');">
			Abstract</a>
			</li>			
			<li><a href="../papers/arxiv2021kkl.pdf">PDF</a></li>
			<li><a href="https://arxiv.org/abs/2103.12443">ArXiv-Version</a></li>				
      		</ul>
		<div class="t-abstract">
			<p>
			We address the problem of output prediction, ie. designing a model for autonomous nonlinear systems capable of forecasting their future observations. We first define a general framework bringing together the necessary properties for the development of such an output predictor. In particular, we look at this problem from two different viewpoints, control theory and data-driven techniques (machine learning), and try to formulate it in a consistent way, reducing the gap between the two fields. Building on this formulation and problem definition, we propose a predictor structure based on the Kazantzis-Kravaris/Luenberger (KKL) observer and we show that KKL fits well into our general framework. Finally, we propose a constructive solution for this predictor that solely relies on a small set of trajectories measured from the system. Our experiments show that our solution allows to obtain an efficient predictor over a subset of the observation space.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c65</dt>
	<dd id="cvpr2021reasoning" class="bib-conference">
		<span class="t-author">
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec*</a>,
			<a href="https://theo-jaunet.github.io/">Théo Jaunet*<a>,			
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a>,
			<a href="https://romain.vuillemot.net/">Romain Vuillemot<a> and
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    How Transferrable are Reasoning Patterns in VQA?
		</span>
		<span class="t-medium">
			To appear in International Conference on Computer Vision and Pattern Recognition (CVPR), 2021 (*=equal contributions).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cvpr2021reasoning');">
			Abstract</a>
			</li>	
			<li><a href="../papers/cvpr2021reasoningpatterns.pdf">PDF</a></li>	
			<li><a href="https://arxiv.org/abs/2104.03656">Arxiv-version</a></li>
			<li><a href="https://reasoningpatterns.github.io">Interactive Visualization tool</a></li>	
			<li><a href="https://reasoningpatterns.github.io/demo.mp4">Demo Video</a></li>		
      		</ul>
		<div class="t-abstract">
			<p>
			Since its inception, Visual Question Answering (VQA) is notoriously known as a task, where models are prone to exploit biases in datasets to find shortcuts instead of performing high-level reasoning, required for generalization. Classical methods address these issues with different techniques including removing biases from training data, or adding branches to models to detect and remove biases. In this paper, we argue that uncertainty in vision is a dominating factor preventing the successful learning of reasoning in vision and language problems. We train a visual oracle with perfect sight, and in a large scale study provide experimental evidence that it is much less prone to exploiting spurious dataset biases compared to standard models.

			In particular, we propose to study the attention mechanisms at work in the visual oracle and compare them with a SOTA Transformer-based model. We provide an in-depth analysis and visualizations of reasoning patterns obtained with an online visualization tool which we make publicly available (https://reasoningpatterns.github.io).
			We exploit these insights by transferring reasoning patterns from the oracle model to a SOTA Transformer-based VQA model taking as input standard noisy inputs. Experiments show successful transfer as evidenced by higher overall accuracy, as well as accuracy on infrequent answers for each type of question, which provides evidence for improved generalization and a decrease of the dependency on dataset biases.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c64</dt>
	<dd id="arxiv2020roses" class="bib-conference">
		<span class="t-author">
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec</a>,
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a> and
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Roses Are Red, Violets Are Blue... but Should VQA Expect Them To?
		</span>
		<span class="t-medium">
			To appear in International Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2020roses');">
			Abstract</a>
			</li>		
			<li><a href="../papers/cvpr2021roses.pdf">PDF</a></li>	
			<li><a href="https://arxiv.org/abs/2006.05121">ArXiv-Version</a></li>	
			<li><a href="https://github.com/gqa-ood/GQA-OOD">Benchmark+Dataset</a></li>	
      		</ul>
		<div class="t-abstract">
			<p>
			To be reliable on rare events is an important requirement for systems based on machine learning. In this work we focus on Visual Question Answering (VQA), where, in spite of recent efforts, datasets remain imbalanced, causing shortcomings of current models: tendencies to overly exploit dataset biases and struggles to generalise to unseen associations of concepts. We focus on a systemic evaluation of model error distributions and address fundamental questions: How is the prediction error distributed? What is the prediction accuracy on infrequent vs. frequent concepts? In this work, we design a new benchmark based on a fine-grained reorganization of the GQA dataset [1], which allows to precisely answer these questions. It introduces distributions shifts in both validation and test splits, which are defined on question groups and are thus tailored to each question. We performed a large-scale study and we experimentally demonstrate that several state-of-the-art VQA models, even those specifically designed for bias reduction, fail to address questions involving infrequent concepts. Furthermore, we show that the high accuracy obtained on the frequent concepts alone is mechanically increasing overall accuracy, covering up the true behavior of current VQA models.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c63</dt>
    <dd id="arxiv2021sstvos" class="bib-conference">
		<span class="t-author">
			<a href="https://scholar.google.com/citations?user=Gd2IGrEAAAAJ">Brendan Duke</a>,
			<a href="https://ca.linkedin.com/in/abdallaahmed7">Abdalla Ahmed</a>,			
            <a href="../index.html">Christian Wolf</a>,
            <a href="https://scholar.google.com/citations?hl=en&user=Htlyw0kAAAAJ">Parham Aarabi</a> and
            <a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>.
	  	</span>
	  	<span class="t-title">
		    SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation
		</span>
		<span class="t-medium">
			To appear in International Conference on Computer Vision and Pattern Recognition (CVPR), 2021 (oral presentation).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2021sstvos');">
			Abstract</a>
			</li>		
			<li><a href="../papers/cvpr2021sstvos.pdf">PDF</a></li>		
			<li><a href="https://arxiv.org/abs/2101.08833">ArXiv-Version</a></li>
			<li><a href="https://github.com/dukebw/SSTVOS">Source code (by Brendan Buke)</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			In this paper we introduce a Transformer-based approach to video object segmentation (VOS). To address compounding error and scalability issues of prior work, we propose a scalable, end-to-end method for VOS called Sparse Spatiotemporal Transformers (SST). SST extracts per-pixel representations for each object in a video using sparse attention over spatiotemporal features. Our attention-based formulation for VOS allows a model to learn to attend over a history of multiple frames and provides suitable inductive bias for performing correspondence-like computations necessary for solving motion segmentation. We demonstrate the effectiveness of attention-based over recurrent networks in the spatiotemporal domain. Our method achieves competitive results on YouTube-VOS and DAVIS 2017 with improved scalability and robustness to occlusions compared with the state of the art.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c62</dt>
    <dd id="visxai2019" class="bib-arxive">
        <span class="t-author">
        	<a href="https://theo-jaunet.github.io/aboutme/">Théo Jaunet</a>,            
            <a href="https://romain.vuillemot.net">Romain Vuillemot</a> and
            <a href="../index.html">Christian Wolf</a>.
        </span>
        <span class="t-title">
            Théo Guesser: Could you beat an AI guessing where you are in Theo’s apartment?
        </span>
        <span class="t-medium">
            In IEEE VIS Workshop on AI Explainability, 2020.
        </span>
        <ul class="reflink-box">            
            <li class="first"> <a href="https://theo-jaunet.github.io/theo-guesser">Visualization</a></li>
            </ul>       
    </dd>

	<dt class="conference">c61</dt>
	<dd id="eccv2020" class="bib-arxive">
		<span class="t-author">
			<a href="https://github.com/edbeeching">Edward Beeching<a>,			
			<a href="http://perso.citi-lab.fr/jdibangoy">Jilles Dibangoye</a> and
			<a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>,
			<a href="../index.html">Christian Wolf</a>.
			
	  	</span>
	  	<span class="t-title">
		  	Learning to plan with uncertain topological maps.  
		</span>
		<span class="t-medium">
			To appear in European Conference on Computer Vision (ECCV), 2020 (spotlight, 5% acceptance rate).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('eccv2020');">
			Abstract</a>
			</li>			
			<li><a href="../papers/eccv2020.pdf">PDF</a></li>
			<li><a href="https://arxiv.org/abs/2007.05270">Arxiv-Version</a></li>
			<li><a href="https://edbeeching.github.io/papers/learning_to_plan">Project page (src etc.)</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c60</dt>
	<dd id="ecml2020egomap" class="bib-arxive">
		<span class="t-author">
			<a href="https://github.com/edbeeching">Edward Beeching<a>,			
			<a href="http://perso.citi-lab.fr/jdibangoy">Jilles Dibangoye</a> and
			<a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>,
			<a href="../index.html">Christian Wolf</a>.
			
	  	</span>
	  	<span class="t-title">
		    EgoMap: Projective mapping and structured egocentric memory for Deep RL.
		</span>
		<span class="t-medium">
			To appear in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2020.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('ecml2020egomap');">
			Abstract</a>
			</li>
			<li><a href="../papers/ecml2020.pdf">PDF</a></li>
			<li><a href="https://arxiv.org/abs/2002.02286">Arxiv-Version</a></li>
			<li><a href="https://edbeeching.github.io/papers/egomap">Project page (src etc.)</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			Tasks involving localization, memorization and planning in partially observable 3D environments are an ongoing challenge in Deep Reinforcement Learning. We present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent's performance in 3D environments on challenging tasks with multi-step objectives. The EgoMap architecture incorporates several inductive biases including a differentiable inverse projection of CNN feature vectors onto a top-down spatially structured map. The map is updated with ego-motion measurements through a differentiable affine transform. We show this architecture outperforms both standard recurrent agents and state of the art agents with structured memory. We demonstrate that incorporating these inductive biases into an agent's architecture allows for stable training with reward alone, circumventing the expense of acquiring and labelling expert trajectories. A detailed ablation study demonstrates the impact of key aspects of the architecture and through extensive qualitative analysis, we show how the agent exploits its structured internal memory to achieve higher performance.
		    </p>
		</div>
  	</dd>

  	<dt class="conference">c59</dt>
	<dd id="cdc2020" class="bib-conference">
		<span class="t-author">
			<a href="https://scholar.google.se/citations?user=Lc6LJ7MAAAAJ&hl=fr">Johan Peralez<a>,
			<a href="https://lagepp.univ-lyon1.fr/membre/galuppo-francesco/">Francesco Galuppo<a>,
            <a href="https://scholar.google.com/citations?user=dsNXtcgAAAAJ&hl=en">Pascal Dufour</a>,            
            <a href="../index.html">Christian Wolf</a> and
            <a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri</a>.
	  	</span>
	  	<span class="t-title">
		    Data-driven multi-model control for a waste heat recovery system.
		</span>
		<span class="t-medium">
			To appear in Control and Decision Conference (CDC), 2020.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cdc2020');">Abstract</a>
			</li>			
			<li><a href="../papers/cdc2020.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			We consider the problem of supervised learning of a multi-model based controller for non-linear systems. Selected multiple linear controllers are used for different operating points and combined with a local weighting scheme, whose weights are predicted by a deep neural network trained online. The network uses process and model outputs to drive the controller towards a suitable mixture of operating points.
			The proposed approach, which combines machine learning and classical control of linear processes, allows efficient imple- mentation on complex industrial processes. In this work, the control problem consists in the design of a controller for a waste heat recovery system (WHRS) mounted on a heavy duty (HD) truck engine to decrease fuel consumption and meet the future pollutant emissions standard. Note that the contribution of this work is not specific to HD truck processes since it can be applied to any nonlinear system with an existing linear controller bank.
			The proposed control scheme is successfully evaluated on an Organic Rankine Cycle (ORC) process simulator and compared to a standard linear controller and to several strong multi-model baselines without learning.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c58</dt>
	<dd id="iclr2020" class="bib-conference">
		<span class="t-author">
			<a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
			<a href="https://nneverova.github.io/">Natalia Neverova<a>,
            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
            <a href="http://www.cs.sfu.ca/~mori">Greg Mori</a>,
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    COPHY: Counterfactual Learning of Physical Dynamics.
		</span>
		<span class="t-medium">
			In International Conference on Learning Representations (ICLR), 2020 (spotlight, 6% acceptance rate).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('iclr2020');">Abstract</a>
			</li>
			<li><a href="https://openreview.net/forum?id=SkeyppEFvS">Openreview-Link</a></li>
			<li><a href="https://arxiv.org/abs/1909.12000">Arxiv-Version</a></li>
			<li><a href="https://projet.liris.cnrs.fr/cophy">Project + Benchmark dataset</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the COPHY benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.
		    </p>
		</div>
  	</dd>

  	<dt class="conference">c57</dt>
	<dd id="ecai2020" class="bib-conference">  	
		<span class="t-author">
			<a href="https://scholar.google.fr/citations?user=Rx507eQAAAAJ&hl=fr">Corentin Kervadec</a>,
			<a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a>,
			<a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a>,
            <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
		    Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks.
		</span>
		<span class="t-medium">
			In European Conference on Artificial Intelligence (ECAI), 2020 (oral presentation).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('ecai2020');">
			Abstract</a>
			</li>
			<li><a href="http://ecai2020.eu/papers/1241_paper.pdf">PDF (Conference)</a>
			<li><a href="https://arxiv.org/abs/1912.03063">Arxiv-Version</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			The large adoption of the self-attention (i.e. transformer model) and BERT-like training principles has recently resulted in a number of high performing models on a large panoply of vision-and-language problems (such as Visual Question Answering (VQA), image retrieval, etc.). In this paper we claim that these State-Of-The-Art (SOTA) approaches perform reasonably well in structuring information inside a single modality but, despite their impressive performances , they tend to struggle to identify fine-grained inter-modality relationships. Indeed, such relations are frequently assumed to be implicitly learned during training from application-specific losses, mostly cross-entropy for classification. While most recent works provide inductive bias for inter-modality relationships via cross attention modules, in this work, we demonstrate (1) that the latter assumption does not hold, i.e. modality alignment does not necessarily emerge automatically, and (2) that adding weak supervision for alignment between visual objects and words improves the quality of the learned models on tasks requiring reasoning. In particular , we integrate an object-word alignment loss into SOTA vision-language reasoning models and evaluate it on two tasks VQA and Language-driven Comparison of Images. We show that the proposed fine-grained inter-modality supervision significantly improves performance on both tasks. In particular, this new learning signal allows obtaining SOTA-level performances on GQA dataset (VQA task) with pre-trained models without finetuning on the task, and a new SOTA on NLVR2 dataset (Language-driven Comparison of Images). Finally, we also illustrate the impact of the contribution on the models reasoning by visualizing attention distributions.
		    </p>
		</div>
  	</dd>

  	<dt class="conference">c56</dt>
	<dd id="icpr2020" class="bib-arxive">
		<span class="t-author">
			<a href="https://github.com/edbeeching">Edward Beeching<a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://perso.citi-lab.fr/jdibangoy">Jilles Dibangoye</a> and
			<a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>.
			
	  	</span>
	  	<span class="t-title">
		    Deep Reinforcement Learning on a Budget: 3D Control and Reasoning Without a Supercomputer.
		</span>
		<span class="t-medium">
			To appear in International Conference on Pattern Recognition (ICPR), 2020.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2020');">
			Abstract</a>
			</li>
			<li><a href="https://arxiv.org/abs/1904.01806">ArXiv-Version</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			An important goal of research in Deep Reinforcement Learning in mobile robotics is to train agents capable of solving complex tasks, which require a high level of scene understanding and reasoning from an egocentric perspective. When trained from simulations, optimal environments should satisfy a currently unobtainable combination of high-fidelity photographic observations, massive amounts of different environment configurations and fast simulation speeds. In this paper we argue that research on training agents capable of complex reasoning can be simplified by decoupling from the requirement of high fidelity photographic observations. We present a suite of tasks requiring complex reasoning and exploration in continuous, partially observable 3D environments. The objective is to provide challenging scenarios and a robust baseline agent architecture that can be trained on mid-range consumer hardware in under 24h. Our scenarios combine two key advantages: (i) they are based on a simple but highly efficient 3D environment (ViZDoom) which allows high speed simulation (12000fps); (ii) the scenarios provide the user with a range of difficulty settings, in order to identify the limitations of current state of the art algorithms and network architectures. We aim to increase accessibility to the field of Deep-RL by providing baselines for challenging scenarios where new ideas can be iterated on quickly. We argue that the community should be able to address challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.
		    </p>
		</div>
  	</dd>

	<dt class="conference">c55</dt>
	<dd id="ecml2019" class="bib-arxive">
		<span class="t-author">
			<a href="https://perso.liris.cnrs.fr/quentin.debard/">Quentin Debard<a>,
            <a href="http://perso.citi-lab.fr/jdibangoy">Jilles Dibangoye</a>,
            <a href="http://asi.insa-rouen.fr/enseignants/~scanu/">Stéphane Canu</a> and
			<a href="../index.html">Christian Wolf</a>.
			
	  	</span>
	  	<span class="t-title">
		    Learning 3D Navigation Protocols on Touch Interfaces with Cooperative Multi-Agent Reinforcement Learning.
		</span>
		<span class="t-medium">
			In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2019.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('ecml2019');">
			Abstract</a>
			</li>
			<li><a href="https://arxiv.org/abs/1904.07802">Arxiv-Version</a></li>
      		</ul>
		<div class="t-abstract">
			<p>
			Using touch devices to navigate in virtual 3D environments such as computer assisted design (CAD) models or geographical information systems (GIS) is inherently difficult for humans, as the 3D operations have to be performed by the user on a 2D touch surface. This ill-posed problem is classically solved with a fixed and handcrafted interaction protocol, which must be learned by the user. We propose to automatically learn a new interaction protocol allowing to map a 2D user input to 3D actions in virtual environments using reinforcement learning (RL). A fundamental problem of RL methods is the vast amount of interactions often required, which are difficult to come by when humans are involved. To overcome this limitation, we make use of two collaborative agents. The first agent models the human by learning to perform the 2D finger trajectories. The second agent acts as the interaction protocol, interpreting and translating to 3D operations the 2D finger trajectories from the first agent. We restrict the learned 2D trajectories to be similar to a training set of collected human gestures by first performing state representation learning, prior to reinforcement learning. This state representation learning is addressed by projecting the gestures into a latent space learned by a variational auto encoder (VAE).
		    </p>
		</div>
  	</dd>

	<dt class="conference">c54</dt>
    <dd id="cvpr2019w" class="bib-arxive">
        <span class="t-author">
        	<a href="https://github.com/anshulpaigwar">Anshul Paigwar</a>,
            <a href="http://ozgurerkent.gforge.inria.fr">Ozgur Erkent</a>,
            <a href="../index.html">Christian Wolf</a> and
            <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>.
        </span>
        <span class="t-title">
            Attentional PointNet for 3D-Object Detection in Point Clouds.
        </span>
        <span class="t-medium">
            In CVPR Workshop on autonomuous driving, 2019.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('cvpr2019w');">
            Abstract</a>
            </li>
            <li> <a href="../papers/cvpr2019w.pdf">PDF</a></li>
            </ul>
        <div class="t-abstract">
            Accurate detection of objects in 3D point clouds is a central problem for autonomous navigation. Most existing methods use techniques of hand-crafted features representation or multi-modal approaches prone to sensor failure. Approaches like PointNet that directly operate on sparse point data have shown good accuracy in the classification of single 3D objects. However, LiDAR sensors on Autonomous Vehicles generate a large scale point cloud. Real-time object detection in such a cluttered environment still remains a challenge. In this study, we propose Attentional PointNet, which is a novel end-to-end trainable deep architecture for object detection in point clouds. We extend the theory of visual attention mechanism to 3D point clouds and introduce a new recurrent 3D Localization Network module. Rather than processing the whole point cloud, the network learns where to look (finding regions of interest), which significantly reduces the number of points to be processed and inference time. Evaluation on KITTI car detection benchmark shows that our Attentional PointNet achieves comparable results with the state-of-the-art LiDAR-based 3D detection methods in detection and speed.
        </div>
    </dd>

      <dt class="conference">c53</dt>
    <dd id="visxai2019" class="bib-arxive">
        <span class="t-author">
        	<a href="https://theo-jaunet.github.io/aboutme/">Théo Jaunet</a>,
            <a href="https://romain.vuillemot.net">Romain Vuillemot</a> and
            <a href="../index.html">Christian Wolf</a>.
        </span>
        <span class="t-title">
            What if we Reduced the Memory of an Artificial Doom Player?
        </span>
        <span class="t-medium">
            In IEEE VIS Workshop on AI Explainability (<span style="font-weight: bold;">Best paper</span>), 2019.
        </span>
        <ul class="reflink-box">            
            <li class="first"> <a href="https://theo-jaunet.github.io/MemoryReduction/">Visualization</a></li>
            </ul>       
    </dd>

	<dt class="conference">c52</dt>
    <dd id="eccv2018" class="bib-arxive">
        <span class="t-author">
            <a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
			<a href="https://nneverova.github.io/">Natalia Neverova<a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
            <a href="http://www.cs.sfu.ca/~mori">Greg Mori</a>.
        </span>
        <span class="t-title">
            Object Level Visual Reasoning in Videos.
        </span>
        <span class="t-medium">
            In European Conference on Computer Vision (ECCV) 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('arxiv2018objectlevel');">
            Abstract</a>
            </li>
            <li><a href="../papers/eccv2018.pdf">PDF (Local)</a></li>
            <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fabien_Baradel_Object_Level_Visual_ECCV_2018_paper.pdf">PDF (CV-Foundation)</a></li>
            <li><a href="https://arxiv.org/abs/1806.06157">ArXiv (Similar Content)</a></li>
            <li><a href="https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/">Project Page</a></li>
            <li><a href="https://www.youtube.com/watch?v=_xlEanOJ8NE">Video/youtube</a></li>
            <li><a href="https://fabienbaradel.github.io/masks_data/">Data</a></li>
            </ul>
        <div class="t-abstract">
            Human activity recognition is typically addressed by training models to detect key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this, requiring fine distinctions and a detailed comprehension of the interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatio-temporal interactions in videos. Key to our approach is the choice of performing this reasoning on an object level through the integration of state of the art object instance segmentation networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluated our method on three standard datasets: the Twenty-BN Something-Something dataset, the VLOG dataset and the EPIC Kitchens dataset, and achieve state of the art results on both. Finally, we also show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.
        </div>
    </dd>

    <dt class="conference">c51</dt>
    <dd id="cvpr2018" class="bib-conference">
        <span class="t-author">
            <a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a> and
            <a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>.
        </span>
        <span class="t-title">
            Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points.
        </span>
        <span class="t-medium">
            In Computer Vision and Pattern Recognition (CVPR), 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('cvpr2018');">
            Abstract</a></li>
            <li><a href="../papers/cvpr2018.pdf">PDF (Local)</a></li>
            <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">PDF (CV Foundation)</a></li>
            <li><a href="https://arxiv.org/abs/1802.07898">ArXiv-Version (longer)</a></li>
            <li><a href="../papers/poster-cvpr2018.pdf">PDF (Poster)</a></li>
            <li><a href="https://www.youtube.com/watch?v=7yPDYYhaYI4&t=1s">Video/youtube</a></li>
        </ul>
        <div class="t-abstract">
            <p>
            We propose a method for human activity recognition from RGB data which does not rely on any pose information during test time, and which does not explicitly calculate pose information internally. Instead, a visual attention module learns to predict glimpse sequences in each frame. These glimpses correspond to interest points in the scene which are relevant to the classified activities. No spatial coherence is forced on the glimpse locations, which gives the module liberty to explore different points at each frame and better optimize the process of scrutinizing visual information.
        </p><p>
Tracking and sequentially integrating this kind of unstructured data is a challenge, which we address by separating the set of glimpses from a set of recurrent 	 tracking/recognition workers.
These workers receive the glimpses, jointly performing subsequent motion tracking and prediction of the activity itself. The glimpses are soft-assigned to the workers, optimizing coherence of the assignments in space, time and feature space using an external memory module. No hard decisions are taken, i.e.~each glimpse point is assigned to all existing workers, albeit with different importance.
Our methods outperform state-of-the-art methods on the largest human activity recognition dataset available to-date; NTU RGB+D Dataset, and on a smaller human action recognition dataset Northwestern-UCLA Multiview Action 3D Dataset.
            </p>
        </div>
    </dd>

	<dt class="conference">c50</dt>
    <dd id="bmvc2018" class="bib-arxive">
        <span class="t-author">
            <a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>.
        </span>
        <span class="t-title">
            Human Activity Recognition by attending to RGB frames from deep pose features.
        </span>
        <span class="t-medium">
            In British Machine Vision Conference (BMVC), 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('arxiv2017attention');">
            Abstract</a>
            </li>
            <li><a href="https://arxiv.org/abs/1703.10106">ArXiv (Similar Content)</a></li>
            <li><a href="https://fabienbaradel.github.io/pose_rgb_attention_human_action/">Videos+Details</a></li>
            </ul>
        <div class="t-abstract">
            We address human action recognition from multi-modal video data involving articulated pose and RGB frames and propose a two-stream approach.
			The pose stream is processed with a convolutional model taking as input a 3D tensor holding data from a sub-sequence. A specific joint ordering, which respects the topology of the human body, ensures that different convolutional layers correspond to meaningful levels of abstraction.
			The raw RGB stream is handled by a spatio-temporal soft-attention mechanism conditioned on features from the pose network. An LSTM network receives input from a set of image locations at each instant. A trainable glimpse sensor extracts features on a set of predefined locations specified by the pose stream, namely the 4 hands of the two people involved in the activity.
			Appearance features give important cues on hand motion and on objects held in each hand.
			We show that it is of high interest to shift the attention to different hands at different time steps depending on the activity itself.
			Finally a temporal attention mechanism learns how to fuse LSTM features over time. State-of-the-art results are achieved on the largest dataset for human activity recognition, namely NTU-RGB+D.
        </div>
    </dd>

	<dt class="conference">c49</dt>
    <dd id="iros2018" class="bib-conference">
        <span class="t-author">
            <a href="http://ozgurerkent.gforge.inria.fr">Ozgur Erkent</a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>,
            <a href="http://www.davidsgonzalez.com/">David Sierra Goncalez</a> and
            <a href="https://scholar.google.com/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero Cano</a>.
        </span>
        <span class="t-title">
            Semantic Grid Estimation with a Hybrid Bayesian and Deep Neural Network Approach.
        </span>
        <span class="t-medium">
            In International Conference on Intelligent Robots (IROS), 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('iros2018');">
            Abstract</a></li>
           	</li>
            <li><a href="../papers/iros2018.pdf">PDF</a></li>
        </ul>
        <div class="t-abstract">
            <p>
            In an autonomous vehicle setting, we propose a method for the estimation of a semantic grid, i.e. a bird's eye grid centered on the car's position and aligned in its driving direction, which contains high-level semantic information on the environment and its actors. Each grid cell contains a semantic label with divers classes, as for instance {Road, Vegetation, Building, Pedestrian, Car  ...}.
        	</p><p>
			We propose a hybrid approach, which combines the advantages of two different methodologies: we use Deep Learning to perform semantic segmentation on monocular RGB images with supervised learning from labeled groundtruth data. We combine these segmentations with occupancy grids calculated from LIDAR data using a generative Bayesian particle filter. The fusion itself is carried out with a deep network, which learns to integrate geometric information from the LIDAR with semantic information from the RGB data.
			</p><p>
			We tested our method on two datasets, namely the KITTI dataset, which is publicly available and widely used, and our own dataset obtained from with our own platform, a Renault ZOE equipped with a LIDAR and various sensors. We largely outperform baselines which calculate the semantic grid either from the RGB image alone or from LIDAR output alone, showing the interest of this hybrid approach.
            </p>
        </div>
    </dd>

	<dt class="conference">c48</dt>
    <dd id="robocup2018" class="bib-conference">
        <span class="t-author">
            Fabrice Jumel, Jacques Saraydaryan, Raphael Leber, Laetitia Matignon, Eric Lombardi,
            <a href="../index.html">Christian Wolf</a>
            and Olivier Simonin.
        </span>
        <span class="t-title">
            Context Aware Robot Architecture,
			Application to the RoboCup@Home Challenge.
        </span>
        <span class="t-medium">
            Robocup Symposium, 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('robocup2018');">
            Abstract</a></li>
           	</li>
        </ul>
        <div class="t-abstract">
            <p>
            This paper presents an architecture dedicated to the orchestration of high level abilities of a humanoid robot, such as a Pepper, which must perform some tasks as the ones proposed in the RoboCup@Home competition. We present the main abilities that a humanoid service robot should provide. We choose to build them based  on recent methodologies linked to social navigation and deep learning. We detail the architecture, on how high level abilities are connected with low level sub-functions. Finally we present first experimental results with a Pepper humanoid.
        </p><p>
            </p>
        </div>
    </dd>

	<dt class="conference">c47</dt>
    <dd id="fg2018" class="bib-conference">
        <span class="t-author">
            <a href="https://liris.cnrs.fr/membres?idn=qdebard">Quentin Debard<a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="http://asi.insa-rouen.fr/enseignants/~scanu/">Stéphane Canu</a> and
            <a href="https://fr.linkedin.com/in/julien-arn%C3%A9-08829b100">Julien Arne</a>.
        </span>
        <span class="t-title">
            Learning to recognize touch gestures: recurrent vs. convolutional features and dynamic sampling.
        </span>
        <span class="t-medium">
            In International Conferene on Automatic Face and Gesture Recognition (FG), oral presentation, 2018.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('fg2018');">
            Abstract</a></li>
            <li><a href="../papers/fg2018.pdf">PDF</a></li>
            <li><a href="https://arxiv.org/abs/1802.09901">ArXiv-Version (longer)</a></li>
        </ul>
        <div class="t-abstract">
            <p>
                We propose a fully automatic method for learning gestures on big touch devices in a potentially multi-user context. The goal is to learn general models capable of adapting to different gestures, user styles and hardware variations (e.g. device sizes, sampling frequencies and regularities).
            </p>
            <p>
                Based on deep neural networks, our method features a novel dynamic sampling and temporal normalization component, transforming variable length gestures into fixed length representations while preserving finger/surface contact transitions, that is, the topology of the signal. This sequential representation is then processed with a convolutional model capable, unlike recurrent networks, of learning hierarchical representations with different levels of abstraction.
            </p>
            <p>
            To demonstrate the interest of the proposed method, we introduce a new touch gestures dataset with 6758 gestures performed by 27 people, which is, up to our knowledge, the first of its kind: a publicly available multi-touch gesture dataset for interaction.
			We also tested our method on a standard dataset in symbolic touch gesture recognition, the MMG dataset, outperforming the state of the art and reporting close to perfect performance.
			</p>
        </div>

    <dt class="conference">c46</dt>
    <dd id="iccv2017w" class="bib-conference">
        <span class="t-author">
            <a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
            <a href="../index.html">Christian Wolf</a>,
            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>.
        </span>
        <span class="t-title">
            Human Action Recognition: Pose-based Attention draws focus to Hands.
        </span>
        <span class="t-medium">
            In ICCV Workshop on Hands in Action, 2017.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('iccv2017w');">
            Abstract</a></li>
            <li><a href="../papers/iccv2017ws.pdf">PDF</a></li>
        </ul>
        <div class="t-abstract">
            <p>
                We propose a new spatio-temporal attention based mechanism for human action recognition able to automatically attend to most important human hands and detect the most discriminative moments in an action.
    Attention is handled in a recurrent manner employing Recurrent Neural Network (RNN) and is fully-differentiable.
    In contrast to standard soft-attention based mechanisms, our approach does not use the hidden RNN state as input to the attention model.
    Instead, attention distributions are drawn using external information: human articulated pose.
    We performed an extensive ablation study to show the strengths of this approach and we particularly studied the conditioning aspect of the attention mechanism.
            </p>
            <p>
                We evaluate the method on the largest currently available human action recognition dataset, NTU-RGB+D, and report state-of-the-art results. Another advantage of our model are certains aspects of explanability, as the spatial and temporal attention distributions at test time allow to study and verify on which parts of the input data the method focuses.
            </p>
        </div>
    </dd>



    <dt class="conference">c45</dt>
    <dd id="bmvc2017" class="bib-conference">
        <span class="t-author">
            <a href="">Damien Fourure</a>,
            <a href="http://home.heeere.com/">Remi Emonet</a>,
            <a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>,
            <a href="http://perso.univ-st-etienne.fr/muda8804/">Damien Muselet</a>,
            <a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp">Alain Trémeau</a>,
            <a href="../index.html">Christian Wolf</a>.
        </span>
        <span class="t-title">
            Residual Conv-Deconv Grid Network for Semantic Segmentation.
        </span>
        <span class="t-medium">
            In British Machine Vision Conference (BMVC), 2017.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('bmvc2017');">
            Abstract</a>
            </li>
            <li><a href="../papers/bmvc2017.pdf">PDF</a></li>
            <li><a href="https://arxiv.org/abs/1707.07958">ArXiv-Version</a></li>
            </ul>
            </ul>
        <div class="t-abstract">
            This paper presents GridNet, a new Convolutional Neural Network (CNN) architecture for semantic image segmentation (full scene labelling). Classical neural networks are implemented as one stream from the input to the output with subsampling operators applied in the stream in order to reduce the feature maps size and to increase the receptive field for the final prediction. However, for semantic image segmentation, where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction.
            To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks. GridNet is trained from scratch and achieves competitive results on the Cityscapes dataset.
        </div>
    </dd>

    <dt class="conference">c44</dt>
    <dd id="icdar2017" class="bib-arxive">
        <span class="t-author">
            <a href="https://liris.cnrs.fr/membres?idn=bmoysset">Bastien Moysset</a>,
            <a href="https://fr.linkedin.com/pub/christopher-kermorvant/2/58b/871">Christopher Kermorvant</a>,
            <a href="../index.html">Christian Wolf</a>.
        </span>
        <span class="t-title">
            Full-Page Text Recognition: Learning Where to Start and When to Stop.
        </span>
        <span class="t-medium">
            In International Conference on Document Analysis and Recognition (ICDAR), 2017.
        </span>
        <ul class="reflink-box">
            <li class="first">
            <a href="javascript:;" onClick="displayAbstractOf('icdar2017');">
            Abstract</a>
            </li>
            <li><a href="../papers/icdar2017.pdf">PDF</a></li>
            </ul>
        <div class="t-abstract">
            <p>
            Text line detection and localization is a crucial step for full page document analysis, but still suffers from heterogeneity of real life documents. In this paper, we present a new approach for full page text recognition. Localization of the text lines is based on regressions with Fully Convolutional Neural Networks and Multidimensional Long Short-Term Memory as contextual layers.
            <p>
            </p>
            In order to increase the efficiency of this localization method, only the position of the left side of the text lines are predicted. The text recognizer is then in charge of predicting the end of the text to recognize. This method has shown good results for full page text recognition on the highly heterogeneous Maurdor dataset.
            </p>
        </div>
    </dd>

	<dt class="conference">c43</dt>
	<dd id="fg2017" class="bib-journal">
		<span class="t-author">
			<a href="https://scholar.google.com/citations?user=YR7UdqsAAAAJ&hl=en">Fan Li</a>,
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>
	  	</span>
	  	<span class="t-title">
		    Modout: Learning Multi-Modal Architectures by Stochastic Regularization.
		</span>
		<span class="t-medium">
			In International Conference on Automatic Face and Gesture Recognition (FG), 2017.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('fg2017');">
			Abstract</a>
			</li>
			<li><a href="../papers/fg2017.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			Model selection methods based on stochastic regularization such as
  Dropout have been widely used in deep learning due to their
  simplicity and effectiveness. The standard Dropout method treats all
  units, visible or hidden, in the same way, thus ignoring any \emph{a
    priori} information related to grouping or structure. Such
  structure is present in multi-modal learning applications such as
  affect analysis and gesture recognition, where
  subsets of units may correspond to individual modalities. In this
  paper we describe Modout, a model selection method based on
  stochastic regularization, which is particularly useful in the
  multi-modal setting. Different from previous methods, it is capable
  of learning whether or when to fuse two modalities in a layer, which
  is usually considered to be an architectural hyper-parameter by deep
  learning researchers and practitioners. Modout is evaluated on one
  synthetic and two real multi-modal datasets.
  The results indicate improved performance compared to other
  stochastic regularization methods. The result on the Montalbano
  dataset shows that learning a fusion structure by Modout is on par
  with a state-of-the-art carefully designed architecture.
		</div>
  	</dd>
</dl>

<dl>
	<dt class="conference">c42</dt>
  	<dd id="sspr2016" class="bib-conf">
		<span class="t-author">
			  <a href="">Damien Fourure</a>,
			  <a href="http://home.heeere.com/">Remi Emonet</a>,
			  <a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>,
			  <a href="http://perso.univ-st-etienne.fr/muda8804/">Damien Muselet</a>,
			  <a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp">Alain Trémeau</a>,
			  <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
			Semantic Segmentation via Multi-task, Multi-domain Learning
		</span>
		<span class="t-medium">
			In joint IAPR International Workshops on Structural and Syntactic Pattern Recognition (SSPR 2016) and Statistical Techniques in Pattern Recognition (SPR 2016).
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('sspr2016');">
			Abstract</a>
			</li>
			<li><a href="../papers/sspr2016.pdf">PDF</a></li>
      	</ul>
			<div class="t-abstract">
				 We present an approach that leverages multiple datasets possibly
 annotated using different classes to improve
 the semantic segmentation accuracy on each
 individual dataset. We propose a new
 selective loss function that can be integrated into deep
 networks to exploit training data coming from multiple datasets with
 possibly different tasks (e.g., different label-sets). We show how
 the gradient-reversal approach for domain adaptation can be used in this setup.
 Thorought experiments on semantic segmentation applications show the relevance of our approach.
		</div>
  	</dd>

	<dt class="conference">c41</dt>
  	<dd id="icfhr2016" class="bib-conf">
		<span class="t-author">
			<a href="https://liris.cnrs.fr/membres?idn=bmoysset">Bastien Moysset</a>,
			<a href="http://www.mendeley.com/profiles/jerome-louradour">Jérome Louradour</a>,
			<a href="https://fr.linkedin.com/pub/christopher-kermorvant/2/58b/871">Christopher Kermorvant</a>,
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
	  		Learning text-line localization with shared and local regression neural networks.
		</span>
		<span class="t-medium">
			In International Conference on Frontiers in Handwriting Recognition, 2016.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icfhr2016');">
			Abstract</a>
			</li>
        <li> <a href="../papers/icfhr2016.pdf">PDF</a></li>
		</ul>
		<div class="t-abstract">
			Text line detection and localisation is a crucial step for full page document analysis, but still suffers from heterogeneity of real life documents. In this paper, we present a novel approach for text line localisation based on Convolutional Neural Networks and Multidimensional Long Short-Term Memory cells as a regressor in order to predict the coordinates of the text line bounding boxes directly from the pixel values. Targeting typically large images in document image analysis, we propose a new model using weight sharing over local blocks. We compare two strategies: directly predicting the four coordinates or predicting lower-left and upper-right points separately followed by matching. We evaluate our work on the highly unconstrained Maurdor dataset and show that our method outperforms both other machine learning and image processing methods.
		</div>
  	</dd>

	<dt class="conference">c40</dt>
  	<dd id="icip2016" class="bib-conf">
		<span class="t-author">
			  <a href="">Damien Fourure</a>,
			  <a href="http://home.heeere.com/">Remi Emonet</a>,
			  <a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>,
			  <a href="http://perso.univ-st-etienne.fr/muda8804/">Damien Muselet</a>,
			  <a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp">Alain Trémeau</a>,
			  <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
			Mixed pooling Neural Networks for Color Constancy.
		</span>
		<span class="t-medium">
			In International Conference on Image Processing (ICIP), 2016.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icip2016');">
			Abstract</a>
			</li>
			<li><a href="../papers/icip2016.pdf">PDF</a></li>
      	</ul>
			<div class="t-abstract">
				Color constancy is the ability of the human visual system to perceive constant colors for a surface despite changes in the spectrum of the illumination. In computer vision, the main approach consists in estimating the illuminant color and then to remove its impact on the color of the objects. Many image processing algorithms have been proposed to tackle this prob- lem automatically. However, most of these approaches are handcrafted and mostly rely on strong empirical assumptions, e.g., that the average reflectance in a scene is gray. State- of-the-art approaches can perform very well on some given datasets but poorly adapt on some others. In this paper, we have investigated how neural networks-based approaches can be used to deal with the color constancy problem. We have proposed a new network architecture based on existing suc- cessful hand-crafted approaches and a large number of im- provements to tackle this problem by learning a suitable deep model. We show our results on most of the standard bench- marks used in the color constancy domain.
		</div>
  	</dd>

	<dt class="conference">c39</dt>
	<dd id="icdar2015" class="bib-conf">
		<span class="t-author">
			<a href="https://liris.cnrs.fr/membres?idn=bmoysset">Bastien Moysset</a>,
			<a href="https://fr.linkedin.com/pub/christopher-kermorvant/2/58b/871">Christopher Kermorvant</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.mendeley.com/profiles/jerome-louradour">Jérome Louradour</a>.
	  	</span>
	  	<span class="t-title">
	  		Paragraph text segmentation into lines with Recurrent Neural Networks.
		</span>
		<span class="t-medium">
			In International Conference on Document Analysis and Recognition (ICDAR), 2015.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icdar2015');">
			Abstract</a>
			</li>
      <li><a href="../papers/icdar2015.pdf">PDF</a></li>
		</ul>
		<div class="t-abstract">
			The detection of text lines, as a first processing step,
is critical in all Text Recognition systems.
State-of-the-art methods to locate lines of text
are based on handcrafted heuristics fine-tuned by the Image Processing Community's experience.
They succeed under certain constraints; for instance the background has to be roughly uniform.
We propose to use more ``agnostic'' Machine Learning-based approaches to address text line location.
The main motivation is to be able to process either damaged documents,
or flows of documents with a high variety of layouts and other characteristics.
A new method is presented in this work, inspired by the latest generation of optical models used for Text Recognition,
namely Recurrent Neural Networks.
As these models are sequential, a column of text lines in our application plays here the same role as a line of characters in more traditional text recognition settings.
A key advantage of the proposed method over other data-driven approaches is that
compiling a training dataset does not require labeling line boundaries:
only the number of lines are required for each paragraph.
Experimental results show that our approach gives similar or better results than traditional handcrafted approaches, with little engineering efforts and less hyper-parameter tuning.
		</div>
  	</dd>

	<dt class="conference">c38</dt>
  	<dd id="icdar2015hip" class="bib-conf">
		<span class="t-author">
			<a href="https://liris.cnrs.fr/membres?idn=bmoysset">Bastien Moysset</a>,
			Pierre Adam,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.mendeley.com/profiles/jerome-louradour">Jérome Louradour</a>.
	  	</span>
	  	<span class="t-title">
	  		Space Displacement Localization Neural Networks to locate origin points of handwritten text lines in historical documents.
		</span>
		<span class="t-medium">
			In ICDAR Workshop on Historical Document Imaging and Processing, 2015.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icdar2015hip');">
			Abstract</a>
			</li>
			<li><a href="../papers/icdar-hip2015.pdf">PDF</a></li>
		</ul>
		<div class="t-abstract">
			We describe a new method for detecting and localizing multiple objects in an image using context aware deep neural networks. Common architectures either proceed locally per pixel-wise sliding-windows, or globally by predicting object localizations for a full image. We improve on this by training a semi-local model to detect and localize objects inside a large image region, which covers an object or a part of it. Context knowledge is integrated, combining multiple predictions for different regions through a spatial context layer modeled as an LSTM network. The proposed method is applied to a complex problem in historical document image analysis, where we show that is capable of robustly detecting text lines in the images from the ANDAR-TL competition. Experiments indicate that the model can cope with difficult situations and reach the state of the art in Vision such as other deep models.
		</div>
  	</dd>

  	<dt class="conference">c37</dt>
	<dd id="icip2015" class="bib-conf">
		<span class="t-author">
			<a href="http://personel.gsu.edu.tr/en/en-emre-dogan">Emre Dogan</a>,
			<a href="http://www.goneneren.com/">Gonen Eren</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>.
	  	</span>
	  	<span class="t-title">
	  		Activity recognition with volume motion templates and histograms of 3D gradients.
		</span>
		<span class="t-medium">
			In International Conference on Image Processing (ICIP), 2015.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icip2015');">
			Abstract</a>
			</li>
			<li><a href="../papers/cviu2014.pdf">PDF</a></li>
		</ul>
		<div class="t-abstract">
			We propose a new method for activity recognition based on a view independent representation of human motion. Robust 3D volume motion templates (VMTs) are calculated from tracklets. View independence is achieved through a rotation with respect to a canonical orientation. From this volumes, features based on 3D gradients are extracted, projected to a codebook and pooled into a bags-of-words model classified with an SVM classifier. Experiments show that the method outperforms the original HoG3D method.
		</div>
  	</dd>


	<dt class="conference">c36</dt>
  	<dd id="arso2015" class="bib-conf">
		<span class="t-author">
			Leslie Guillaume,
			<a href="https://www.liglab.fr/evenements/keynote-speeches/veronique-auberge-affecte-affectueux-affectif-quand-l-affective">Véronique Aubergé</a>,
			Romain Magnani, Frédéric Aman, Cécile Cottier, Yuko Sasa,
			<a href="../index.html">Christian Wolf</a>,
			Florian Nebout,
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			Nicolas Bonnefond, Amaury Negre, Liliya Tsvetanova, Maxence Girard-Rivier.
	  	</span>
	  	<span class="t-title">
	  		Gestural HRI in an ecological dynamic experiment: the GEE corpus based approach for the Emox robot.
		</span>
		<span class="t-medium">
			In International Workshop on Advanced Robotics and its Social Impacts
			(ARSO), 2015.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arso2015');">
			Abstract</a>
			</li>
			<li><a href="../papers/arso2015.pdf">PDF</a></li>
		</ul>
		<div class="t-abstract">
			As part of a human-robot interaction project, the gestural modality is one of a possible way to communicate. In order to develop a relevant gesture recognition system associated to a smart home butler robot, our methodology is based on an IQ game-like Wizard of Oz experiment to collect spontaneous and implicitly produced gestures in an ecological context where the robot is the referee of the game. These gestures are compared with explicitly produced gestures to determine a relevant ontology of gestures. This preliminary qualitative analysis will be the base to build a big data corpus in order to optimize acceptance of the gesture dictionary in coherence with the “socio-affective glue” dynamics.
		</div>
  	</dd>

  	<dt class="conference">c35</dt>
  	<dd id="hriw2015" class="bib-conf">
		<span class="t-author">
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a>,
			<a href="http://www.gipsa-lab.grenoble-inp.fr/page_pro.php?vid=1752">Alaeddine Mihoub</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~frederic.elisei/">Frédéric Elisei</a>.
	  	</span>
	  	<span class="t-title">
			Learning joint multimodal behaviors for face-to-face interaction: performance & properties of statistical models.
		</span>
		<span class="t-medium">
		      In HRI Workshop on Behavior Coordination between Animals, Humans, and Robots,
		</span>
		2015.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('hriw2015');">Abstract</a></li>
        	<li><a href="../papers/hriw2015.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
			We  evaluate  here  the  ability  of  statistical  models,
  namely  Hidden  Markov   Models   (HMMs)   and   Dynamic   Bayesian   Networks
(DBNs),   in   capturing   the   interplay   and  coordination
   between multimodal behaviors of two individuals involved in
 a face-to-face interaction.  We  structure  the  intricate  sensory-mot
or  coupling  of the  joint  multimodal  scores  by  segmenting  the  whole
  interaction into  so-called  interaction  units  (IU).  We  show  that
  the  proposed statistical  models  are  able  to  capture  the  natural
dynamics  of  the interaction and that DBNs are particularly suitable
 for reproducing original distributions of so-called coordination histograms.
		</p>
		</div>
  	</dd>

  	<dt class="conference">c34</dt>
	<dd id="bmvc2014" class="bib-conf">
		<span class="t-author">
			  <a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>,
			  <a href="http://home.heeere.com/">Remi Emonet</a>,
			  Taygun Kekec,
			  <a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp">Alain Trémeau</a>,
			  <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
			Contextually Constrained Deep Networks for Scene Labeling.
		</span>
		<span class="t-medium">
			In British Machine Vision Conference (BMVC), 2014.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('bmvc2014');">
			Abstract</a>
			</li>
			<li><a href="../papers/bmvc2014.pdf">PDF</a></li>
      	</ul>
			<div class="t-abstract">
				Learning using deep learning architectures is a difficult problem: the complexity of
the prediction model and the difficulty of solving non-convex optimization problems inherent in most learning algorithms can both lead to overfitting phenomena and bad local optima. To overcome these problems we would like to constraint parts of the network using some semantic context to 1) control its capacity while still allowing complex func- tions to be learned 2) obtain more meaningful layers. We first propose to learn a weak convolutional network which would provide us rough label maps over the neighborhood of a pixel. Then, we incorporate this weak learner in a bigger network. This iterative process aims at increasing the interpretability by constraining some feature maps to learn precise contextual information. Using Stanford and SIFT Flow scene labeling datasets,
we show how this contextual knowledge improves accuracy of state-of-the-art architectures. The approach is generic and can be applied to similar networks where contextual cues are available at training time.
		</div>
  	</dd>

  	<dt class="conference">c33</dt>
  	<dd id="accv2014" class="bib-conf">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>,
			Florian Nebout.
	  	</span>
	  	<span class="t-title">
		    Hand segmentation with structured convolutional learning
		</span>
		<span class="t-medium">
			In Asian Conference on Computer Vision (ACCV), 2014.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('accv2014');">
			Abstract</a>
			</li>
			<li><a href="../papers/accv2014.pdf">PDF</a></li>
			<li><a href="../graphics/animated_handpose.gif">Animated-gif</a></li>
      		</ul>
		<div class="t-abstract">
			The availability of cheap and effective depth sensors has resulted
  in recent advances in human pose estimation and tracking. Detailed
  estimation of hand pose, however, remains a challenge since fingers
  are often occluded and may only represent just a few
  pixels. Moreover, labelled data is difficult to obtain. We propose a
  deep learning based-approach for hand pose estimation, targeting
  gesture recognition, that requires very little labelled data. It
  leverages both unlabeled data and synthetic data from
  renderings. The key to making it work is to integrate structural
  information not into the model architecture, which would slow down
  inference, but into the training objective. We show that adding
  unlabelled real-world samples significantly improves results
  compared to a purely supervised setting.
		</div>
  	</dd>

  	<dt class="conference">c32</dt>
  	<dd id="eccv2014ws" class="bib-conf">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>,
			Florian Nebout.
	  	</span>
	  	<span class="t-title">
		    Multi-scale deep learning for gesture detection and localization
		</span>
		<span class="t-medium">
			In ECCV ChaLearn Workshop on Looking at People, 2014.
		</span>
			(<i>This paper describes the winning entry of the ChaLearn 2014 gesture recognition competition</i>)
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('eccv2014ws');">
			Abstract</a>
			</li>
			<li><a href="../papers/eccv2014ws.pdf">PDF</a></li>
			<li><a href="../papers/eccv2014ws-pres.pdf">Presentation-PDF</a></li>
      		</ul>
		<div class="t-abstract">
			We present a method for gesture detection and localization
based on multi-scale and multi-modal deep learning. Each visual
modality captures spatial information at a particular spatial scale
(such as motion of the upper body or a hand), and the whole system
operates at two temporal scales. Key to our technique is a
 training strategy which exploits i) careful initialization of
 individual modalities; and ii) gradual fusion of modalities from strongest to weakest cross-modality
 structure. We present experiments on the "ChaLearn 2014 Looking
   at People Challenge" gesture recognition track, in which we placed
 first out of 17 teams.
		</div>
  	</dd>

  	<dt class="conference">c31</dt>
	<dd id="hai2014" class="bib-conf">
		<span class="t-author">
			<a href="http://www.gipsa-lab.grenoble-inp.fr/page_pro.php?vid=1752">Alaeddine Mihoub</a>,
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a> and
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
			Modeling Perception-Action Loops: Comparing Sequential Models with Frame-Based Classifiers.
		</span>
		<span class="t-medium">
		      In ACM Human-Agent Interaction,
		</span>
		2014.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('hai2014');">Abstract</a></li>
        	<li><a href="../papers/hai2014.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
			Modeling multimodal perception-action loops in face-to- face interactions is a crucial step in the process of building sensory-motor behaviors for social robots or users-aware Embodied Conversational Agents (ECA). In this paper, we compare trainable behavioral models based on sequential models (HMMs) and classifiers (SVMs and Decision Trees) inherently inappropriate to model sequential aspects. These models aim at giving pertinent perception/action skills for robots in order to generate optimal actions given the perceived actions of others and joint goals. We applied these models to parallel speech and gaze data collected from interacting dyads. The challenge was to predict the gaze of one subject given the gaze of the interlocutor and the voice activity of both. We show that Incremental Discrete HMM (IDHMM) generally outperforms classifiers and that injecting input context in the modeling process significantly improves the performances of all algorithms.
		</p>
		</div>
  	</dd>

  	<dt class="conference">c30</dt>
  	<dd id="epirob2014" class="bib-conf">
		<span class="t-author">
			  <a href="http://liris.cnrs.fr/simon.gay">Simon Gay</a>,
		      <a href="http://http://www.oliviergeorgeon.com">Olivier Georgeon,</a>
		      <a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
				Autonomous object modeling based on affordances for spatial organization of behavior.
		</span>
		<span class="t-medium">
			In International joint conference on development and learning and on epigenetic robotics, 2014.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('epirob2014');">
			Abstract</a>
		</li>
			<li><a href="../papers/epirob2014.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			We present an architecture for self-motivated agents to organize their behaviors in space according to possibilities of interactions afforded by initially unknown objects. The long-term goal is to design agents that construct their own knowledge of objects through experience, rather than exploiting
			precoded knowledge. Self-motivation is defined here as a tendency to experiment and to respond to behavioral opportunities afforded by the environment. Some interactions have predefined valences that specify inborn behavioral preferences. Over time, the agent learns the relation between its perception of objects and the interactions that they afford, in the form of data structures, called signatures of interaction, which encode the minimal spatial configurations that afford an interaction. The agent keeps track of enacted interactions in a topological spatial memory, to recognize and localize subsequent possibilities of interaction (through their signatures) afforded by surrounding objects. Experiments with a simulated agent and a robot show that they learn to navigate in their environment, taking into account multiple surrounding objects, reaching or avoiding objects according to the valence of the interactions that they afford.
		</div>
  	</dd>

  	<dt class="conference">c29</dt>
	<dd id="iccv2013ws" class="bib-conf">
		<span class="t-author">
			<a href="https://nneverova.github.io">Natalia Neverova</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.istc.cnr.it/people/giulio-paci">Giulio Paci</a>,
			<a href="http://www.istc.cnr.it/people/giacomo-sommavilla">Giacomo Sommavilla</a>,
			<a href="http://www.uoguelph.ca/~gwtaylor">Graham W. Taylor</a>,
			Florian Nebout.
	  	</span>
	  	<span class="t-title">
		    A  multi-scale approach  to  gesture  detection  and  recognition.
		</span>
		<span class="t-medium">
			In ICCV Workshop on Understanding Human Activities: Context and Interactions, 2013.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('iccv2013ws');">
			Abstract</a>
		</li>
		<li><a href="../papers/iccv2013ws.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			We propose a generalized approach to human gesture recognition based on multiple data modalities such as depth video, articulated pose and speech. In our system, each gesture is decomposed into large-scale body motion and local subtle movements such as hand articulation. The idea of learning at multiple scales is also applied to the temporal dimension, such that a gesture is considered as a set of characteristic motion impulses, or dynamic poses. Each modality is first processed separately in short spatio-temporal blocks, where discriminative data-specific features are either manually extracted or learned. Finally, we employ a Recurrent Neural Network for modeling large-scale temporal dependencies, data fusion and ultimately gesture classification.
			Our experiments on the 2013 Challenge on Multi-modal Gesture Recognition dataset have demonstrated that using multiple modalities at several spatial and temporal scales leads to a significant increase in performance allowing the model to compensate for errors of individual classifiers as well as noise in the separate channels.
		</div>
  	</dd>

  	<dt class="conference">c28</dt>
	<dd id="acmmm2013" class="bib-conf">
		<span class="t-author">
			<a href="http://www.busim.ee.boun.edu.tr/~oya/">Oya Celiktutan</a>,
			 Akgül Ceyhun burak,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>.
	  	</span>
	  	<span class="t-title">
			Graph-Based Analysis of Physical Exercise Actions.
		</span>
		<span class="t-medium">
			In the Proceedings of the ACM Multimedia Workshop on Multimedia Indexing and Information Retrieval for Healthcare, 2013.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('acmmm2013');">
			Abstract</a>
		</li>
		<li><a href="../papers/acmmiirh2013.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			In this paper, we develop a graph-based method to align two dynamic sequences, and apply it to both action recognition tasks as well as to the objective quantification of the goodness of the action performance. The automated measurement of “action quality" has potential to be used to monitor action imitations, for example, during a physical therapy. We seek matches between a query sequence and model sequences selected with graph mining. The best matches are obtained through minimizing an energy function that jointly measures space and time domain discrepancies. This graph discrepancy measure has been used for recognizing actions, for separating acceptable and unacceptable action performances, or as a continuous quantification of the action performance goodness. Experimental evaluations demonstrate the improved results of our scheme vis-à-vis its nearest competitors. Furthermore, a plausible relationship has been obtained between action perturbation, given by the joint noise variances, and quality measure, given by matching energies averaged over a sequence.
		</div>
  	</dd>

	<dt class="conference">c27</dt>
	<dd id="epirob2013" class="bib-conf">
		<span class="t-author">
		      <a href="http://http://www.oliviergeorgeon.com">Olivier Georgeon,</a>
		      <a href="../index.html">Christian Wolf</a>,
		      <a href="http://liris.cnrs.fr/simon.gay">Simon Gay</a>.
	  	</span>
	  	<span class="t-title">
			An Enactive Approach to Autonomous Agent and Robot Learning.
		</span>
		<span class="t-medium">
			In the Proceedings of the international joint conference on development and learning and on epigenetic robotics, 2013.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('epirob2013');">
			Abstract</a>
		</li>
      		</ul>
		<div class="t-abstract">
			A novel way to model autonomous learning in artificial agents and robots is introduced, called an Enactive Markov Decision Process (EMDP). An EMDP keeps perception and action embedded within sensorimotor schemes rather than dissociated. On each decision cycle, the agent tries to enact a sensorimotor scheme, and the environment informs the agent whether it was indeed enacted or whether another sensorimotor scheme was enacted instead. This new modeling approach leads to implementing a new form of self-motivation called interactional motivation. An EMDP learning algorithm is presented. Results show that this algorithm allows the agent to develop active perception as it learns to master the sensorimotor contingences afforded by its coupling with the environment.
		</div>
  	</dd>

	<dt class="conference">c26</dt>
	<dd id="visapp2013" class="bib-conf">
		<span class="t-author">
		      <a href="http://dblp.uni-trier.de/pers/hd/j/Jiu:Mingyuan">Mingyuan Jiu,</a>
		      <a href="../index.html">Christian Wolf</a>,
		      <a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>.
	  	</span>
	  	<span class="t-title">
			Integrating spatial layout of object parts into classification without pairwise terms: application to fast body parts estimation from depth images.
		</span>
		<span class="t-medium">
			In the Proceedings of the international conference on computer vision theory and applications (Visapp), oral presentation, 2013.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('visapp2013');">
			Abstract</a>
		</li>
		<li><a href="../papers/visapp2013.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			Object recognition or human pose estimation methods often resort to a decomposition into a collection of parts. This local representation has significant advantages, especially in case of occlusions and when the “object” is non-rigid. Detection and recognition requires modelling the appearance of the different object parts as well as their spatial layout. The latter can be complex and requires the minimization of complex energy functions, which is prohibitive in most real world applications and therefore often omitted. However, ignoring the spatial layout puts all the burden on the classifier, whose only available information is local appearance. We propose a new method to integrate the spatial layout into the parts classification without costly pairwise terms. We present an application to body parts classification for human pose estimation.
		</div>
  	</dd>

	<dt class="conference">c25</dt>
	<dd id="hbu2013" class="bib-conf">
		<span class="t-author">
			<a href="http://www.gipsa-lab.grenoble-inp.fr/page_pro.php?vid=1752">Alaeddine Mihoub</a>,
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a> and
			<a href="../index.html">Christian Wolf</a>.
	  	</span>
	  	<span class="t-title">
			Social behavior modeling based on Incremental Discrete Hidden Markov Models.
		</span>
		<span class="t-medium">
		      In the Proceedings of the International Workshop on Human Behavior Understanding,
		</span>
		2013.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('hbu2013');">Abstract</a></li>
		<li><a href="../papers/hbu2013.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		   Modeling multimodal face-to-face interaction is a crucial step in the process of building
		   social robots or users-aware Embodied Conversational Agents (ECA). In this context, we present
		   a novel approach for human behavior analysis and generation based on what we called
		   “Incremental Discrete Hidden Markov Model” (IDHMM). Joint multimodal activities of
		   interlocutors are first modeled by a set of DHMMs that are specific to supposed joint
		   cognitive states of the interlocutors. Respecting a task-specific syntax, the IDHMM is
		   then built from these DHMMs and split into i) a recognition model that will determine
		   the most likely sequence of cognitive states given the multimodal activity of the
		   interlocutor, and ii) a generative model that will compute the most likely activity
		   of the speaker given this estimated sequence of cognitive states. Short-Term Viterbi
		   (STV) decoding is used to incrementally recognize and generate behavior.
		   The proposed model is applied to parallel speech and gaze data of interacting dyads.
		</p>
		</div>
  	</dd>

  	<dt class="conference">c24</dt>
	<dd id="bmvc2012" class="bib-conf">
		<span class="t-author">
		      <a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			Christophe Garcia,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			Spatio-Temporal Convolutional Sparse Auto-Encoder for Sequence Classification.
		</span>
		<span class="t-medium">
			In the Proceedings of the British Machine Vision Conference (BMVC), oral presentation, 2012.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('bmvc2012');">
			Abstract</a>
		</li>
		<li><a href="../papers/bmvc2012.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			We present in this paper a novel learning-based approach for video sequence classiﬁcation. Contrary to the dominant methodology, which relies on hand-crafted features that are manually engineered to be optimal for a speciﬁc task, our neural model automatically learns a sparse shift-invariant representation of the local 2D+t salient information, without any use of prior knowledge. To that aim, a spatio-temporal convolutional sparse auto-encoder is trained to project a given input in a feature space, and to reconstruct it from its projection coordinates. Learning is performed in an unsupervised manner by minimizing a global parametrized objective function. The sparsity is ensured by adding a sparsifying logistic between the encoder and the decoder, while the shift-invariance is handled by including an additional hidden variable to the objective function. The temporal evolution of the obtained sparse features is learned by a long short-term memory recurrent neural network rained to classify each sequence. We show that, since the feature learning process is problem-independent, the model achieves outstanding performances when applied to two different problems, namely human action and facial expression recognition. Obtained results are superior to the state of the art on the GEMEP-FERA dataset and among the very best on the KTH dataset.
		</div>
  	</dd>

  	<dt class="conference">c23</dt>
	<dd id="icpr2012" class="bib-conf">
		<span class="t-author">
		      <a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			Christophe Garcia,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			Sparse Shift-Invariant Representation of Local 2D Patterns and Sequence Learning for Human Action Recognition
		</span>
		<span class="t-medium">
			in the Proceedings of the IEEE International Conference on Pattern Recognition (ICPR), oral presentation, 2012.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2012');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2012-moez.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			Most existing methods for action recognition mainly rely on manually engineered features which, despite their good performances, are highly problem dependent. We propose in this paper a fully automated model, which learns to classify human actions without using any prior knowledge. A convolutional sparse auto- encoder learns to extract sparse shift-invariant representations of the 2D local patterns present in each video frame. The evolution of these mid-level features is learned by a Recurrent Neural Network trained to classify each sequence. Experimental results on the KTH dataset show that the proposed approach outperforms existing models which rely on learned-features, and gives comparable results with the best related works.
		</div>
  	</dd>

	<dt class="conference">c22</dt>
	<dd id="sgp2012" class="bib-conf">
		<span class="t-author">
		    <a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>
	  	</span>
	  	<span class="t-title">
			Mesh Segmentation and Global 3D Model Extraction.
		</span>
		<span class="t-medium">
			Symposium on Geometry Processing, Poster, 2012.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('sgp2012');">
			Abstract</a>
		</li>
      		</ul>
		<div class="t-abstract">
			This paper presents a method for segmenting noisy 2-manifold meshes based on a decomposition into local shape primitives maximizing global coherence. This technique works by partitioning the input mesh into regions which can be approximated by a simple geometrical primitive such as a plane, a sphere or a cylinder. The partitioning is guided by robust shape extractions based on RANSAC sampling and the final decision to keep a 3D model into the final decomposition is based on a global graphical model which involves spatial and label cost priors. Obtained segmentations on noisy mesh models outperform other approaches in terms of region contour smoothness and consistency with mechanical object decomposition. Applications of this work are reverse engineering, mesh structure analysis, mesh feature enhancement, noise removal, mesh compression, piecewise approximation of mesh geometry (points, normals, curvatures), and remeshing.
		</div>
  	</dd>

	<dt class="conference">c20</dt>
	<dd id="hbu2012" class="bib-conf">
		<span class="t-author">
			<a href="http://www.busim.ee.boun.edu.tr/~oya/">Oya Celiktutan</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>,
			<a href="https://liris.cnrs.fr/membres?id=2207&onglet=publis">Eric Lombardi</a>
	  	</span>
	  	<span class="t-title">
			Real-Time Exact Graph Matching with Application in Human Action Recognition
		</span>
		<span class="t-medium">
		      In International Workshop on Human Behavior Understanding,
		</span>
		Istanbul, 2012.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('hbu2012');">Abstract</a></li>
		<li><a href="../papers/hbu2012.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		    Graph matching is one of the principal methods to formulate the correspondence between two set of points in computer vision and pattern recognition. Most formulations are based on the minimization of a difficult energy function which is known to be NP-hard. Traditional methods solve the minimization problem approximately. In this paper, we derive an exact minimization algorithm and successfully applied to action recognition in videos. In this context, we take advantage of special properties of the time domain, in particular causality and the linear order of time, and propose a new spatio-temporal graphical structure. We show that a better solution can be obtained by exactly solving an approximated problem instead of approximately solving the original problem.
		</p>
		</div>
  	</dd>

  	<dt class="conference">c19</dt>
	<dd id="hbu2011" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/christophe.garcia">Christophe Garcia</a> and
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>,
	  	</span>
	  	<span class="t-title">
			Sequential Deep Learning for Human Action Recognition,
		</span>
		<span class="t-medium">
			In the Proceedings of the International Workshop on Human Behavior Understanding: Inducing Behavioral Change, 2011. Oral presentation.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('hbu2011');">Abstract</a></li>
		<li><a href="../papers/hbu2011.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		    We propose in this paper a fully automated deep model, which learns to classify human actions without using any prior knowledge. The first step of our scheme, based on the extension of Convolutional Neural Networks to 3D, automatically learns spatio-temporal features. A Recurrent Neural Network is then trained to classify each sequence considering the temporal evolution of the learned features for each timestep. Experimental results on the KTH dataset show that the proposed approach outperforms existing deep models, and gives comparable results with the best related works.
		</p>
		</div>
  	</dd>

  	<dt class="conference">c18</dt>
	<dd id="grapp2011" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>
	  	</span>
	  	<span class="t-title">
			Robust feature line extraction on CAD triangular meshes,
		</span>
		<span class="t-medium">
			in the Proceedings of the International Conference on Computer Graphics Theory and Applications, oral presentation, 2011
		</span>

  	</dd>

  	<dt class="conference">c17</dt>
	<dd id="icpr2010w" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>
	  	</span>
	  	<span class="t-title">
			Integrating a discrete motion model into GMM based background subtraction,
		</span>
		<span class="t-medium">
			In the Proceedings of the IEEE International Conference on Pattern Recognition, oral presentation, 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2010w');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2010-bgsub.pdf">PDF</a></li>
		<li><a href="../papers/pres-icpr2010.pdf">Presentation</a></li>
      		</ul>
		<div class="t-abstract">
			GMM based algorithms have become the de facto standard for background subtraction in video sequences, mainly because of their ability to track multiple background distributions, which allows them to handle complex scenes including moving trees, flags moving in the wind etc. However, it is not always easy to determine which distributions of the mixture belong to the background and which distributions belong to the foreground, which disturbs the results of the labeling process for each pixel. In this work we tackle this problem by taking the labeling decision together for all pixels of several consecutive frames minimizing a global energy function taking into account spatial and temporal relationships. A discrete approximative optical-flow like motion model is integrated into the energy function and solved with Ishikawa's convex graph cuts algorithm.
		</div>
  	</dd>

  	<dt class="conference">c16</dt>
	<dd id="icpr2010p" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/anh-phuong.ta">Anh-Phong Ta</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a>,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>
	  	</span>
	  	<span class="t-title">
			Pairwise features for human action recognition,
		</span>
		<span class="t-medium">
			in the Proceedings of the IEEE International Conference on Pattern Recognition, 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2010p');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2010-phuong.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			Existing action recognition approaches mainly rely on the discriminative power of individual local descriptors extracted from spatio-temporal interest points (STIP), while the geometric relationships among the local features are ignored. This paper presents new features, called pairwise features (PWF), which encode both the appearance and the spatio-temporal relations of the local features for action recognition. First STIPs are extracted, then PWFs are constructed by grouping pairs of STIPs which are both close in space and close in time. We propose a combination of two codebooks for video representation. Experiments on two standard human action datasets: the KTH dataset and the Weizmann dataset show that the proposed approach outperforms most existing methods.
		</div>
  	</dd>

  	<dt class="conference">c15</dt>
	<dd id="avss2010" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/anh-phuong.ta">Anh-Phong Ta</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a> and
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>

	  	</span>
	  	<span class="t-title">
			Recognizing and localizing individual activities through graph matching,
		</span>
		<span class="t-medium">
			in the Proceedings of the International Conference on Advanced Video and Signal-Based Surveillance, 2010 (IEEE),
		</span>
		,oral presentation, 22.5% acceptance rate; <span class="t-emph">Best Paper</span> for track 'recognition', 5% acceptance rate.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('avss2010');">
			Abstract</a>
		</li>
		<li><a href="../papers/avss2010.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			In this paper we tackle the problem of detecting individual human actions in video sequences. While the most successful methods are based on local features, which proved that they can deal with changes in background, scale and illumination, most existing methods have two main shortcomings: first, they are mainly based on the individual power of spatio-temporal interest points (STIP), and therefore ignore the spatio-temporal relationships between them. Second, these methods mainly focus on direct classification techniques to classify the human activities, as opposed to detection and localization. In order to overcome these limitations, we propose a new approach, which is based on a graph matching algorithm for activity recognition. In contrast to most previous methods which classify entire video sequences, we design a video matching method from two sets of ST-points for human activity recognition. First, points are extracted, and a hyper graphs are constructed from them, i.e. graphs with edges involving more than 2 nodes (3 in our case). The activity recognition problem is then transformed into a problem of finding instances of model graphs in the scene graph. By matching local features instead of classifying entire sequences, our method is able to detect multiple different activities which occur simultaneously in a video sequence. Experiments on two standard datasets demonstrate that our method is comparable to the existing techniques on classification, and that it can, additionally, detect and localize activities.
		</div>
  	</dd>

  	<dt class="conference">c14</dt>
  	<dd id="gi2010" class="bib-conf">
		<span class="t-author">
			Pierre-Yves Laffont, Jong-Yun Jun,
			<a href="../index.html">Christian Wolf</a>,
			Yu-Wing Tai,
			<a href="http://liris.cnrs.fr/khalid.idrissi">Khalid Idrissi</a>,
			George Drettakis, Sung-Eui Yoon,
	  	</span>
	  	<span class="t-title">
			Interactive Content-Aware Zooming,
		</span>
		<span class="t-medium">
			In Graphics Interface, 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('gi2010');">
			Abstract</a>
			</li>
      		</ul>
		<div class="t-abstract">
	We propose a novel, interactive content-aware zooming operator that allows effective and efficient visualization of high resolution images on small screens, which may have different aspect ratios compared to the input images. Our approach applies an image retargeting method in order to fit an entire image into the limited screen space. This can provide global, but approximate views for lower zoom levels. However, as we zoom more closely into the image, we continuously unroll the distortion to provide local, but more detailed and accurate views for higher zoom levels. In addition, we propose to use an adaptive view-dependent mesh to achieve high retargeting quality, while maintaining interactive performance. We demonstrate the effectiveness of the proposed operator by comparing it against the traditional zooming approach, and a method stemming from a direct combination of existing works.
		</div>
 	 </dd>

 	<dt class="conference">c13</dt>
	<dd id="icann2010" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			Christophe Garcia,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			Action Classifcation in Soccer Videos with Long Short-Term Memory Recurrent Neural Networks
		</span>
		<span class="t-medium">
			In International Conference on Artificial Neural Networks (ICANN), 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icann2010');">
			Abstract</a>
		</li>
		<li><a href="../papers/icann2010.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			In this paper, we propose a novel approach for action classification in soccer videos using a recurrent neural network scheme. Thereby, we extract from each video action at each timestep a set of features which describe both the visual content (by the mean of a BoW approach) and the dominant motion (with a key point based approach). A Long Short-Term Memory-based Recurrent Neural Network is then trained to classify each video sequence considering the temporal evolution of the features for each timestep. Experimental results on the MICC-Soccer-Actions-4 database show that the proposed approach outperforms classification methods of related works (with a classification rate of 77 %), and that the combination of the two features (BoW and dominant motion) leads to a classification rate of 92 %.
		</div>
 	 </dd>

 	 <dt class="conference">c12</dt>
 	 <dd id="cbmi2009p" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/anh-phuong.ta">Anh-Phong Ta</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a>,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			3D Object detection and viewpoint selection in sketch images using local patch-based Zernike moments,
		</span>
		<span class="t-medium">
			in the Proceedings of the IEEE Workshop on Content Based Multimedia Indexing, pp. 189-194, 2009.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cbmi2009p');">
			Abstract</a>
			</li>
		<li><a href="../papers/cbmi2009-phuong.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	In this paper we present a new approach to detect and recognize 3D models in 2D storyboards which have been drawn during the production process of animated cartoons. Our method is robust to occlusion, scale and rotation. The lack of texture and color makes it difficult to extract local features of the target object from the sketched storyboard. Therefore the existing approaches using local descriptors like interest points can fail in such images.  We propose a new framework which combines patch-based Zernike descriptors with a method enforcing spatial constraints for exactly detecting 3D models represented as a set of 2D views in the storyboards. Experimental results show that the proposed method can deal with partial object occlusion and is suitable for poorly textured objects.
		</div>
  </dd>

  <dt class="conference">c11</dt>
  <dd id="cbmi2009m" class="bib-conf">
		<span class="t-author">
			Marc Mouret,
			<a href="http://liris.cnrs.fr/christine.solnon">Christine Solnon</a>,
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Classification of images based on Hidden Markov Models,
		</span>
		<span class="t-medium">
			in the Proceedings of the IEEE Workshop on Content Based Multimedia Indexing, pp. 169-174, 2009.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cbmi2009m');">
			Abstract</a>
			</li>
		<li><a href="../papers/cbmi2009-marc.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	We propose to use hidden Markov models (HMMs) to classify images. Images are modeled by extracting symbols corresponding to 3x3 binary neighborhoods of interest points, and by ordering these symbols by decreasing saliency order, thus obtaining strings of symbols. HMMs are learned from sets of strings modeling classes of images. The method has been tested on the SIMPLIcity database and shows an improvement over competing approaches based on interest points. We also evaluate these approaches for classifying thumbnail images, i.e., low resolution images.
		</div>
  </dd>

 	 <dt class="conference">c10</dt>
  	<dd id="sgp2009" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavou&eacute;</a>
	  	</span>
	  	<span class="t-title">
			Global triangular mesh regularization using conditional Markov random fields.
		</span>
		<span class="t-medium">
			Poster (refereed, but not published: acceptance rate ~35%) at Symposium on Geometry Processing, 2009
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('sgp2009');">
			Abstract</a>
			</li>
      		</ul>
		<div class="t-abstract">
			We present a global mesh optimization framework based on a Conditional Markov Random Fied (CMRF or CRF) model suited for 3D triangular meshes of arbitrary topology. The remeshing task is formulated as a Bayesian estimation problem including data attached terms measuring the fidelity to the original mesh as well as a prior favoring high quality triangles. Since the best solution for vertex relocation is strongly related to the mesh connectivity, our approach iteratively modifies the mesh structure (connectivity plus vertex addition/removal) as well as the vertex positions, which are moved according to a well-defined energy function resulting from the CMRF model. Good solutions for the proposed model are obtained by a discrete graph cut algorithm examining global combinations of local candidates. Results on various 3D meshes compare favorably to recent state-of-the-art algorithms regarding the trade-off between triangle shape improvement and surface fidelity. Applications of this work mainly consist in regularizing meshes for numerical simulations and for improving mesh rendering.
		</div>
 	 </dd>

	<dt class="conference">c9</dt>
  <dd id="mlsp2009" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Families of Markov models for document image segmentation,
		</span>
		<span class="t-medium">
			In IEEE Machine Learning for Signal Processing Workshop, 2009
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('mlsp2009');">
			Abstract</a>
		</li>
		<li><a href="../papers/mlsp2009.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			In this paper we compare several directed and undirected graphical models for different image segmentation problems in the domain of document image processing and analysis. We show that adapting the structure of the model to specific sitations at hand, for instance character restoration, recto/verso separation and segmenting high resolution character images, can significantly improve segmentation performance. We propose inference algorithms for the different models and we test them on different data sets.
		</div>
  </li>

	<dt class="conference">c8</dt>
  	<dd id="icpr2008" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>,
	  	</span>
	  	<span class="t-title">
			Improving recto document side restoration with an estimation of the verso side from a single scanned page
		</span>
		<span class="t-medium">
			In the Proceedings of the IEEE International Conference on Pattern Recognition, pp. 1-4, 2008.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2008');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2008.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	We present a new method for blind document bleed through removal based on separately restoring the recto and the verso side. The segmentation algorithm is based on separate Markov random fields (MRF) which results in a better adaptation of the prior to the content creation process (e.g. superimposing two pages), and the improvement of the estimation of the verso pixels through an estimation of the verso pixels covered by recto pixels. The labels of the initial recto and verso clusters are recognized without using any color or gray value information. The proposed method is evaluated empirically as well as through OCR improvement.
		</div>
  	</dd>

  	<dt class="conference">c7</dt>
  <dd id="eg2008" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavou&eacute;</a> and <a href="../index.html">Christian Wolf</a>
	  	</span>,
	  	<span class="t-title">
			Markov Random Fields for Improving 3D Mesh Analysis and Segmentation,
		</span>
		<span class="t-medium">
			In the Proceedings of the Eurographics 2008 Workshop on 3D Object Retrieval.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('eg2008');">
			Abstract</a>
			<li><a href="../papers/eurographics2008.pdf">PDF</a></li>
		</li>
      		</ul>
		<div class="t-abstract">
    Abstract
Mesh analysis and clustering have became important issues in order to improve the efficiency of common processing
operations like compression, watermarking or simplification. In this context we present a new method for
clustering / labeling a 3D mesh given any field of scalar values associated with its vertices (curvature, density,
roughness etc.). Our algorithm is based on Markov Random Fields, graphical probabilistic models. This Bayesian
framework allows (1) to integrate both the attributes and the geometry in the clustering, and (2) to obtain an optimal
global solution using only local interactions, du to the Markov property of the random field. We have defined
new observation and prior models for 3D meshes, adapted from image processing which achieve very good results
in terms of spatial coherency of the labeling. All model parameters are estimated, resulting in a fully automatic
process (the only required parameter is the number of clusters) which works in reasonable time (several seconds).
		</div>
  	</dd>

    <dt class="conference">c5</dt>
	<dd id="icict2004" class="bib-conf">
		<span class="t-author">
			<a href="http://www.eng.uwaterloo.ca/%7Egwtaylor">Graham W.
        	Taylor</a> and <a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Reinforcement Learning for Parameter Control of Text Detection in Images
        	and Video Sequences
		</span>
		<span class="t-medium">
			Proceedings of the IEEE International Conference
        	on Information &amp; Communication Technologies
		</span>,
      2004.
			6 pages.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icict2004');">
			Abstract</a>
		</li>
        	<li><a href="../papers/icict2004.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		A framework for parameterization in computer vision algorithms is evaluated by
		optimizing ten parameters of the text detection for semantic indexing algorithm
		preposed by Wolf et al. The Fuzzy ARTMAP neural network is used for generalization,
		offering much faster learning than in a previous tabular implementation.
		Difficulties in using a continuous action space are overcome by employing the DIRECT
		method for global optimization without derivatives. The chosen parameters are
		evaluated using metrics of recall and precision, and are shown to be superior to the
		parameters previously recommended.
		</div>
  	</dd>

	<dt class="conference">c4</dt>
	<dd id="icpr2002v" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf </a>,<a href="http://rfv.insa-lyon.fr/%7Ejolion">
			Jean-Michel Jolion</a> and Francoise Chassaing.
	  	</span>
	  	<span class="t-title">
			Text Localization, Enhancement and Binarization in Multimedia Documents
		</span>
		<span class="t-medium">
			Proceedings of the International Conference on Pattern Recognition (ICPR),
		</span>
			volume 4, pages 1037-1040, IEEE Computer Society.
        	August 11th-15th, 2002, Quebec City, Canada. 4 pages.
        	<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('icpr2002v');">Abstract</a></li>
        	<li><a href="../papers/icpr2002v.pdf">PDF</a></li>
        	<li><a href="../papers/icpr2002v.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('icpr2002v');">BibTeX</a></li>
        	<li><a href="../software/binarize/index.html">C++ Code (binarization only)</a></li>
      		</ul>
		<div class="t-abstract">
		The systems currently available for content based image and video retrieval work
		without semantic knowledge, i.e. they use image processing methods to extract low
		level features of the data. The similarity obtained by these ap-proaches does not
		always correspond to the similarity a human user would expect. A way to include more
		semantic knowledge into the indexing process is to use the text
		included in the images and video sequences. It is rich in information but easy to
		use, e.g. by key word based queries. In this paper we present an algorithm to
		localize artificial text in images and videos using a measure of accumulated
		gradients and morphological post processing to detect the text. The quality of the
		localized text is improved by robust multiple frame integration. A new technique for
		the bina-rization of the text boxes is proposed. Finally, detection and OCR results
		for a commercial OCR are presented.
		</div>
		<div class="t-bibtex"><pre>
@InProceedings{WolfICPR2002V,
  Author         = {C. Wolf and J.-M. Jolion and F. Chassaing},
  Title          = {Text {L}ocalization, {E}nhancement and {B}inarization in {M}ultimedia {D}ocuments},
  BookTitle      = {Proceedings of the {I}nternational {C}onference on {P}attern {R}ecognition},
  Volume         = {2},
  Pages          = {1037-1040},
  year           = 2002,
}
		</pre></div>
  	</dd>

	<dt class="conference">c3</dt>
	<dd id="icpr2002m" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.cfar.umd.edu/%7Edoermann/">David Doermann</a>
	  	</span>
	  	<span class="t-title">
			Binarization of Low Quality Text using a Markov Random Field Model.
		</span>
		<span class="t-medium">
			Proceedings of the International Conference on Pattern Recognition (ICPR),
		</span>
			volume 2, pages 160-163, IEEE Computer Society. August 11th-15th, 2002,
        	Quebec City, Canada. 4 pages.
			<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('icpr2002m');">Abstract</a></li>
        	<li><a href="../papers/icpr2002m.pdf">PDF</a></li>
        	<li><a href="../papers/icpr2002m.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('icpr2002m');">BibTeX</a></li>
      		</ul>
		<div class="t-abstract">
		Binarization techniques have been developed in the document analysis community for
		over 30 years and many algorithms have been used successfully. On the other hand,
		document analysis tasks are more and more frequently being applied to multimedia
		documents such as video sequences. Due to low resolution and lossy compression, the
		binarization of text included in the frames is a non trivial task. Existing
		techniques work without a model of the spatial relationships in the image, which
		makes them less powerful. We introduce a new technique based on a Markov Random
		Field (MRF) model of the document. The model parameters (clique potentials) are
		learned from training data and the binary image is estimated in a Bayesian
		framework. The performance is evaluated using commercial OCR software.
		</div>
		<div class="t-bibtex">
		<pre>
@InProceedings{WolfICPR2002M,
  Author         = {C. Wolf and D. Doermann},
  Title          = {Binarization of {L}ow {Q}uality {T}ext using a {M}arkov {R}andom {F}ield {M}odel},
  BookTitle      = {Proceedings of the {I}nternational {C}onference on {P}attern {R}ecognition},
  Volume         = {3},
  Pages          = {160-163},
  year           = 2002,
}
		</pre></div>
  	</dd>

	<dt class="conference">c1</dt>
	<dd id="icpr2000" class="bib-conf">
		<span class="t-author">
			<a href="../index.html"> Christian Wolf </a>,
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion </a>,
			<a href="http://www.prip.tuwien.ac.at/%7Ekrw"> Walter Kropatsch </a>, and
			<a href="http://www.prip.tuwien.ac.at/%7Ebis"> Horst Bischof </a>.
	  	</span>
	  	<span class="t-title">
			Content based Image Retrieval using Interest Points and Texture Features,
		</span>
		<span class="t-medium">
			Proceedings of the International Conference on Pattern Recognition (ICPR),
		</span>
			volume 4, pages 234-237. IEEE Computer Society. September 3rd,
			2000, Barcelona,
        	Spain. 4 pages.
      		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('icpr2000');">Abstract</a></li>
        	<li><a href="../papers/icpr2000.ps.gz">Postscript</a></li>
        	<li><a href="../papers/icpr2000_poster.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('icpr2000');">BibTeX</a></li>
        	<li><a href="http://www.prip.tuwien.ac.at/Research/ImageDatabases/Query">Demo</a></li>
     		</ul>
		<div class="t-abstract">
		<p>Interest point detectors are used in computer vision to detect image points with
		special properties, which can be geometric (corners) or non-geometric (contrast
		etc.). Gabor functions and Gabor filters are regarded as excellent tools for
		feature extraction and texture segmentation. This article presents methods how to
		combine these methods for content based image retrieval and to generate a textural
		description of images. Special emphasis is devoted to distance measure texture
		descriptions. Experimental results of a query system are given.
		</p><p>
		This work was supported in part by the Austrian Science Foundation (FWF) under grant
		S-7002-MAT.</p>
		</div>
		<div class="t-bibtex"><pre>
@InProceedings{WolfICPR2000,
  Author         = {C. Wolf and J.M. Jolion and W. Kropatsch and H. Bischof},
  Title          = {Content {B}ased {I}mage {R}etrieval using {I}nterest {P}oints and {T}exture {F}eatures},
  BookTitle      = {Proceedings of the {I}nternational {C}onference on {P}attern {R}ecognition},
  Volume         = {4},
  Pages          = {234-237},
  year           = 2000,
}
		</pre></div>
  	</dd>

</dl>

<!-- ****************************************************************************************
    PATENTS
  ******************************************************************************************* -->

<h2>Patents</h2>
<ul>
	<li class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>, <a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a> and
			Christophe  Laurent.
	  	</span>
	  	<span class="t-title">
			D&eacute;termination de caract&eacute;ristiques textuelles de pixels.
		</span>
		Patent submitted by France Telecom. Reference: FR 03 11918, date: October 10th, 2003.
  	</li>

	<li class="bib-empty">
		<span class="t-author"> <a href="../index.html">Christian Wolf</a>, <a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a> and
		Francoise Chassaing.
	  	</span>
	  	<span class="t-title"> Proc&egrave;de de d&eacute;tection de zones de texte dans une image vid&eacute;o
		</span>
		Patent submitted by France Telecom. Reference: FR 01 06776, Date: May 23th, 2001.
  	</li>
</ul>



<!-- ****************************************************************************************
  NATIONAL CONFERENCES - REFEREED.
  ******************************************************************************************* -->

<h2>Articles in conferences with national audience (refereed)</h2>

<ul>

	<li id="cap2014" class="bib-conf">
		<span class="t-author">
			Taygun Kekec,
			<a href="http://home.heeere.com/">Rémi Emonet</a>,
			<a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>,
			<a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp?RH=INTERNATIONAL">Alain Trémeau</a> and
			<a href="../index.html">Christian Wolf</a>,
	  	</span>
	  	<span class="t-title">
				Contextually Constrained Deep Networks for Scene Labeling.
		</span>
		<span class="t-medium">
			In Conférence d'Apprentissage Automatique,
		</span>
			2014.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cap2014');">
			Abstract</a>
		</li>
		</ul>

		<div class="t-abstract">
			Learning using deep learning architectures is a difficult problem: the complexity of the network and the gradient descent method used to update the network’s weights can both lead to overfitting phenomena and bad local optima. To overcome these problems in the context of full scene labeling, we would like to constraint parts of the network using some semantic context to 1) control its capacity while still allowing complex functions to be learned 2) obtain more meaningful layers which will avoid bad local optima. We first propose to learn a weak convolutional network which would provide us rough label maps over the neighborhood of a pixel. Then, we incorporate this weak learner in a bigger network previously trained using some label information on the neighborhood of a pixel. This iterative augmentation process aims at increasing the interpretability by constraining some features maps to learn precise contextual information. We show how this contextual knowledge yields higher accuracy than state-of- the-art architectures for Stanford and SIFT Flow scene labeling datasets. The approach is generic and can be applied to similar networks where contextual cues are available at training time.
		</div>
  	</li>

  	<li id="wacai2014" class="bib-conf">
		<span class="t-author">
			<a href="http://www.gipsa-lab.grenoble-inp.fr/page_pro.php?vid=1752">Alaeddine Mihoub</a>,
			<a href="http://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly">Gerard Bailly</a> and
			<a href="../index.html">Christian Wolf</a>,
	  	</span>
	  	<span class="t-title">
			Modeling sensory-motor behaviors for social robots.
		</span>
		<span class="t-medium">
		      In Workshop Affect, Compagnon Artificiel, Interaction, Rouen,
		</span>
			2014.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('hbu2013');">Abstract</a></li>
      	</ul>
		<div class="t-abstract">
		<p>
		   Modeling multimodal perception-action loops in face-to-face interactions is a crucial step in the process of building sensory-motor behaviors for social robots or users-aware Embodied Conversational Agents (ECA). In this paper, we compare trainable behavioral models based on sequential models (HMMs) and classifiers (SVMs and Decision Trees) inherently inappropriate to model sequential aspects. These models aim at giving pertinent perception/action skills for robots in order to generate optimal actions given the perceived actions of others and joint goals. We applied these models to parallel speech and gaze data collected from interacting dyads. The challenge was to predict the gaze of one subject given the gaze of the interlocutor and the voice activity of both. We show that Incremental Discrete HMM (IDHMM) generally outperforms classifiers and that injecting input context in the modeling process significantly improves the performances of all algorithms.
		</p>
		</div>
  	</li>

	<li id="coresa2012oya" class="bib-conf">
		<span class="t-author">
			<a href="http://www.busim.ee.boun.edu.tr/~oya/">Oya Celiktutan</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>,
	  	</span>
	  	<span class="t-title">
			Appariement de points spatio-temporels par hyper-graphes et optimisation discrète exacte.
		</span>
		<span class="t-medium">
			In "COmpression et REpr&eacute;sentation des Signaux Audiovisuels" (CORESA),
		</span>
			2012, 6 pages.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('coresa2012oya');">
			Abstract</a>
		</li>
		<li><a href="../papers/coresa2012oya.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		Les graphes et les hyper-graphes sont souvent utilisés pour la reconnaissance de modèles complexes et non-rigides en vision par ordinateur, soit par appariement de graphes ou par appariement de nuages de points par graphes. La plupart des formulations recourent à la minimisation d'une fonction d'énergie difficile contenant des termes géométriques ou structurels, souvent couplés avec des termes d'attache aux données comportant des informations liées à l'apparence locale. Les méthodes traditionnelles tente une résolution approximative du problème de minimisation, par exemple avec des techniques spectrales. Dans cet article nous traitons des données embarquées dans l'"espace-temps", comme cela est typiquement le cas pour les applications de reconnaissance d'actions. Nous montrons que, dans ce contexte, nous pouvons profiter des propriétés particulières du domaine temporel, notamment la causalité et l'ordre stricte imposé par cette dimension. Nous montrons que la complexité du problème est inférieure à la complexité de la problématique générale et nous dérivons un algorithme calculant la solution exacte. Comme une seconde contribution, nous proposons une nouvelle structure graphique allongée dans le temps. Nous soutenons que, au lieu résoudre le problème d'origine de manière approximative, une meilleure solution peut être obtenue par en résolvant, de manière exacte, un problème approché. Un algorithme de minimisation exacte est dérivé de cette structure et appliqué avec succès à la reconnaissance d'actions dans les vidéos.
		</div>
  	</li>

	<li id="coresa2012mingyuan" class="bib-conf">
		<span class="t-author">
			<a href="http://dblp.uni-trier.de/pers/hd/j/Jiu:Mingyuan">Mingyuan Jiu,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/christophe.garcia">Christophe Garcia</a> and
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			Supervised learning and codebook optimization with neural network,
		</span>
		<span class="t-medium">
			"COmpression et REpr&eacute;sentation des Signaux Audiovisuels" (CORESA),
		</span>
			2012, 6 pages.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('coresa2012mingyuan');">
			Abstract</a>
		</li>
      		</ul>

		<div class="t-abstract">
		In this paper, we present a novel approach for supervised codebook learning and optimization with neural networks for bag of words models in visual recognition tasks. We propose a new supervised framework for joint codebook creation and class learning, which learns the codewords in a goal-directed way using the class labels of the training set. As a result, the codebook becomes more discriminative. Two different learning algorithms, one based on error backpropagation and one based on cluster label reassignment, are presented. We evaluate them on the KTH dataset for human action recognition, reporting very promising results. The proposed technique allows to improve the discriminative power of an unsupervised learned codebook, or to keep the discriminative power while decreasing the size of the learned codebook.
		</div>
  	</li>

	<li id="coresa2010" class="bib-conf">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			Christophe Garcia,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			Une approche neuronale pour la classification d'actions de sport par la prise en compte du contenu visuel et du mouvement dominant
		</span>
		<span class="t-medium">
			CORESA 2010, "COmpression et REpr&eacute;sentation des Signaux Audiovisuels",
		</span>
			2010, 6 pages.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('coresa2010');">
			Abstract</a>
		</li>
		<li><a href="../papers/coresa2010.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		Dans cet article, nous proposons une approche de classification automatique de séquences vidéo d’actions de sport. Pour cela, nous extrayons de chaque action des caractéristiques du contenu visuel, en utilisant deux approches, l’une par sac de mots, et l’autre par le mouvement dominant de la scène à chaque instant. La classification de l’évolution temporelle de ces caractéristiques extraites est gérée dynamiquement par un modèle neuronal, basé sur les réseaux de neurones récurrents à large « mémoire court-terme» (LSTM). Les expérimentations faites sur la base « MICCSoccer-Actions-4 » montrent que l’approche neuronale de classification permet d’obtenir des résultats supérieurs à l’état de l’art (76 % de bonne classification), et que la combinaison des caractéristiques (information visuelle et mouvement dominant) permet un taux de bonne classification de 92 %.
		</div>
  	</li>

	<li id="coresa2009" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			S&eacute;paration recto/verso d'un document par mod&eacute;lisation markovienne &agrave;  double couche
		</span>
		<span class="t-medium">
			CORESA 2009, "COmpression et REpr&eacute;sentation des Signaux Audiovisuels",
		</span>
			Toulouse, 2009, 6 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('coresa2009');">
			Abstract</a>
		</li>
		<li><a href="../papers/coresa2009.pdf">Paper-PDF</a></li>
		<li><a href="../papers/prescoresa2009.pdf">Presentation-PDF</a></li>
      		</ul>

		<div class="t-abstract">
		Nous proposons un modèle markovien &agrave;  deux couches pour la s&eacute;paration des deux faces d'un document, dont une seule face a &eacute;t&eacute; num&eacute;ris&eacute;. A l'aide de deux champs de Markov s&eacute;par&eacute;s, un pour chaque face, chaque pixel est mod&eacute;lis&eacute; par deux variables cach&eacute;es connect&eacute;es par une unique variable observ&eacute;e. L'avantage de cette formulation est une meilleure adaptation au processus ayant cr&eacute;&eacute; l'image observ&eacute;e (la superposition de deux pages ind&eacute;pendantes) ainsi que l'am&eacute;lioration de la restauration, &ccedil;.&agrave; .d. de l'estimation des pixels recto, par une estimation des pixels verso couverts par ce derniers. L'inf&eacute;rence des variables cach&eacute;es est r&eacute;alis&eacute;e par un algorithme it&eacute;ratif &agrave;  base de coupure minimale dans un graphe &eacute;tendant l'algorithme d'&eacute;xpansion alpha. Les r&eacute;sultats sont &eacute;valu&eacute;s &agrave;  la fois de fa&ccedil; on empirique ainsi que par l'am&eacute;lioration d'un r&eacute;sultat de reconnaissance OCR.
		</div>
  	</li>

  	<li id="jfrb2008" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and G&eacute;rald Gavin
	  	</span>,
	  	<span class="t-title">
			Inference and parameter estimation on belief networks for image segmentation
		</span>
		<span class="t-medium">
			Journ&eacute;es francophones des r&eacute;seaux bay&eacute;siens, Lyon (France), May 2008
		</span>.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('jfrb2008');">
			Abstract</a>
			</li>
      		</ul>
		<div class="t-abstract">
        	We introduce a new causal hierarchical belief network for image segmentation. Contrary
to classical tree structured (or pyramidal) models, the factor graph of the network contains
cycles. Each level of the hierarchical structure features the same number of sites as the base
level and each site on a given level has several neighbors on the parent level. Compared to
tree structured models, the (spatial) random process on the base level of the model is stationary
which avoids known drawbacks, namely visual artifacts in the segmented image. We propose
different parametrisations of the conditional probability distributions governing the transitions
between the image levels. A parametric distribution depending on a single parameter allows
the design of a fast inference algorithm on graph cuts, whereas the parameter is estimated with
a least squares technique. For arbitrary distributions, we propose inference with loopy belief
propagation and we introduce a new parameter estimation technique adapted to the model.
		</div>
  	</li>

	<li class="bib-conf">
		<span class="t-author">
			Remi Landais, <a href="../index.html">Christian Wolf</a>,
			Laurent Vinet and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Utilisation de connaissances a priori pour le param&eacute;trage d'un
			algorithme de
        		d&eacute;tection de textes dans les documents audiovisuels. Appliation
        		&agrave; un corpus de journaux t&eacute;l&eacute;vis&eacute;s.
		</span>
		<span class="t-medium">
			14&egrave;me Congr&eacute;s Francophone de Reconnaissance des Formes
        	et Intelligence Artificielle.
		</span>
			2004, 10 pages.
  	</li>

	<li id="coresa2003" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			D&eacute;tection de textes de scenes dans des images issues d'un flux
			vid&eacute;o,
		</span>
		<span class="t-medium">
			CORESA 2003, "COmpression et REpr&eacute;sentation des Signaux Audiovisuels",
		</span>
			pages 63-66. January 16th - 17th 2003, Lyon. 4 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('coresa2003');">
			Abstract</a>
		</li>
        	<li><a href="../papers/coresa2003.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		La plupart des travaux sur la d&eacute;tection de texte se concentre sur le texte
		artificiel et horizontal. Nous proposons une m&eacute;thode de d&eacute;tection en orientation
		g&eacute;n&eacute;rale qui repose sur un filtre directionnel appliqu&eacute; dans plusieurs orientations.
		Un algorithme de relaxation hi&eacute;rarchique est employ&eacute; pour consolider les r&eacute;sultats
		locaux de direction. Une &eacute;tape de vote entre des directions permet d'obtenir une
		image binaire localisant les zones de textes.
		</div>
  	</li>

  	<li id="cifed2002" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Vid&eacute;o OCR - D&eacute;tection et extraction du texte.
		</span>
		<span class="t-medium">
			Colloque International Francophone sur l&acute;Ecrit et le Document,
		</span>
			pages 215-224. Hammamet-Tunesie, 20-23 octobre 2002.
			<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cifed2002');">
			Abstract</a>
		</li>
        	<li><a href="../papers/cifed2002.pdf">PDF</a></li>
        	<li><a href="http://telesun.insa-lyon.fr/%7Ewolf/demos/textdetect.html">Demo</a></li>
      		</ul>
		<div class="t-abstract">
		<p>Les syst&eacute;mes d'indexation ou de recherche par le contenu disponibles actuellement
		travaillent sans connaissance (syst&eacute;mes pr&eacute;-attentifs). Malheureusement les requ&eacute;tes
		construites ne correspondent pas toujours aux r&eacute;sultats obtenus par un humain qui
		interpr&eacute;te le contenu du document. Le texte pr&eacute;sent dans les vid&eacute;os repr&eacute;sente une
		caract&eacute;ristique &eacute; la fois riche en information et cependant simple, cela permet de
		compl&eacute;ter les requ&eacute;tes classiques par des mots clefs.
		</p><p>
		Nous pr&eacute;sentons dans cet article un projet visant &eacute; la d&eacute;tection et la
		reconnaissance du texte pr&eacute;sent dans des images ou des s&eacute;quences vid&eacute;o. Nous
		proposons un sch&eacute;ma de d&eacute;tection s'appuyant sur la mesure du gradient directionnel
		cumul&eacute;. Dans le cas des s&eacute;quences vid&eacute;o, nous introduisons un processus de
		fiabilisation des d&eacute;tections et l'am&eacute;lioration des textes d&eacute;tect&eacute;s par un suivi et
		une int&eacute;gration temporelle.</p>
		</div>
  	</li>

	<li id="rfia2002" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Extraction de texte dans des videos: le cas de la binarisation,
		</span>
		<span class="t-medium">
			Congr&eacute;s Francophone de Reconnaissance des Formes
        		et Intelligence Artificielle,
		</span>
			volume 1, pages 145-152, January 8th-10th 2002, Angers.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('rfia2002');">
			Abstract</a>
		</li>
        	<li><a href="../papers/rfia02.pdf">PDF</a></li>
        	<li><a href="http://telesun.insa-lyon.fr/%7Ewolf/demos/binarize.html">Demo</a></li>
      		</ul>

		<div class="t-abstract">
		<p>Dans cet article nous abordons la probl&eacute;me de la binarisation de "boites", i.e.
		sous-image, contenant du texte. Nous montrons que la sp&eacute;cificit&eacute; des contenus vid&eacute;os
		am&eacute;ne &eacute; la conception d'une nouvelle approche de cette &eacute;tape de binarisation en
		regard des techniques habituelles tant du traitement d'image au sens large, que du
		domaine de l'analyse de documents &eacute;crits.
		</p><p>We present in this paper some researches on thresholding of "text boxes"
		(sub-images containing artificial texts and extracted from videos). We show that the
		particular context of videos leads to the formalization of a new approach of this
		step regarding the usual and wellknow techniques used in image analysis and more
		particularly for segmentation of written documents.</p>
		</div>
  	</li>

	<li id="coresa2001" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf </a> ,
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a> and
			Francoise Chassaing.
	  	</span>
	  	<span class="t-title">
			Vid&eacute;o OCR - D&eacute;tection et extraction du texte,
		</span>
		<span class="t-medium">
			CORESA 2001, 7&egrave;me Journ&eacute;es d&acute;&Eacute;tudes
        		et d&acute;Echanges "COmpression et REpr&eacute;sentation des Signaux
        		Audiovisuels",
		</span>
			pages 251-258, November 12th - 13th 2001, Dijon.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('coresa2001');">
			Abstract</a>
		</li>
        	<li><a href="../papers/coresa2001.pdf">PDF</a></li>
        	<li><a href="http://telesun.insa-lyon.fr/%7Ewolf/demos/textdetect.html">Demo</a></li>
      		</ul>

		<div class="t-abstract">
		Les syst&eacute;mes d'indexation ou de recherche par le contenu disponibles actuellement
		travaillent sans connaissance (syst&eacute;mes pr&eacute;-attentifs). Malheureusement les requ&eacute;tes
		construites ne correspondent pas toujours aux r&eacute;sultats obtenus par un humain qui
		interpr&eacute;te le contenu du document. Le texte pr&eacute;sent dans les vid&eacute;os repr&eacute;sente une
		caract&eacute;ristique &eacute; la fois riche en information et cependant simple, cela permet de
		compl&eacute;ter les requ&eacute;tes classiques par des mots clefs. Nous pr&eacute;sentons dans cet
		article un projet visant &eacute; la d&eacute;tection et la reconnaissance du texte pr&eacute;sent dans
		des images ou des s&eacute;quences vid&eacute;o. Nous proposons un sch&eacute;ma de d&eacute;tection s'appuyant
		sur la mesure du gradient directionnel cumul&eacute;. Dans le cas des s&eacute;quences vid&eacute;o, nous
		introduisons un processus de fiabilisation des d&eacute;tections et l'am&eacute;lioration des
		textes d&eacute;tect&eacute;s par un suivi et une int&eacute;gration temporelle.
		</div>
  	</li>

	<li id="orasis2001" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf </a> and
			<a href="http://rfv.insa-lyon.fr/~jolion">  Jean-Michel Jolion </a>.
	  	</span>
	  	<span class="t-title">
			Vid&eacute;o OCR - D&eacute;tection et extraction du texte,
		</span>
		<span class="t-medium">
			ORASIS 2001, Congr&egrave;s francophone de vision,
		</span>
			5-8 Juin 2001, pages 415-424, IRIT, route de Narbonne, 31062, Toulouse
			Cedex 4, France.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('orasis2001');">
			Abstract</a>
		</li>
        	<li><a href="../papers/orasis2001.pdf">PDF</a></li>
        	<li><a href="../papers/orasis2001.ppt">Powerpoint</a></li>
        	<li><a href="http://telesun.insa-lyon.fr/%7Ewolf/demos/textdetect.html">Demo</a></li>
      		</ul>

		<div class="t-abstract">
		Nous pr&eacute;sentons dans cet article les premi&eacute;res &eacute;tapes d'un projet visant &eacute; la
		d&eacute;tection et la reconnaissance du texte pr&eacute;sent dans des images ou des s&eacute;quences
		vid&eacute;o. Nous insisterons ici sur la caract&eacute;risation de ce type de texte en regard des
		textes pr&eacute;sents dans le documents classiques. Nous proposons un sch&eacute;ma de d&eacute;tection
		s'appuyant sur la mesure du gradient dir&eacute;ctionnel cumul&eacute;. Dans le cas des s&eacute;quences,
		nous introduisons un processus de fiabilisation des d&eacute;tections et l'am&eacute;lioration des
		textes d&eacute;tect&eacute;s par un suivi et une int&eacute;gration temporelle.
		</div>
  	</li>

	 <li id="oeagm2000" class="bib-conf">
		<span class="t-author">
			<a href="../index.html"> Christian Wolf </a>,
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion </a>, and
			<a href="http://www.prip.tuwien.ac.at/%7Ebis"> Horst Bischof</a>.
	  	</span>
	  	<span class="t-title">
			Histograms for Texture based Image Retrieval,
		</span>
		<span class="t-medium">
			Proceedings of the OEAGM,
		</span>
			Robert Sablatnig and Christian Menard (Ed.), pages 169-176. Oldenbourg, 25th
			May 2000. 8 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('oeagm2000');">
			Abstract</a>
		</li>
        	<li><a href="../papers/oeagm2000.ps.gz">Postscript</a></li>
        	<li><a href="../papers/oeagm2000_pres.ppt">Powerpoint</a></li>
        	<li><a href="http://www.prip.tuwien.ac.at/Research/ImageDatabases/Query">
			Demo</a></li>
      		</ul>

		<div class="t-abstract">
		Interest point detectors are used in computer vision to detect image points with
		special properties, which can be geometric (corners) or non-geometric (contrast
		etc.). Gabor functions and Gabor filters are regarded as excellent tools for feature
		extraction and texture segmentation. This article presents methods how to combine
		these methods for content based image retrieval and to generate a textural
		description of images. Special emphasis is devoted to distance measure texture
		descriptions. Experimental results of a query system are given.
		</div>
  	</li>
</ul>

<!-- ****************************************************************************************
  NATIONAL CONFERENCES - NON REFEREED
  ******************************************************************************************* -->

<h2>Other papers </h2>

<dl>



  	<dt class="conference">c6</dt>
	<dd id="imageeval2007" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>
	  	</span>
	  	<span class="t-title">
			Quality, quantity and generality in the evaluation of object detection algorithms
		</span>
		<span class="t-medium">
			Proceedings of the Image Eval Conference,
		</span>
			July 12th, 2007, Amsterdam, NL. 8 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('imageeval2007');">Abstract</a></li>
			<li><a href="../papers/imageeval2007.pdf">PDF</a></li>
      	</ul>

		<div class="t-abstract">
		<p>
		Evaluation of object detection algorithms is a non-trivial task: a detection
		result is usually evaluated by comparing the bounding box of the detected
		object with the bounding box of the ground truth object. The commonly used
		precision and recall measures are computed from the overlap area of these two
		rectangles. However, these measures have several drawbacks: they don't give
		intuitive information about the proportion of the correctly detected objects
		and the number of false alarms, and they cannot be accumulated across multiple
		images without creating ambiguity in their interpretation. Furthermore,
		quantitative and qualitative evaluation is often mixed resulting in ambiguous
		measures.
		</p>

		<p>
		In this paper we propose an approach to evaluation which tackles these problems.
		The performance of a detection algorithm is illustrated intuitively by performance
		graphs which present object level precision and recall depending on
		constraints on detection quality. In order to compare different detection
		algorithms, a representative single performance value is computed from the graphs.
		The evaluation method can be applied to different types of object detection
		algorithms. It has been tested on different text detection algorithms, among which
		are the participants of the Image Eval text detection competition.
		</p>
		</div>
  	</dd>

  	<dt class="conference">c2</dt>
	<dd id="trec2002" class="bib-conf">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.cfar.umd.edu/%7Edoermann/">David Doermann</a> and
			<a
			href="http://www.ee.oulu.fi/EE/Info.Laboratory/personnel/Rautiainen.Mika.htm">
			Mika Rautiainen</a>.
	  	</span>
	  	<span class="t-title">
			Video Indexing and Retrieval at UMD,
		</span>
		<span class="t-medium">
			Proceedings of the Text Retrieval Conference (TREC),
		</span>
			November 19th-22th, 2002, Gaithersburg, USA. 10 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trec2002');">
			Abstract</a>
		</li>
      		</ul>

		<div class="t-abstract">
		<p>Our team from the University of Maryland and INSA de Lyon
		participated in the feature extraction evaluation with overlay text
		features and in the search evaluation with a query retrieval and browsing system. For
		search we developed a weighted query mechanism by integrating 1) text
		(OCR and speech recognition) content using full text and n-grams
		through the MG system, 2) color correlogram indexing of image and
		video shots reported last year in TREC, and 3) ranked versions of the
		extracted binary features. A command line version of the interface
		allows users to formulate simple queries, store them and use weighted
		combinations of the simple queries to generate compound queries.
		</p><p>
		One novel component of our interactive approach is the ability for the users to
		formulate dynamic queries previously developed for database applications at
		Maryland.
		The interactive interface treats each video clip as
		visual object in a multi-dimensional space,  and each "feature" of that clip is
		mapped to one dimension. The user can visualize any two dimensions by placing
		any two features on the horizontal and vertical axis with additional dimensions
		visualized by adding attributes to each object.</p>
		</div>
  	</dd>

</dl>

<!-- ****************************************************************************************
  PHD et HDR
  ******************************************************************************************* -->

<h2>Theses : PhD, habilitation</h2>

<ul>
	<li id="hdr" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf </a>
	  	</span>
	  	<span class="t-title">
			Modélisation globalement cohérente d'interactions complexes avec prise en compte de critères géométriques.
		</span>
		<span class="t-medium">
			Habilitation thesis, Institut National de Sciences Appliqu&eacute;es de Lyon
		</span>
			France, December 10th, 2012. 195 pages. (French!)
			<ul class="reflink-box">
      	  	<li><a href="javascript:;" onClick="displayAbstractOf('hdr');">Abstract</a></li>
       	 	<li><a href="../papers/hdr2012.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>Les recherches présentées ici traitent d'analyse d'images, de vidéos et de maillages. L'idée directrice est la modélisation d'interactions complexes entre plusieurs variables, le plus souvent réalisée à l'aide de modèles graphiques, généralement probabilistes; la modélisation globalement cohérente d'un problème; la résolution de problèmes complexes par minimisation de fonctions  d'énergie globales; les modèles structurés et semi-structurés : graphes, chaînes, arbres etc.
		</p>
		<p>
		Ces travaux peuvent être globalement regroupés en quatre thèmes applicatifs :
		</p>
		<p>
		(i) Segmentation d'images et de vidéos - les défis de cette thématique résident dans la modélisation de contenus complexes et de dégradations complexes tout en permettant une inférence efficace.
		</p>
		<p>
		(ii) Détection et reconnaissance d'objets - ces travaux se basent essentiellement sur les modèles structurés et semi-structurés. Le verrou scientifique majeur est l'augmentation du pouvoir de discrimination d'un modèle, tout en gardant, ou en augmentant, l'invariance vis-à-vis de transformations diverses comme les changements d'échelle, les rotations, les mouvements articulés, les changements d'éclairage etc. L'inférence efficace reste un souci.
		</p>
		<p>
		(iii) Reconnaissance d'actions - une partie de ces travaux est liée aux travaux sur la reconnaissance d'objets de par leurs contributions théoriques sur les modèles structurés et semi-structurés. Les contributions les plus notables concernent la modélisation d'activités humaines par graphes.
		</p>
		<p>
		(iv) Analyse de maillages - l'objectif de cette thématique est la conception de modèles de graphiques pour les maillages surfaciques en vue de leur analyse, segmentation et filtrage. Dans un contexte de modélisation globalement cohérente, la difficulté principale provient de la structure très irrégulière d'un maillage.
		</p>
		</div>
		<div class="t-bibtex"><pre>
@PhdThesis{WolfPhD2003,
	author = {C. Wolf},
	title = {Text  {D}etection in {I}mages taken from {V}ideos {S}equences for {S}emantic {I}ndexing},
	school = {INSA de Lyon},
	year = {2003},
	address = {20, rue Albert Einstein, 69621 Villeurbanne Cedex, France},
}
		</pre></div>
  	</li>
</ul>



<ul>
	<li id="phd" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf </a>
	  	</span>
	  	<span class="t-title">
			Text detection in images taken from video sequences
			for semantic indexing (fr: D&eacute;tection de textes dans des images
        	issues d'un flux vid&eacute;o pour l'indexation s&eacute;mantique).
		</span>
		<span class="t-medium">
			PhD thesis, Institut National de Sciences Appliqu&eacute;es de Lyon
		</span>
			France, December 3rd, 2003. 211 pages. (English!)
			<ul class="reflink-box">
      	  	<li><a href="javascript:;" onClick="displayAbstractOf('phd');">Abstract</a></li>
       	 	<li><a href="../papers/these.pdf">PDF</a></li>
       	 	<li><a href="../papers/these.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('phd');">BibTeX</a></li>
      		</ul>
		<div class="t-abstract">
		<p>This work situates itself within the framework of image and video indexation. The
		systems currently available for the content based image and video retrieval work
		without semantic knowledge, i.e. they use image processing methods to extract low
		level features of the data. The similarity obtained by these approaches does not
		always correspond to the similarity a human user would expect. A way to include more
		semantic knowledge into the indexing process is to use the text included in the
		images and video sequences. It is rich in information but easy to use.
		</p><p>
		Existing methods for text detection are simple: most of them are based on texture
		estimation or edge detection followed by an accumulation of these characteristics.
		Geometrical contraints are enforced by most of the methods. However, it is done in a
		morphological post-processing step only. It is obvious, that a weak detection is
		very difficult --- up to impossible --- to correct in a post-processing step.
		We propose to take into account the geometrical constraints directly in the
		detection phase. Unfortunately, this is a chicken-egg problem: in order to estimate
		geometrical constraints, we first need to detect text. Consequently, we suggest a
		two-step algorithm: a first coarse detection calculates a text "probability" image.
		Afterwards, for each pixel we calculate geometrical properties of the eventual
		surrounding text rectangle. These features are added to the features of the first
		step and fed into a support vector machine classifier.
		</p><p>
		For the application to video sequences, we propose an algorithm which detects text
		on a frame by frame basis, tracking the found text rectangles accross multiple
		frames. For each text appearance, a single enhanced image is robustly created by
		multiple frame integration.
		</p><p>
		We tackle the character segmentation problem and suggest two different methods: the
		first algorithm maximizes a criterion based on the local contrast in the image. The
		second approach exploits a priori knowledge on the spatial distribution of the text
		and non-text pixels in the image in order to enhance the segmentation decisions. The
		a priori knowledge is learned from training images and stored in a statistical
		Markov random field model. This model is integrated into Bayesian estimation
		framework in order to obtain an estimation of the original binary image.
		</p><p>
		We address the video indexing challenge with a method integrating several features
		extracted from the video. Among others, text extracted with the method mentioned
		above, is one of the informations sources for the indexing algorithm.
		</p>
		</div>
		<div class="t-bibtex"><pre>
@PhdThesis{WolfPhD2003,
	author = {C. Wolf},
	title = {Text  {D}etection in {I}mages taken from {V}ideos {S}equences for {S}emantic {I}ndexing},
	school = {INSA de Lyon},
	year = {2003},
	address = {20, rue Albert Einstein, 69621 Villeurbanne Cedex, France},
}
		</pre></div>
  	</li>
</ul>

<!-- ****************************************************************************************
  TECHNICAL REPORTS
  ******************************************************************************************* -->

<h2>Technical reports and arXive pre-prints</h2>

<ul>

		<li id="arxiv2015b" class="bib-arxive">
		<span class="t-author">
			<a href="https://liris.cnrs.fr/membres?id=2207&onglet=publis">Eric Lombardi</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://scholar.google.com/citations?user=CCCoMqcAAAAJ&hl=en">Oya Celiktutan</a> and
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>.
	  	</span>
	  	<span class="t-title">
		    Activity recognition from videos with parallel hypergraph matching on GPUs.
		</span>
		<span class="t-medium">
			<a href="http://arxiv.org/abs/1505.00581">arxiv:1505.00581</a>
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('arxiv2015b');">
			Abstract</a>
			</li>
			<li><a href="http://arxiv.org/abs/1505.00581">Arxiv-Version</a></li>
			<li><a href="../papers/arxiv2015b.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			In this paper, we propose a method for activity recognition from videos based on sparse local features and hypergraph matching. We benefit from special properties of the temporal domain in the data to derive a sequential and fast graph matching algorithm for GPUs.

			Traditionally, graphs and hypergraphs are frequently used to recognize complex and often non-rigid patterns in computer vision, either through graph matching or point-set matching with graphs. Most formulations resort to the minimization of a difficult discrete energy function mixing geometric or structural terms with data attached terms involving appearance features. Traditional methods solve this minimization problem approximately, for instance with spectral techniques.

			In this work, instead of solving the problem approximatively, the exact solution for the optimal assignment is calculated in parallel on GPUs. The graphical structure is simplified and regularized, which allows to derive an efficient recursive minimization algorithm. The algorithm distributes subproblems over the calculation units of a GPU, which solves them in parallel, allowing the system to run faster than real-time on medium-end GPUs.
		</div>
  	</li>

		<li id="trliris2014mesh" class="bib-empty">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>
	  	</span>.
	  	<span class="t-title">
			Mechanical Mesh Segmentation and Global 3D Shape Extraction
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2014-016
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information (LIRIS)</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France, October 20th, 2014.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2014mesh');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2014mesh.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
			This paper presents a method for segmenting noisy 2-manifold meshes based on a decomposition into local shape primitives maximizing global coherence. This technique works by partitioning the input mesh into regions which can be approximated by a simple geometrical primitive such as a plane, a sphere or a cylinder. The proposed approach is entirely error-driven, convergence-proven, and does not need to specify a number of segments. The partitioning is guided by robust shape extractions based on RANSAC sampling and by a global graphical model which regularizes the segmented regions. The final decomposition is based on the minimum of the energy associated with this graphical model. Obtained segmentations on noisy mechanical meshes outperform other approaches in terms of region contour correctness and consistency with mechanical object decomposition. Applications of this work are reverse engineering, mesh structure analysis, mesh feature enhancement, noise removal, mesh compression, piecewise approximation of mesh geometry, and remeshing.
		</div>
  	 </li>

	<li id="trliris2012harl" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
			<a href="https://liris.cnrs.fr/membres?id=2207&onglet=publis">Eric Lombardi</a>,
			<a href="http://www.busim.ee.boun.edu.tr/~oya/">Oya Celiktutan</a>,
			<a href="http://dblp.uni-trier.de/pers/hd/j/Jiu:Mingyuan">Mingyuan Jiu</a>,
			<a href="http://liris.cnrs.fr/moez.baccouche/">Moez Baccouche</a>,
			<a href="http://perso.ec-lyon.fr/emmanuel.dellandrea/">Emmanuel Dellandréa</a>,
			<a href="http://perso.ec-lyon.fr/charles-edmond.bichot/">Charles-Edmond Bichot</a>,
			<a href="https://liris.cnrs.fr/membres?id=3701&onglet=publis">Christophe Garcia</a>,
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>,


	  	</span>
	  	<span class="t-title">
			The LIRIS Human activities dataset and the ICPR 2012 human activities recognition and localization competition
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2012-004
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information (LIRIS)</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France, March 28th, 2012.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2012harl');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2012harl.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>
		      We describe the LIRIS human activities dataset, the dataset used for the ICPR 2012 human activities recognition and localization competition. In contrast to previous competitions and existing datasets, the tasks focus on complex human behavior involving several people in the video at the same time, on actions involving several interacting people and on human-object interactions. The goal is not only to classify activities, but also to detect and to localize them. The dataset has been shot with two different cameras: a moving camera mounted on a mobile robot delivering grayscale videos in VGA resolution and depth images from a consumer depth camera (Primesense/MS Kinect); and a consumer camcorder delivering color videos in DVD resolution.
		</p>
		</div>
  	 </li>

	<li id="trliris2012" class="bib-empty">
		<span class="t-author">
			<a href="http://www.busim.ee.boun.edu.tr/~oya/">Oya Celiktutan</a>,
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://www.busim.ee.boun.edu.tr/~sankur">Bülent Sankur</a>,
	  	</span>
	  	<span class="t-title">
			Fast Exact Matching and Correspondence with Hyper-graphs on Spatio-temporal Data
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2012-002
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information (LIRIS)</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France, February 2012.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2012');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2012.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>
			Graphs and hyper-graphs are frequently used to recognize complex and often non-rigid patterns in computer vision, either through graph matching or point-set matching with graphs. Most formulations resort to the minimization of a difficult energy function containing geometric or structural terms, frequently coupled with data attached terms involving appearance information. Traditional methods solve the minimization problem approximately, for instance with spectral techniques. In this paper we deal with data embedded in a 3D "space-time", for instance in action recognition applications. We show that, in this context, we can take advantage of special properties of the time domain, in particular causality and the linear order of time. We show that the complexity of the exact matching problem is far inferior to the complexity of the general problem and we derive an algorithm calculating the exact solution. As a second contribution, we propose a new graphical structure which is elongated in time. We argue that, instead of approximately solving the original problem, a better solution can be obtained by exactly solving an approximated problem. An exact minimization algorithm is derived for this structure and successfully applied to action recognition in videos.
		</p>
		</div>
  	 </li>

	<li id="trliris2011" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>, Graham Taylor and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.

	  	</span>
	  	<span class="t-title">
			Learning individual human activities from short binary shape sequences
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2011-018
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information (LIRIS)</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France, October 2011.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2011');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2011.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>
			We present a new machine learning-based algorithm capable of classifying individual human activities from very short sequences. Our method is based on a "deep" multi-stage architecture where each layer is learned independently of the other layers. Low-level shape features are extracted from short sequences of binary shapes and fed to a sequential probabilistic model (a conditional deep belief network), which learns the evolution of the low-level features through time through interactions with binary latent variables. No appearance model is needed. Actions are classified using an SVM trained on the posterior probabilities of the latent features extracted by the motion model. The method is capable of not only recognizing actions but also localizing them in space and time. We evaluated the algorithm on two different databases, the well known Weizmann dataset and our own, more challenging, dataset.
		</p>
		</div>
  	 </li>

	<li id="trliris2010" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>, Graham Taylor and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.

	  	</span>
	  	<span class="t-title">
			Learning individual human activities from short binary shape sequences
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2010-010
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information (LIRIS)</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. June 1st,
			2010. This technical report has been replaced by the more recent
			<a href="#trliris2011">LIRIS-RR-2011-018</a>.

  	 </li>

	<li id="trliris2009" class="bib-empty">
		<span class="t-author">
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a> and
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a>,

	  	</span>
	  	<span class="t-title">
			An iterative approach for global triangular mesh regularization
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2009-032
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information (LIRIS)</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France, 2009.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2009');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2009.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>
			This paper presents a global mesh optimization framework for 3D triangular meshes of arbitrary topology. The mesh optimization task is formulated as an energy minimization problem including data attached terms measuring the fidelity to the original mesh as well as a shape potential favoring high quality triangles. Since the best solution for vertex relocation is strongly related to the mesh connectivity, our approach iteratively modifies this connectivity (edge and vertex addition/removal) as well as the vertex positions. Good solutions for the energy function minimization are obtained by a discrete graph cut algorithm examining global combinations of local candidates. Results on various 3D meshes compare favorably to recent state-of-the-art algorithms regarding the trade-off between triangle shape improvement and surface fidelity. Applications of this work mainly consist in regularizing meshes for numerical simulations, for improving mesh rendering or for improving the geometric prediction in mesh compression techniques.
		</p>
		</div>
  	 </li>

	<li id="trliris2008-2" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and G&eacute;rald Gavin

	  	</span>
	  	<span class="t-title">
			Inference and parameter estimation on hierarchical belief networks for image segmentation
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2008-21
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. October 21th,
			2008. 12 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2008-2');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2008-markovcube.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>
		We introduce a new causal hierarchical belief network for image segmentation. Contrary to classical tree structured (or pyramidal) models, the factor graph of the network contains cycles. Each level of the hierarchical structure features the same number of sites as the base level and each site on a given level has several neighbors on the parent level. Compared to tree structured models, the (spatial) random process on the base level of the model is stationary which avoids known drawbacks, namely visual artifacts in the segmented image.
We propose different parameterizations of the conditional probability distributions governing the transitions between the image levels. A parametric distribution depending on a single parameter allows the design of a fast inference algorithm on graph cuts, whereas for arbitrary distributions, we propose inference with loopy belief propagation. The method is evaluated on scanned document images from the 18th century, showing an improvement of character recognition results compared to other methods.
		</p>
		</div>
  	 </li>

	<li id="trliris2008" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			An iterative graph cut optimization algorithm for a double MRF prior
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2008-17
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. July 19th,
			2008. 14 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2008');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2008-doublemrf.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>In a previous publication we presented a double MRF model capable of separatly regularizing the recto and verso side of a document suffering from ink bleed through. In this paper we show that this model naturally leads to an efficient optimization method based on the minimum cut/maximum flow in a graph. The proposed method is evaluated on scanned document images from the 18th century, showing an improvement of character recognition results compared to other restoration methods.</p>
		</div>
  	 </li>

	<li id="trliris2006" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Document Ink bleed-through removal with two hidden Markov random fields and a single observation field
		</span>
		<span class="t-medium">
			Technical Report RR-LIRIS-2006-019
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. November 26th,
			2006. 14 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2006');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-bleedthrough-2006.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>We present a new method for blind document bleed through removal based on separate Markov Random Field (MRF) regularization for the recto and for the verso side. The segmentation algorithm is based on Bayesian Maximum a Posteriori (MAP) estimation, where the prior model is made of two conditionally independent MRFs with a single observation field. The advantages of this separate approach are the adaptation of the prior to the contents creation process (e.g. superimposing two hand written pages), and the improvement of the estimation of the verso pixels through an estimation of the verso pixels covered by recto pixels. Optimization is carried out with the simulated annealing algorithm. The labels of the initial recto and verso clusters are recognized without using any color or gray value information. The proposed method is evaluated on synthetic images as well as scanned document images. The results on real scanned data have been evaluated using statistical evaluation on an empirical test performed by 16 people.</p>
		</div>
  	 </li>

	<li id="trliris2005" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms
		</span>
		<span class="t-medium">
			Technical Report LIRIS-RR-2005-024
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. September 28th,
			2005. 28 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2005');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2005-wolfjolion.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		<p>Evaluation of object detection algorithms is a non-trivial task: a detection result is usually evaluated by comparing the bounding box of the detected object with the bounding box of the ground truth object. The commonly used precision and recall measures are computed from the overlap area of these two rectangles. However, these measures have several drawbacks: they don't give intuitive information about the proportion of the correctly detected objects and the number of false alarms, and they cannot be accumulated across multiple images without creating ambiguity in their interpretation. Furthermore, quantitative and qualitative evaluation is often mixed resulting in ambiguous measures.</p>
		<p>In this paper we propose a new approach which tackles these problems. The performance of a detection algorithm is illustrated intuitively by performance graphs which present object level precision and recall depending on constraints on detection quality. In order to compare different detection algorithms, a representative single performance value is computed from the graphs. The influence of the test database on the detection performance is illustrated by performance/generality graphs. The evaluation method can be applied to different types of object detection algorithms. It has been tested on different text detection algorithms, among which are the participants of the ICDAR 2003 text detection competition.</p>
		</div>
  	 </li>

	 <li id="trliris2004" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Model based text detection in images and videos: a learning approach.
		</span>
		<span class="t-medium">
			Technical Report LIRIS-RR-2004-13
		</span>
			<a href="http://liris.cnrs.fr">
			Laboratoire d'Informatique en Images et Syst&egrave;mes d'Information</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. March 19th,
			2004. 24 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trliris2004');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-liris-2004-13.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		 Existing methods for text detection in images are simple: most of them are based on
		 texture estimation or edge detection followed by an accumulation of these
		 characteristics. Geometrical constraints are enforced by most of the methods.
		 However, it is done in a morphological post-processing step only. It is obvious,
		 that a weak detection is very difficult --- up to impossible --- to correct in a
		 post-processing step. We propose a text model which takes into account the
		 geometrical constraints directly in the detection phase: a first coarse detection
		 calculates a text "probability" image. After wards, for each pixel we calculate
		 geometrical properties of the eventual surrounding text rectangle. These features
		 are added to the features of the first step and fed into a support vector machine
		 classifier.
		</div>
  	</li>

	<li id="trrfv2002" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Extraction and Recognition of Artificial Text in Multimedia Documents,
		</span>
		<span class="t-medium">
			Technical Report RFV-RR-2002.01,
		</span>
			<a href="http://rfv.insa-lyon.fr"> Laboratoire Reconnaissance de Formes et
			Vision</a>,
			<a href="http://www.insa-lyon.fr">INSA de Lyon</a>, France. February 22nd,
			2002. 42 pages.

		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trrfv2002');">
			Abstract</a>
		</li>
        	<li><a href="../papers/tr-rfv-2002-01.pdf">PDF</a></li>
      		</ul>

		<div class="t-abstract">
		 The systems currently available for content based image and video retrieval work
		 without semantic knowledge, i.e. they use image processing methods to extract low
		 level features of the data. The similarity obtained by these approaches does not
		 always correspond to the similarity a human user would expect. A way to include
		 more semantic knowledge into the indexing process is to use the text included in
		 the images and video sequences. It is rich in information but easy to use, e.g. by
		 key word based queries. In this paper we present an algorithm to localize
		 artificial text in images and videos using a measure of accumulated gradients and
		 morphological processing. The quality of the localized text is improved by robust
		 multiple frame integration. A new technique for the binarization of the text boxes
		 based on a criterion maximizing local contrast is proposed. Finally, detection and
		 OCR results for a commercial OCR are presented, justifying the choice of the
		 binarization technique.
		</div>
  	</li>

	<li id="trprip2000" class="bib-empty">
		<span class="t-author">
			<a href="../index.html">Christian Wolf </a>.
	  	</span>
	  	<span class="t-title">
			Content based image retrieval using interest points and texture features,
		</span>
		<span class="t-medium">
			Technical Report PRIP-TR-061
		</span>
			<a href="http://www.prip.tuwien.ac.at">Pattern Recognition and Image
			Processing Group</a>,
			Institute of Computer Aided Automation,
			<a href="http://www.tuwien.ac.at">Vienna University of Technology</a>,
			April 2000. 110 pages.

		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('trprip2000');">Abstract</a></li>
        	<li><a href="../papers/tr-prip.ps.gz">Postscript</a></li>
      		</ul>
		<div class="t-abstract">
		 Content based image retrieval is the task of searching images from a database,
		 which are visuall&eacute;y similar to a given example image. Since there is no general
		 definition for visual similarity, there are different possible ways to query for
		 visual content. In this work we present methods for content based image retrieval
		 based on texture similarity using interest points and Gabor features. Interest
		 point detectors are used in computer vision to detect image points with special
		 properties, which can be geometric (corners) or non-geometric (contrast etc.).
		 Gabor functions and Gabor filters are regarded as excellent tools for texture
		 feature extraction and texture segmentation. We present methods how to combine
		 these methods for content based image retrieval and to generate a texture
		 description of images. Special emphasis is devoted to distance measures for the
		 texture descriptions. Experimental results of the query system on different test
		 image databases are given.
		</div>
  	</li>
</ul>

</body>
</html>
