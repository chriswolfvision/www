<!-- ****************************************************************************************
  Author: Christian Wolf
  Begin: 19.4.2005
  ******************************************************************************************* -->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
	<title>Christian Wolf's Publications</title>
  	<link rel=stylesheet href="../main.css" type="text/css">
	<link rel=stylesheet href="publications.css" type="text/css">
	<script type="text/javascript" language="JavaScript" src="publications.js"></script>
	<link rel="shortcut icon" href="../graphics/favicon.ico">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
</head>

<!-- ****************************************************************************************
  For google analytics
  **************************************************************************************** -->

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1423815-1";
urchinTracker();
</script>

<body  onload="startPage()">

<!-- ****************************************************************************************
  The nav menu box  
  ******************************************************************************************* -->

<div class="navmenu">
	<ul>
		<li class="first"><a href="../index.html">Main</a></li>
		<li> <a href="../research/index.html">Research</a></li>
		<li> <a href="../teaching/index.html">Teaching</a></li>
		<li> <a href="../cv/index.html">CV</a></li>
		<li> <a href="../publications/index_bydate.html">Publications</a></li>
		<li> <a href="../publications/pres.html">Presentations</a></li>
		 
		<li> <a href="../software/index.html">Software</a></li>
		<li> <a href="http://mady.n.free.fr/chris/noah">Private</a></li>
	</ul>
</div>  

<h1>Publications</h1>

<div style="text-align: center">
	<p>
	<a href="index.html">By Type</a> | by Date | <a href="https://liris.cnrs.fr/membres?id=833&onglet=publis">Liris-DB</a>
	</p>

	<p>
	Refereed publications with international audience only (patents, French publications and technical reports are <a href="index.html">here</a>)
	</p>
</div>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2011</h2>

<ul>

	<li id="tvc2011" class="bib-journal">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>
	  	</span>
	  	<span class="t-title">
			Combinatorial Mesh Optimization,
		</span>
		<span class="t-medium"> 
			To appear in The Visual Computer,
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('tvc2011');">Abstract</a></li>
        	<li><a href="../papers/visualcomputer2011.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		
		</p>
		</div>
  	</li>
	
	<li id="hbu2011" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/christophe.garcia">Christophe Garcia</a> and 
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>,	
	  	</span>
	  	<span class="t-title">
			Sequential Deep Learning for Human Action Recognition,	
		</span>
		<span class="t-medium"> 
			to appear in the Proceedings of the Inernational Workshop on Human Behavior Understanding: Inducing Behavioral Change, 2011.
		</span>
  	</li>

	<li id="grapp2011" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>
	  	</span>
	  	<span class="t-title">
			Robust feature line extraction on CAD triangular meshes,	
		</span>
		<span class="t-medium"> 
			in the Proceedings of the International Conference on Computer Graphics Theory and Applications, 2011
		</span>
		(Oral presentation).
  	</li>
</ul>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2010</h2>

<ul>
	<li id="pami2009" class="bib-journal">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Document Ink bleed-through removal with two hidden Markov random fields and a single observation field.
		</span>
		<span class="t-medium"> 
			To appear in IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 32(3):431-447, 2010.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('pami2009');">Abstract</a></li>
        	<li><a href="../papers/pami2009.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		We present a new method for blind document bleed through removal based on separate Markov Random Field (MRF) regularization for the recto and for the verso side, where separate priors are derived from the full graph. The segmentation algorithm is based on Bayesian Maximum a Posteriori (MAP) estimation. The advantages of this separate approach are the adaptation of the prior to the contents creation process (e.g. superimposing two hand written pages), and the improvement of the estimation of the recto pixels through an estimation of the verso pixels covered by recto pixels; Moreover, the formulation as a binary labeling problem with two hidden labels per pixels naturally leads to an efficient optimization method based on the minimum cut/maximum flow in a graph. The proposed method is evaluated on scanned document images from the 18th century, showing an improvement of character recognition results compared to other restoration methods.
		</p>
		</div>
  	</li>	

	<li id="neuro2009" class="bib-journal">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian Wolf</a> and G&eacute;rald Gavin
	  	</span>
	  	<span class="t-title">
			Inference and parameter estimation on hierarchical belief networks for image segmentation.
		</span>
		<span class="t-medium"> 
			In Neurocomputing 73(4-6):563-569, 2010.
		</span>
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('neuro2009');">Abstract</a></li>
        	<li><a href="../papers/neurocomputing2009.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		We introduce a new causal hierarchical belief network for image segmentation. Contrary to classical tree structured (or pyramidal) models, the factor graph of the network contains cycles. Each level of the hierarchical structure features the same number of sites as the base level and each site on a given level has several neighbors on the parent level. Compared to tree structured models, the (spatial) random process on the base level of the model is stationary which avoids known drawbacks, namely visual artifacts in the segmented image. 
		We propose different parameterizations of the conditional probability distributions governing the transitions between the image levels. A parametric distribution depending on a single parameter allows the design of a fast inference algorithm on graph cuts, whereas for arbitrary distributions, we propose inference with loopy belief propagation. The method is evaluated on scanned documents, showing an improvement of character recognition results compared to other methods.
		</p>
		</div>
  	</li>

	<li id="icpr2010w" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>
	  	</span>
	  	<span class="t-title">
			Integrating a discrete motion model into GMM based background subtraction,	
		</span>
		<span class="t-medium"> 
			in the Proceedings of the IEEE International Conference on Pattern Recognition, 2010.
		</span>
		(Oral presentation).
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2010w');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2010-bgsub.pdf">PDF</a></li>
		<li><a href="../papers/pres-icpr2010.pdf">Presentation</a></li>
      		</ul>
		<div class="t-abstract">
			GMM based algorithms have become the de facto standard for background subtraction in video sequences, mainly because of their ability to track multiple background distributions, which allows them to handle complex scenes including moving trees, flags moving in the wind etc. However, it is not always easy to determine which distributions of the mixture belong to the background and which distributions belong to the foreground, which disturbs the results of the labeling process for each pixel. In this work we tackle this problem by taking the labeling decision together for all pixels of several consecutive frames minimizing a global energy function taking into account spatial and temporal relationships. A discrete approximative optical-flow like motion model is integrated into the energy function and solved with Ishikawa's convex graph cuts algorithm. 
		</div>
  	</li>

	<li id="icpr2010p" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/anh-phuong.ta">Anh-Phong Ta</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a>,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>
	  	</span>
	  	<span class="t-title">
			Pairwise features for human action recognition,	
		</span>
		<span class="t-medium"> 
			to appear in the Proceedings of the IEEE International Conference on Pattern Recognition, 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2010p');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2010-phuong.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			Existing action recognition approaches mainly rely on the discriminative power of individual local descriptors extracted from spatio-temporal interest points (STIP), while the geometric relationships among the local features are ignored. This paper presents new features, called pairwise features (PWF), which encode both the appearance and the spatio-temporal relations of the local features for action recognition. First STIPs are extracted, then PWFs are constructed by grouping pairs of STIPs which are both close in space and close in time. We propose a combination of two codebooks for video representation. Experiments on two standard human action datasets: the KTH dataset and the Weizmann dataset show that the proposed approach outperforms most existing methods.
		</div>
  	</li>

	<li id="avss2010" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/anh-phuong.ta">Anh-Phong Ta</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a> and
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> 
			
	  	</span>
	  	<span class="t-title">
			Recognizing and localizing individual activities through graph matching,	
		</span>
		<span class="t-medium"> 
			in the Proceedings of the International Conference on Advanced Video and Signal-Based Surveillance, 2010 (IEEE).
		</span>
		,oral presentation, 22.5% acceptance rate; <span class="t-emph">Best Paper</span> for track 'recognition', 5% acceptance rate.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('avss2010');">
			Abstract</a>
		</li>
		<li><a href="../papers/avss2010.pdf">PDF</a></li>	
      		</ul>
		<div class="t-abstract">
			In this paper we tackle the problem of detecting individual human actions in video sequences. While the most successful methods are based on local features, which proved that they can deal with changes in background, scale and illumination, most existing methods have two main shortcomings: first, they are mainly based on the individual power of spatio-temporal interest points (STIP), and therefore ignore the spatio-temporal relationships between them. Second, these methods mainly focus on direct classification techniques to classify the human activities, as opposed to detection and localization. In order to overcome these limitations, we propose a new approach, which is based on a graph matching algorithm for activity recognition. In contrast to most previous methods which classify entire video sequences, we design a video matching method from two sets of ST-points for human activity recognition. First, points are extracted, and a hyper graphs are constructed from them, i.e. graphs with edges involving more than 2 nodes (3 in our case). The activity recognition problem is then transformed into a problem of finding instances of model graphs in the scene graph. By matching local features instead of classifying entire sequences, our method is able to detect multiple different activities which occur simultaneously in a video sequence. Experiments on two standard datasets demonstrate that our method is comparable to the existing techniques on classification, and that it can, additionally, detect and localize activities.
		</div>
  	</li>

	<li id="gi2010" class="bib-conf">	  
		<span class="t-author"> 		
			Pierre-Yves Laffont, Jong-Yun Jun, 			
			<a href="../index.html">Christian Wolf</a>,
			Yu-Wing Tai, 
			<a href="http://liris.cnrs.fr/khalid.idrissi">Khalid Idrissi</a>,
			George Drettakis, Sung-Eui Yoon,
	  	</span>
	  	<span class="t-title">
			Interactive Content-Aware Zooming,
		</span>
		<span class="t-medium"> 
			to appear in the Proceedings of Grapĥics Interface, 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('gi2010');">
			Abstract</a>
		</li>		
		<li><a href="../papers/gi2010.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	We propose a novel, interactive content-aware zooming operator that allows effective and efficient visualization of high resolution images on small screens, which may have different aspect ratios compared to the input images. Our approach applies an image retargeting method in order to fit an entire image into the limited screen space. This can provide global, but approximate views for lower zoom levels. However, as we zoom more closely into the image, we continuously unroll the distortion to provide local, but more detailed and accurate views for higher zoom levels. In addition, we propose to use an adaptive view-dependent mesh to achieve high retargeting quality, while maintaining interactive performance. We demonstrate the effectiveness of the proposed operator by comparing it against the traditional zooming approach, and a method stemming from a direct combination of existing works.
		</div>
 	 </li>

	<li id="icann2010" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/moez.baccouche">Moez Baccouche</a>,
			Frank Mamalet
			<a href="../index.html">Christian Wolf</a>,
			Christophe Garcia, 
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> 	
	  	</span>
	  	<span class="t-title">
			Action Classifcation in Soccer Videos with Long Short-Term Memory Recurrent Neural Networks 
		</span>
		<span class="t-medium"> 
			to appear in the Proceedings of the International Conference on Artificial Neural Networks, 2010.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icann2010');">
			Abstract</a>
		</li>		
		<li><a href="../papers/icann2010.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
			In this paper, we propose a novel approach for action classification in soccer videos using a recurrent neural network scheme. Thereby, we extract from each video action at each timestep a set of features which describe both the visual content (by the mean of a BoW approach) and the dominant motion (with a key point based approach). A Long Short-Term Memory-based Recurrent Neural Network is then trained to classify each video sequence considering the temporal evolution of the features for each timestep. Experimental results on the MICC-Soccer-Actions-4 database show that the proposed approach outperforms classification methods of related works (with a classification rate of 77 %), and that the combination of the two features (BoW and dominant motion) leads to a classification rate of 92 %.
		</div>
 	 </li>
</ul>


<!-- ****************************************************************************************
  2009
  ******************************************************************************************* -->

<h2>2009</h2>

<ul>	
	<li class="bib-empty">	  
		Ranked 5th of 43 in the ICDAR 2009 
		<a href="http://www.cvc.uab.es/icdar2009/papers/3725b375.pdf">document image binarisation contest</a>!
  	</li>
	
	<li id="cbmi2009p" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/anh-phuong.ta">Anh-Phong Ta</a>,
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavoué</a>,
			<a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a>
	  	</span>
	  	<span class="t-title">
			3D Object detection and viewpoint selection in sketch images using local patch-based Zernike moments,
		</span>
		<span class="t-medium"> 
			in the Proceedings of the IEEE Workshop on Content Based Multimedia Indexing, pp. 189-194, 2009.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cbmi2009p');">
			Abstract</a>
			</li>
		<li><a href="../papers/cbmi2009-phuong.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	In this paper we present a new approach to detect and recognize 3D models in 2D storyboards which have been drawn during the production process of animated cartoons. Our method is robust to occlusion, scale and rotation. The lack of texture and color makes it difficult to extract local features of the target object from the sketched storyboard. Therefore the existing approaches using local descriptors like interest points can fail in such images.  We propose a new framework which combines patch-based Zernike descriptors with a method enforcing spatial constraints for exactly detecting 3D models represented as a set of 2D views in the storyboards. Experimental results show that the proposed method can deal with partial object occlusion and is suitable for poorly textured objects.
		</div>
  </li>

  <li id="cbmi2009m" class="bib-conf">	  
		<span class="t-author"> 		
			Marc Mouret,
			<a href="http://liris.cnrs.fr/christine.solnon">Christine Solnon</a>,
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Classification of images based on Hidden Markov Models,
		</span>
		<span class="t-medium"> 
			in the Proceedings of the IEEE Workshop on Content Based Multimedia Indexing, pp. 169-174, 2009.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('cbmi2009m');">
			Abstract</a>
			</li>
		<li><a href="../papers/cbmi2009-marc.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	We propose to use hidden Markov models (HMMs) to classify images. Images are modeled by extracting symbols corresponding to 3x3 binary neighborhoods of interest points, and by ordering these symbols by decreasing saliency order, thus obtaining strings of symbols. HMMs are learned from sets of strings modeling classes of images. The method has been tested on the SIMPLIcity database and shows an improvement over competing approaches based on interest points. We also evaluate these approaches for classifying thumbnail images, i.e., low resolution images.
		</div>
 	 </li>

	<li id="sgp2009" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/vincent.vidal">Vincent Vidal,</a>
			<a href="../index.html">Christian Wolf</a>,
			<a href="http://liris.cnrs.fr/florent.dupont">Florent Dupont</a>,
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavou&eacute;</a>
	  	</span>
	  	<span class="t-title">
			Global triangular mesh regularization using conditional Markov random fields.
		</span>
		<span class="t-medium"> 
			Poster (refereed but not published, acceptance rate ~35%) at Symposium on Geometry Processing, 2009
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('sgp2009');">
			Abstract</a>
			</li>
      		</ul>
		<div class="t-abstract">
			We present a global mesh optimization framework based on a Conditional Markov Random Fied (CMRF or CRF) model suited for 3D triangular meshes of arbitrary topology. The remeshing task is formulated as a Bayesian estimation problem including data attached terms measuring the fidelity to the original mesh as well as a prior favoring high quality triangles. Since the best solution for vertex relocation is strongly related to the mesh connectivity, our approach iteratively modifies the mesh structure (connectivity plus vertex addition/removal) as well as the vertex positions, which are moved according to a well-defined energy function resulting from the CMRF model. Good solutions for the proposed model are obtained by a discrete graph cut algorithm examining global combinations of local candidates. Results on various 3D meshes compare favorably to recent state-of-the-art algorithms regarding the trade-off between triangle shape improvement and surface fidelity. Applications of this work mainly consist in regularizing meshes for numerical simulations and for improving mesh rendering.
		</div>
 	 </li>

     	<li id="mlsp2009" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Families of Markov models for document image segmentation,
		</span>
		<span class="t-medium"> 
			to appear in the Proceedings of the IEEE Machine Learning for Signal Processing Workshop, 2009
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('mlsp2009');">
			Abstract</a>
		</li>
		<li><a href="../papers/mlsp2009.pdf">PDF</a></li>	
      		</ul>
		<div class="t-abstract">
			In this paper we compare several directed and undirected graphical models for different image segmentation problems in the domain of document image processing and analysis. We show that adapting the structure of the model to specific sitations at hand, for instance character restoration, recto/verso separation and segmenting high resolution character images, can significantly improve segmentation performance. We propose inference algorithms for the different models and we test them on different data sets.
		</div>
  </li>
   
	

</ul>

<h2>2008</h2>

<ul>
	<li id="icpr2008" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian Wolf</a>,
	  	</span>
	  	<span class="t-title">
			Improving recto document side restoration with an estimation of the verso side from a single scanned page
		</span>
		<span class="t-medium"> 
			In the Proceedings of the IEEE International Conference on Pattern Recognition, pp. 1-4, 2008.
		</span>.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icpr2008');">
			Abstract</a>
		</li>
		<li><a href="../papers/icpr2008.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
	We present a new method for blind document bleed through removal based on separately restoring the recto and the verso side. The segmentation algorithm is based on separate Markov random fields (MRF) which results in a better adaptation of the prior to the content creation process (e.g. superimposing two pages), and the improvement of the estimation of the verso pixels through an estimation of the verso pixels covered by recto pixels. The labels of the initial recto and verso clusters are recognized without using any color or gray value information. The proposed method is evaluated empirically as well as through OCR improvement. 
		</div>
  	</li>

	<li id="eg2008" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://liris.cnrs.fr/guillaume.lavoue">Guillaume Lavou&eacute;</a> and <a href="../index.html">Christian Wolf</a>
	  	</span>, 
	  	<span class="t-title">
			Markov Random Fields for Improving 3D Mesh Analysis and Segmentation,
		</span>
		<span class="t-medium"> 
			In the Proceedings of the Eurographics 2008 Workshop on 3D Object Retrieval.
		</span>
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('eg2008');">
			Abstract</a>
			<li><a href="../papers/eurographics2008.pdf">PDF</a></li>
		</li>
      		</ul>
		<div class="t-abstract">
    Abstract
Mesh analysis and clustering have became important issues in order to improve the efficiency of common processing
operations like compression, watermarking or simplification. In this context we present a new method for
clustering / labeling a 3D mesh given any field of scalar values associated with its vertices (curvature, density,
roughness etc.). Our algorithm is based on Markov Random Fields, graphical probabilistic models. This Bayesian
framework allows (1) to integrate both the attributes and the geometry in the clustering, and (2) to obtain an optimal
global solution using only local interactions, du to the Markov property of the random field. We have defined
new observation and prior models for 3D meshes, adapted from image processing which achieve very good results
in terms of spatial coherency of the labeling. All model parameters are estimated, resulting in a fully automatic
process (the only required parameter is the number of clusters) which works in reasonable time (several seconds).
		</div>
  	</li>

	
</ul>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2007</h2>

<ul>
	<li id="imageeval2007" class="bib-conf">	  
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a> and
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>
	  	</span>
	  	<span class="t-title">
			Quality, quantity and generality in the evaluation of object detection algorithms
		</span>		
		<span class="t-medium">
			Proceedings of the Image Eval Conference,
		</span>	
			July 12th, 2007, Amsterdam, NL. 8 pages.
			
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('imageeval2007');">Abstract</a></li>
			<li><a href="../papers/imageeval2007.pdf">PDF</a></li>		
      	</ul>
		
		<div class="t-abstract">
		<p>
		Evaluation of object detection algorithms is a non-trivial task: a detection 
		result is usually evaluated by comparing the bounding box of the detected 
		object with the bounding box of the ground truth object. The commonly used 
		precision and recall measures are computed from the overlap area of these two 
		rectangles. However, these measures have several drawbacks: they don't give 
		intuitive information about the proportion of the correctly detected objects 
		and the number of false alarms, and they cannot be accumulated across multiple 
		images without creating ambiguity in their interpretation. Furthermore, 
		quantitative and qualitative evaluation is often mixed resulting in ambiguous 
		measures.
		</p>

		<p>
		In this paper we propose an approach to evaluation which tackles these problems. 
		The performance of a detection algorithm is illustrated intuitively by performance 
		graphs which present object level precision and recall depending on 
		constraints on detection quality. In order to compare different detection 
		algorithms, a representative single performance value is computed from the graphs. 
		The evaluation method can be applied to different types of object detection 
		algorithms. It has been tested on different text detection algorithms, among which 
		are the participants of the Image Eval text detection competition.
		</p>
		</div>
  	</li>
	
</ul>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2006</h2>

<ul>
	<li id="ijdar2006" class="bib-journal">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian Wolf</a> and 
       		<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms,
		</span>
			In			
		<span class="t-medium"> 
			International Journal on Document Analysis and Recognition
		</span>, 8(4):280-296, 2006.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('ijdar2006');">	Abstract</a></li>
        	<li><a href="../papers/tr-liris-2005-wolfjolion.pdf">Similar-Content-PDF</a></li>
        	<li><a href="javascript:;" onClick="displayBibtexOf('ijdar2006');">BibTeX</a></li>
        	<li><a href="../software/deteval/index.html">Software</a></li>
      		</ul>
		<div class="t-abstract">
		<p>
		Evaluation of object detection algorithms is a non-trivial task: a detection result is usually 
		evaluated by comparing the bounding box of the detected object with the bounding box of the 
		ground truth object. The commonly used precision and recall measures are computed from the overlap 
		area of these two rectangles. However, these measures have several drawbacks: they don't give 
		intuitive information about the proportion of the correctly detected objects and the number of 
		false alarms, and they cannot be accumulated across multiple images without creating ambiguity 
		in their interpretation. Furthermore, quantitative and qualitative evaluation is often mixed 
		resulting in ambiguous measures.
		</p><p>
		In this paper we propose a new approach which tackles these problems. The performance of a 
		detection algorithm is illustrated intuitively by performance graphs which present object level 
		precision and recall depending on constraints on detection quality. In order to compare 
		different detection algorithms, a representative single performance value is computed from the 
		graphs. The influence of the test database on the detection performance is illustrated by
		performance/generality graphs. The evaluation method can be applied to different types of object 
		detection algorithms. It has been tested on different text detection algorithms, among which are 
		the participants of the ICDAR 2003 text detection competition.
		</p>
		</div>
		<div class="t-bibtex">
		<PRE>
@Article{WolfIJDAR2006,
  Author         = {C. Wolf and J.-M. Jolion},
  Title          = {Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms},
  Journal        = {International Journal on Document Analysis and Recognition},
  year           = {2006},
  volume     = {8},
  number     = {4},
  pages      = {280-296}
}
		</PRE>
		</div>
  	</li>
	
</ul>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2005</h2>

<ul>
	<li id="ijdar2004simon" class="bib-journal">	  
		<span class="t-author"> 		
			S.M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, R. Young, 
      			K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto, H. Miyao, J. Zhu, W. Ou, 
			<a href="../index.html">C. Wolf</a>, 
			<a href="http://rfv.insa-lyon.fr/~jolion">J.-M. Jolion</a>, 
			L. Todoran, M. Worring, 
      		et X. Lin.
	  	</span>
	  	<span class="t-title">
			ICDAR 2003 Robust Reading Competitions: Entries, Results and 
			Future Directions
		</span>
		<span class="t-medium"> 
			International Journal on Document Analysis and Recognition (IJDAR),
		</span>
			7(2-3):105-122, 2005 
			(Special Issue on Camera-based Text and Document Recognition)
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('ijdar2004simon');">
			Abstract</a>
		</li>        
      		</ul>
		<div class="t-abstract">
		This paper describes the robust reading competitions for ICDAR 2003. With the rapid 
		growth in research over the last few years on recognizing text in natural scenes, 
		there is an urgent need to establish some common benchmark datasets, and
		gain a clear understanding of the current state of the art. We use the term robust 
		reading to refer to text images that are beyond the capabilities of current 
		commercial OCR packages. We chose to break down the robust reading problem into 
		three sub-problems, and run competitions for each stage, and also a competition for 
		the best overall system. The sub-problems we chose were text locating, character 
		recognition and word recognition. By breaking down the problem in this way, we hoped 
		to gain a better understanding of the state of the art in each of the sub-problems. 
		Furthermore, our methodology involved storing detailed results of applying each 
		algorithm to each image in the data sets, allowing researchers to study in depth the 
		strengths and weaknesses of each algorithm. The text locating contest was the only 
		one to have any entries. We give a brief description of each entry, and present the 
		results of this contest, showing cases where the leading entries succeed and fail. 
		We also describe an algorithm for combining the outputs of the individual text 
		locaters, and show how the combination scheme improves on any of the individual 
		systems.
		</div>
  	</li>
	
</ul>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2004</h2>

<ul>

	<li id="icict2004" class="bib-conf">	  
		<span class="t-author"> 		
			<a href="http://www.eng.uwaterloo.ca/%7Egwtaylor">Graham W.  
        	Taylor</a> and <a href="../index.html">Christian Wolf</a>
	  	</span>
	  	<span class="t-title">
			Reinforcement Learning for Parameter Control of Text Detection in Images 
        	and Video Sequences
		</span>
		<span class="t-medium"> 
			Proceedings of the IEEE International Conference 
        	on Information &amp; Communication Technologies
		</span>,
      			2004.
			6 pages.
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('icict2004');">
			Abstract</a>
		</li>
        	<li><a href="../papers/icict2004.pdf">PDF</a></li>
      		</ul>
		<div class="t-abstract">
		A framework for parameterization in computer vision algorithms is evaluated by 
		optimizing ten parameters of the text detection for semantic indexing algorithm 
		preposed by Wolf et al. The Fuzzy ARTMAP neural network is used for generalization, 
		offering much faster learning than in a previous tabular implementation. 
		Difficulties in using a continuous action space are overcome by employing the DIRECT 
		method for global optimization without derivatives. The chosen parameters are 
		evaluated using metrics of recall and precision, and are shown to be superior to the 
		parameters previously recommended.
		</div>
  	</li>
	
</ul>

<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2003</h2>

<ul>

	<li id="paa2003" class="bib-journal">	  
		<span class="t-author"> 		
			<a href="../index.html">Christian 
       		Wolf</a> and <a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion</a>.
	  	</span>
	  	<span class="t-title">
			Extraction and Recognition of Artificial Text in Multimedia Documents. 
		</span>
		<span class="t-medium"> Pattern Analysis and Applications,
		</span>
		6(4):309-326, 2003.
		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('paa2003');">	Abstract</a></li>
        	<li><a href="../papers/tr-rfv-2002-01.pdf">Similar-Content-PDF</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('paa2003');">BibTeX</a></li>
        	<li><a href="http://telesun.insa-lyon.fr/%7Ewolf/demos/textdetect.html">Demo</a></li>
      		</ul>
		<div class="t-abstract">
		The systems currently available for content based image and video retrieval 
		work without semantic knowledge, i.e. they use image processing methods to 
		extract low level features of the data. The similarity obtained by these 
		approaches does not always correspond to the similarity a human user would expect. 
		A way to include more semantic knowledge into the
		indexing process is to use the text included in the images and video sequences. 
		It is rich in information but easy to use, e.g. by key word based queries. In 
		this paper we present an algorithm to localize artificial text in images and 
		videos using a measure of accumulated gradients and morphological processing. 
		The quality of the localized text is improved by robust multiple frame integration.
		A new technique for the binarization of the text boxes based on a criterion 
		maximizing local contrast is proposed. Finally, detection and OCR results for a 
		commercial OCR are presented, justifying the choice of the binarization technique
		</div>
		<div class="t-bibtex">
		<PRE>
@Article{WolfPAA03,
  Author         = {C. Wolf and J.-M. Jolion},
  Title          = {Extraction and {R}ecognition of {A}rtificial {T}ext in {M}ultimedia {D}ocuments},
  Journal        = {Pattern {A}nalysis and {A}pplications},
  year           = {2003},
  volume     = {6},
  number     = {4},
  pages      = {309-326}
}
		</PRE>
		</div>
  	</li>
	
</ul>


<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2002</h2>

<ul>
	
	<li id="icpr2002v" class="bib-conf">	  
		<span class="t-author"> 
			<a href="../index.html">Christian Wolf </a>,<a href="http://rfv.insa-lyon.fr/%7Ejolion"> 
			Jean-Michel Jolion</a> and Francoise Chassaing. 
	  	</span>			
	  	<span class="t-title">
			Text Localization, Enhancement and Binarization in Multimedia Documents
		</span>		
		<span class="t-medium">
			Proceedings of the International Conference on Pattern Recognition (ICPR),
		</span>
			volume 4, pages 1037-1040, IEEE Computer Society. 
        	August 11th-15th, 2002, Quebec City, Canada. 4 pages.
        	<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('icpr2002v');">Abstract</a></li>
        	<li><a href="../papers/icpr2002v.pdf">PDF</a></li>
        	<li><a href="../papers/icpr2002v.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('icpr2002v');">BibTeX</a></li>
        	<li><a href="../software/binarize/index.html">C++ Code (binarization only)</a></li>
      		</ul>
		<div class="t-abstract">
		The systems currently available for content based image and video retrieval work 
		without semantic knowledge, i.e. they use image processing methods to extract low 
		level features of the data. The similarity obtained by these ap-proaches does not 
		always correspond to the similarity a human user would expect. A way to include more 
		semantic knowledge into the indexing process is to use the text
		included in the images and video sequences. It is rich in information but easy to 
		use, e.g. by key word based queries. In this paper we present an algorithm to 
		localize artificial text in images and videos using a measure of accumulated 
		gradients and morphological post processing to detect the text. The quality of the 
		localized text is improved by robust multiple frame integration. A new technique for 
		the bina-rization of the text boxes is proposed. Finally, detection and OCR results 
		for a commercial OCR are presented.
		</div>
		<div class="t-bibtex"><pre>
@InProceedings{WolfICPR2002V,
  Author         = {C. Wolf and J.-M. Jolion and F. Chassaing},
  Title          = {Text {L}ocalization, {E}nhancement and {B}inarization in {M}ultimedia {D}ocuments},
  BookTitle      = {Proceedings of the {I}nternational {C}onference on {P}attern {R}ecognition},
  Volume         = {2},
  Pages          = {1037-1040},
  year           = 2002,
}		
		</pre></div>
  	</li>
	
	<li id="icpr2002m" class="bib-conf">	  
		<span class="t-author"> 
			<a href="../index.html">Christian Wolf</a> and 
			<a href="http://www.cfar.umd.edu/%7Edoermann/">David Doermann</a>	
	  	</span>
	  	<span class="t-title">
			Binarization of Low Quality Text using a Markov Random Field Model.
		</span>		
		<span class="t-medium">
			Proceedings of the International Conference on Pattern Recognition (ICPR),
		</span>
			volume 2, pages 160-163, IEEE Computer Society. August 11th-15th, 2002, 
        	Quebec City, Canada. 4 pages.
			<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('icpr2002m');">Abstract</a></li>
        	<li><a href="../papers/icpr2002m.pdf">PDF</a></li>
        	<li><a href="../papers/icpr2002m.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('icpr2002m');">BibTeX</a></li>
      		</ul>
		<div class="t-abstract">
		Binarization techniques have been developed in the document analysis community for 
		over 30 years and many algorithms have been used successfully. On the other hand, 
		document analysis tasks are more and more frequently being applied to multimedia 
		documents such as video sequences. Due to low resolution and lossy compression, the 
		binarization of text included in the frames is a non trivial task. Existing
		techniques work without a model of the spatial relationships in the image, which 
		makes them less powerful. We introduce a new technique based on a Markov Random 
		Field (MRF) model of the document. The model parameters (clique potentials) are 
		learned from training data and the binary image is estimated in a Bayesian 
		framework. The performance is evaluated using commercial OCR software.
		</div>
		<div class="t-bibtex">
		<pre>		
@InProceedings{WolfICPR2002M,
  Author         = {C. Wolf and D. Doermann},
  Title          = {Binarization of {L}ow {Q}uality {T}ext using a {M}arkov {R}andom {F}ield {M}odel},
  BookTitle      = {Proceedings of the {I}nternational {C}onference on {P}attern {R}ecognition},
  Volume         = {3},
  Pages          = {160-163},
  year           = 2002,
}
		</pre></div>
  	</li>	
		

	<li id="trec2002" class="bib-conf">	  
		<span class="t-author">
			<a href="../index.html">Christian Wolf</a>, 
			<a href="http://www.cfar.umd.edu/%7Edoermann/">David Doermann</a> and 
			<a
			href="http://www.ee.oulu.fi/EE/Info.Laboratory/personnel/Rautiainen.Mika.htm">
			Mika Rautiainen</a>.	
	  	</span>
	  	<span class="t-title">
			Video Indexing and Retrieval at UMD, 
		</span>		
		<span class="t-medium">
			Proceedings of the Text Retrieval Conference (TREC),
		</span>	
			November 19th-22th, 2002, Gaithersburg, USA. 10 pages.
			
		<ul class="reflink-box">
        	<li class="first">
			<a href="javascript:;" onClick="displayAbstractOf('trec2002');">
			Abstract</a>
		</li>
      		</ul>
		
		<div class="t-abstract">
		<p>Our team from the University of Maryland and INSA de Lyon
		participated in the feature extraction evaluation with overlay text
		features and in the search evaluation with a query retrieval and browsing system. For
		search we developed a weighted query mechanism by integrating 1) text
		(OCR and speech recognition) content using full text and n-grams
		through the MG system, 2) color correlogram indexing of image and
		video shots reported last year in TREC, and 3) ranked versions of the
		extracted binary features. A command line version of the interface
		allows users to formulate simple queries, store them and use weighted
		combinations of the simple queries to generate compound queries.
		</p><p>
		One novel component of our interactive approach is the ability for the users to
		formulate dynamic queries previously developed for database applications at 
		Maryland. 
		The interactive interface treats each video clip as
		visual object in a multi-dimensional space,  and each "feature" of that clip is
		mapped to one dimension. The user can visualize any two dimensions by placing
		any two features on the horizontal and vertical axis with additional dimensions 
		visualized by adding attributes to each object.</p>
		</div>
  	</li>
</ul>


<!-- ****************************************************************************************
  
  ******************************************************************************************* -->

<h2>2000</h2>

<ul>

	<li id="icpr2000" class="bib-conf">	  
		<span class="t-author">
			<a href="../index.html"> Christian Wolf </a>,
			<a href="http://rfv.insa-lyon.fr/%7Ejolion"> Jean-Michel Jolion </a>,
			<a href="http://www.prip.tuwien.ac.at/%7Ekrw"> Walter Kropatsch </a>, and
			<a href="http://www.prip.tuwien.ac.at/%7Ebis"> Horst Bischof </a>.
	  	</span>
	  	<span class="t-title">
			Content based Image Retrieval using Interest Points and Texture Features, 
		</span>		
		<span class="t-medium">
			Proceedings of the International Conference on Pattern Recognition (ICPR),
		</span>
			volume 4, pages 234-237. IEEE Computer Society. September 3rd, 
			2000, Barcelona, 
        	Spain. 4 pages.
      		<ul class="reflink-box">
        	<li><a href="javascript:;" onClick="displayAbstractOf('icpr2000');">Abstract</a></li>
        	<li><a href="../papers/icpr2000.ps.gz">Postscript</a></li>
        	<li><a href="../papers/icpr2000_poster.ppt">Powerpoint</a></li>
			<li><a href="javascript:;" onClick="displayBibtexOf('icpr2000');">BibTeX</a></li>
        	<li><a href="http://www.prip.tuwien.ac.at/Research/ImageDatabases/Query">Demo</a></li>
     		</ul>
		<div class="t-abstract">
		<p>Interest point detectors are used in computer vision to detect image points with 
		special properties, which can be geometric (corners) or non-geometric (contrast 
		etc.). Gabor functions and Gabor filters are regarded as excellent tools for 
		feature extraction and texture segmentation. This article presents methods how to 
		combine these methods for content based image retrieval and to generate a textural 
		description of images. Special emphasis is devoted to distance measure texture 
		descriptions. Experimental results of a query system are given.
		</p><p>
		This work was supported in part by the Austrian Science Foundation (FWF) under grant 
		S-7002-MAT.</p>
		</div>		
		<div class="t-bibtex"><pre>
@InProceedings{WolfICPR2000,
  Author         = {C. Wolf and J.M. Jolion and W. Kropatsch and H. Bischof},
  Title          = {Content {B}ased {I}mage {R}etrieval using {I}nterest {P}oints and {T}exture {F}eatures},
  BookTitle      = {Proceedings of the {I}nternational {C}onference on {P}attern {R}ecognition},
  Volume         = {4},
  Pages          = {234-237},
  year           = 2000,
}		
		</pre></div>
  	</li>			 
</ul>	


</body>
</html>
