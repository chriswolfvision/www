<!-- ****************************************************************************************
  Author: Christian Wolf
  ******************************************************************************************* -->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

<html>
<head>
	<title>Deep Learning and Differential programming: Exercise session</title>
	<link rel=stylesheet href="../../main.css" type="text/css">
	<link rel="shortcut icon" href="../../graphics/favicon.ico">
	<LINK REL=stylesheet HREF="../teaching.css" TYPE="text/css">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
</head>
<body>

<!-- ****************************************************************************************
  For google analytics
  **************************************************************************************** -->

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1423815-1";
urchinTracker();
</script>


<!-- ********************************************************************************************
  The nav menu box	
************************************************************************************************* -->

<div class="navmenu">
	<ul>
		<li class="first"><a href="../../index.html">Christian Wolf</a></li>
		<li><a href="../index.html">Teaching & Deep-Learning Lecture Slides</a></li>
	</ul>
</div>  

<!-- ********************************************************************************************
  Main contents
************************************************************************************************* -->

<h1>Deep Learning and Differential programming: Exercise session</h1>

<p>
The dataset used in this exercise is a <b>very tiny</b> version of the counterfactual learning dataset we used in the following paper (<b>This is not the real counterfactual learning dataset, which will be published later</b>!!):
</p>

<p>
<a href="https://fabienbaradel.github.io/">Fabien Baradel<a>,
<a href="https://nneverova.github.io/">Natalia Neverova<a>,
<a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
<a href="http://www.cs.sfu.ca/~mori">Greg Mori</a>,
<a href="../index.html">Christian Wolf</a>.
	  	<span class="t-title">
		    COPHY: Counterfactual Learning of Physical Dynamics.
		</span>
		<span class="t-medium">
			pre-print arXiv:1909.12000, 2019.
		</span>
<a href="https://arxiv.org/abs/1909.12000">[ArXiv]</a></li>
</p>

<p>
The dataset has been created by my student <a href="https://fabienbaradel.github.io">Fabien Baradel</a> during his PhD at INSA-Lyon.
</p>


<h2> Exercise 1: detect balls in images</h2>

<p>
We will take as input images of the following kind:
</p>
	
<img src="imgs/img_03919.jpg">
<img src="imgs/img_10000.jpg">
<img src="imgs/img_20999.jpg">


<p>
The objective is to detect the spheres of different colors in the image. There are 9 different colored spheres:
</p>

<pre>
COLORS = ['red', 'green', 'blue', 'yellow', 'lime', 'purple', 'orange', 'cyan', 'magenta']	
</pre>

<p>
Only 3 spheres will be in each individual image. We provide ground truth data for each image in the form of two matrices: 
<p>
<ul>
	<li> A 9x1 matrice (a 9D vector) which binary indicates the presence of the ball of a given color. 3 balls are present in each image.</li>
	<li> A 9x4 matrice with ball positions:
		<ul>
			<li> Rows correspond to the balls of different colors. Only 3 balls are in each image, so only 3 rows in this matrix are non-zero.</li>
			<li> Columns correspond to: x1,y1, x2,y2.</li>
		</ul>
	</li>
</ul>
<p>

<p><b>Task 1:</b> Write a neural model which detects the presence of each of the 9 possible balls in each image.
</p>


<p><b>Task 2:</b> Augment the neural model such that it also detects the bounding box coordinates of the 3 balls which are present.
</p>

<p>
The set consists of 21000 images and ground truth matrices <a href="data/train.tgz">[TGZ]</a>.<br>
Don't forget to split it into a training and a validation set.
</p>

</ul>

The following python code is also provided:
<ul>
	<li>A data loader (a subclass of torch.data.Dataset class), which provides for each sample a 3 tuple of (image tenor (3,100,100), presence vector (9), bounding box matrix (9,4)).</li>
	<li>Visualization code, which allows to paint a bounding box on an image.</li>
</ul>

Zip archive of both files: <a href="src/tp_src.zip">[ZIP].</a></li>

<h2>Exercise 2 (Advanced!): future forecasting</h2> 

<p><b>Objective:</b>
Write a neural model, which takes two sequences of ball bounding box coordinates. Take the initial sequence of coordinates and predict the positions of the last instant.</p>

<p>This is actually an illposed problem, since the data has been created with several different physical properties, namely ball masses, friction coefficients, restitution coefficients. These coefficients are not observable from a single time instant (but they can be partially inferred by the network taking into account time). Therefore, do not expect the error to be close to zero.
</p>

<p>
The set consists of 7000 sequences, each composed of 20 time steps <a href="data/train_seq.tgz">[TGZ]</a>.<br>
Don't forget to split it into a training and a validation set.
</p>

<p>
Again, as before, only 3 spheres will be in each individual image. We provide ground truth data for each sequence in the follwing format:
<p>
<ul>
	<li> A 9x1 matrice (a 9D vector) which binary indicates the presence of the ball of a given color. 3 balls are present in each image.</li>
	<li> A 20x9x4 matrice with ball positions:
		<ul>
			<li> Dimension 0 is time, from 0 to 19.</li>
			<li> The other two dimensions are like in the first dataset above:
				<ul>
					<li> Dimension 1 corresponds to the balls of different colors. Only 3 balls are in each image, so only 3 rows in this matrix are non-zero.</li>
					<li> Dimension 2 corresponds to: x1,y1, x2,y2.</li>
				</ul>
			</li>	
		</ul>
	</li>
</ul>
<p>

Zip archive of the data loader for this sequences dataset: <a href="src/tp_seq_src.zip">[ZIP].</a></li>


</body>
</html>
