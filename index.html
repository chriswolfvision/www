 <!-- ****************************************************************************************
  Author: Christian Wolf
  ******************************************************************************************* -->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

<html>
<head>
	<link rel=stylesheet href="main.css?v=1" type="text/css">
	<link rel=stylesheet href="portrait.css" type="text/css">
	<link rel=stylesheet href="lightfont.css" type="text/css">
	<link rel="shortcut icon" href="graphics/favicon.ico">
	<script type="text/javascript" language="JavaScript" src="ballonsv4.js"></script>
	<script type="text/javascript" language="JavaScript" src="imagearray.js"></script>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  	<title>Christian Wolf - LIRIS UMR 5205 - INSA de Lyon</title>
</head>

<body>

<!-- ****************************************************************************************
  For google analytics
  **************************************************************************************** -->

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1423815-1";
urchinTracker();
</script>


<!-- ****************************************************************************************
  The hidden help field
  **************************************************************************************** -->

<!-- The box which contains the ballon help - hidden at the beginning -->
<div style="left: 50px; top: 0ex; position: absolute; border: 4px solid #44a3c0; background-color: #ffffff; padding: 1ex; visibility: hidden;" class="bulle-aide" id="bulle_aide">XXXXX</div>

<!-- ****************************************************************************************
  The nav menu box
  **************************************************************************************** -->

<a href="contribution/yosemite.html">
<img style="max-width: 100%; height: auto; width: auto\9;" src="miscphotos/chris_banner_usa2014_v2.jpg">
</a>

<!-- ****************************************************************************************
  The personal data
  **************************************************************************************** -->

<div class="block">

  	<!-- The Photo -->
	<div class="floater">
		<table>
			<!--
        	<tr><td style="padding-top: 1em;">
                <span class="aide" title="">
                    <img src="miscphotos/chris_milwaukee.png">
                </span>
            </td></tr>
            -->
        	<tr><td style="padding-top: 1em;">
                <span class="aide" title="An in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments<br>arXiv:2111.14666, 2021<br>Work by A. Sadek, G. Bono, B. Chidlovskii, C. Wolf<br><br><br><img width=100% src=graphics/icra2022assem_big.jpg>">
                    <img src="graphics/icra2022assem.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>Winning entry of the CVPR 2021 Multi-Object Navigation Challenge</b><br>Teaching Agents how to map: Spatial Reasoning for Multi-Object Navigation<br>arXiv:2107.06011<br>Work by P. Marza, L. Matignon, O. Simonin, C. Wolf<br><br><br><img width=100% src=graphics/multion_big.jpg>">
                    <img src="graphics/animated_multion.gif">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>NeurIPS 2021</b><br>Supervising the Transfer of Reasoning Patterns in VQA<br>Work by C. Kervadec, C. Wolf, G. Antipov, M. Baccouche, M. Nadri<br><br><br><img width=100% src=graphics/neurips2021_big.jpg>">
                    <img src="graphics/neurips2021.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>CDC 2021</b><br>Deep KKL: Data-driven Output Prediction for Non-Linear Systems<br>Work by C. Janny, V. Andrieu, M. Nadri, C. Wolf<br><br><br><img width=100% src=graphics/cdc2021_big.jpg>">
                    <img src="graphics/cdc2021.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>IEEE Transactions on Visualization and Computer Graphics (Proceedings of VIS 2021)</b></br>VisQA: X-raying Vision and Language Reasoning in Transformers<br>Work by T. Jaunet, C. Kervadec, G. Antipov, M. Baccouche, R. Vuillemot, C. Wolf<br><br><img width=100% src=graphics/vis2021_big.jpg>">
                    <img src="graphics/animated_vis2021.gif">
                </span>
            </td></tr>
        	<tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>CVPR 2021</b></br>How transferrable are reasoning patterns in VQA?<br>Work by C. Kervadec, T. Jaunet, G. Antipov, M. Baccouche, R. Vuillemot, C. Wolf<br><br><img width=100% src=graphics/cvpr2021reasoning_big.jpg>">
                    <img src="graphics/cvpr2021reasoning.png">
                </span>
            </td></tr>
        	<tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>CVPR 2021</b></br>Roses Are Red, Violets Are Blue... but Should VQA Expect Them To?<br>Work by C. Kervadec, G. Antipov, M. Baccouche, C. Wolf<br>https://arxiv.org/abs/2006.05121<br><br><img width=100% src=graphics/cvpr2021roses_big.jpg>">
                    <img src="graphics/cvpr2021roses.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>CVPR 2021</b></br>SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation<br>Work by B. Duke, A. Ahmed, C. Wolf, P. Aarabi and G.W. Taylor.<br>https://arxiv.org/abs/2101.08833<br><br><img width=100% src=graphics/cvpr2021sstvos_big.jpg>">
                    <img src="graphics/cvpr2021sstvos.png">
                </span>
            </td></tr>
			<tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>ECCV 2020</b><br>Learning to plan with uncertain topological maps<br>Work by E. Beeching, J. Dibangoye, O. Simonin, C. Wolf<br><br><br><img width=100% src=graphics/topomap_big.jpg>">
                    <img src="graphics/animated_eccv2020.gif">
                </span>
            </td></tr>
			<tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>ECML-PKDD 2020</b><br>EgoMap: Projective mapping and structured egocentric memory for Deep RL<br>Work of E. Beeching, J. Dibangoye, O. Simonin, C. Wolf<br>https://arxiv.org/abs/2002.02286<br><br><img width=100% src=graphics/egomap_big.jpg>">
                    <img class="coucou" src="graphics/egomap_small.png">
                </span>
            </td></tr>            
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="Blog post:<br>What is translation equivariance, and why do we use convolutions to get it?<br><br><img width=100% src=graphics/animated_equivariancebig.gif>">
                    <img src="graphics/animated_equivariance200.gif">
                </span>
            </td></tr>            
			<tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>ECAI 2020</b><br>Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks<br>Work of C. Kervadec, G. Antipov, M. Baccouche, C. Wolf<br>https://arxiv.org/abs/1912.03063<br><br><img width=100% src=graphics/ecai2020_big.jpg>">
                    <img src="graphics/ecai2020_small.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>ICLR 2020</b><br>COPHY: Counterfactual Learning of Physical Dynamics<br>Work by F. Baradel, N. Neverova, J. Mille, G. Mori, C. Wolf<br>https://arxiv.org/abs/1909.12000<br><br><img width=100% src=graphics/cophy_big.jpg>">
                    <img src="graphics/animated_cophy.gif">
                </span>
            </td></tr>
            <!--
			<tr><td style="padding-top: 1em;">
                <span class="aide" title="Deep Reinforcement Learning on a Budget: 3D Control and Reasoning Without a Supercomputer<br>Work of E. Beeching, C. Wolf, J. Dibangoye, O. Simonin<br>Under review, https://arxiv.org/abs/1904.01806">
                    <img src="graphics/teaser_edward_bench2019.png">
                </span>
            </td></tr>
        	-->
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>ECML-PKDD 2019</b><br>Learning 3D Navigation Protocols on Touch Interfaces with Cooperative Multi-Agent Reinforcement Learning<br>Work of Q. Debard, J. Dibangoye, S. Canu, C. Wolf<br>https://arxiv.org/abs/1904.07802<br><br><img width=100% src=graphics/marlvae_big.jpg>">
                    <img src="graphics/teaser_marl_quentin.png">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>ECCV 2018</b><br>Object Level Visual Reasoning in Videos<br>Work of F. Baradel, N. Neverova, C. Wolf, J. Mille, G. Mori<br>https://arxiv.org/abs/1806.06157<br><br><img width=100% src=graphics/objectlevel_big.jpg>">
                    <img src="graphics/objectlevel2.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>CVPR 2018</b><br>Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points<br>F. Baradel, C. Wolf, J. Mille, G.W. Taylor<br>https://arxiv.org/abs/1802.07898<br><br><img width=100% src=graphics/glimpse_big.jpg>">
                    <img src="graphics/animated_cvpr2018.gif">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>Tectonics</b><br>An anticipation experiment for plate tectonics (predicting tectonic plates with GANs)<br>T. Gillooly, N. Coltice, C. Wolf.<br><br><img width=100% src=graphics/geogans_big.jpg>">
                    <img src="graphics/geogans200.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>IROS 2018</b><br>Semantic Grid Estimation with a Hybrid Bayesian and Deep Neural Network Approach<br>O. Erkent,  C. Wolf, C. Laugier, D.S. Goncalez and V.R. Cano<br><br><img width=100% src=graphics/iros2018_big.jpg>"">
                    <img src="graphics/iros2018.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>BMVC 2018</b><br>Human Activity Recognition by attending to RGB frames from deep pose features<br>F. Baradel, C. Wolf, J. Mille<br>https://arxiv.org/abs/1703.10106<br><br><img width=100% src=graphics/bmvc2018_big.jpg>">
                    <img src="graphics/animated_handattention.gif">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>FG 2018</b><br>Learning to recognize touch gestures: recurrent vs. convolutional features and dynamic sampling<br>https://arxiv.org/abs/1802.09901<br><br><b>Itekube-7</b>: a new dataset for multi-touch gesture recognition<br>http://itekube7.itekube.com<br><br><img width=100% src=graphics/itekube7_big.jpg>">                	
                    <img src="graphics/itekube7.jpg">
                </span>
            </td></tr>
            <tr><td style="padding-top: 1em;">
                <span class="aide" title="<b>BMVC 2017</b><br>Residual Conv-Deconv Grid Network for Semantic Segmentation<br>D. Fourure, R. Emonet, E. Fromont, D. Muselet, A. Trémeau, C. Wolf<br><br><img width=100% src=graphics/bmvc2017_big.jpg>">
                        <img src="graphics/animated_cityscapesdamien.gif">
                </span>
            </td></tr>
			<tr><td style="padding-top: 1em;">
				<span class="aide" title="<b>CVIU 2017</b><br>Hand Pose Estimation through Weakly-Supervised Learning of a Rich Intermediate Representation<br>N. Neverova, C. Wolf, F. Nebout, G.W. Taylor<br>arxiv.org:1511.06728<br><br><img width=100% src=graphics/cviu2017_big.jpg>">
						<img src="graphics/animated_handposejoints.gif">
				</span>
			</td></tr>

			<tr><td style="padding-top: 1em;">
				<span class="aide" title="<b>IJDAR 2018</b><br>Learning to detect, localize and recognize many text objects in document images from few examples<br>B. Moysset, C. Kermorvant, C. Wolf<br>arxiv:1611.05664<br><br><img width=100% src=graphics/sdl_big.jpg>">
						<img src="graphics/fully-convolutional-2d_lstms.png">
				</span>
			</td></tr>

			<tr><td style="padding-top: 1em;">
				<span class="aide" title="<b>IEEE Access 2016</b><br>Learning human identity from motion pattern<br>N. Neverova, C. Wolf, G. Lacey, L. Fridman, D. Chandra, B. Barbello and G.W. Taylor<br>arxiv:1511.03908)<br><br><img width=100% src=graphics/access2016_big.jpg>">
						<img src="graphics/teaser_atap2016.jpg">
				</span>
			</td></tr>
			<tr><td>
				<span class="aide" title="<b>IEEE PAMI 2016</b><br>Moddrop: Multimodal Gesture Recognition<br>Ranked 1st at the ChaLearn 2014 Competition<br>N. Neverova, C. Wolf, G.W.Taylor, F. Nebout<br><br><img width=100% src=graphics/pami2016_big.jpg>">
					<img src="graphics/animated_gestes1.gif">
				</span>
			</td></tr>
			<!--
      		<tr><td>
				<span class="aide" title="<b>JMIV 2015</b><br>Fast Exact Hyper-Graph Matching with Dynamic Programming for Spatio-Temporal Data<br>O. Celiktutan, C. Wolf, B. Sankur and E. Lombardi<br><br><img width=100% src=graphics/jmiv2015_big.png>">
					<img src="research/teaser_stgraphmatching.png">
				</span>
			</td></tr>
			<tr><td>
				<span class="aide" title="The LIRIS human activities dataset (created for the ICPR 2012 HARL competition, CVIU 2014)">
					<img src="graphics/animated_harl.gif">
				</span>
			</td></tr>			
			<tr>
				<TD><a href="research/index.html">More...</a></TD>
			</tr>
			-->	
		</table>
	</div>

	<div class="portrait-right">

		<table class="portrait-table">
		<tr>
		<td>
            <div class="navmenunew" style="text-align:  left; max-width: 100%;">
	<ul>		
		<li class="first"> <a href="teaching/index.html">Teaching</a></li>		
		<li> <a href="publications/index_bydate.html">Publications</a></li>
		<li> <a href="publications/pres.html">Talks</a></li>		
	</ul>
</div>

            <!--
            <a href="https://www.inria.fr/">
                <img height=25 alt="" src="graphics/logo_inria.jpg">
            </a>
        -->
            <p>
				<span style="color: black; font-weight:bold; font-size: larger;">Christian WOLF</span>
				is associate professor (Maître de Conférences, HDR)
				at <a href="http://www.insa-lyon.fr">INSA de Lyon</a> and <a href="http://liris.cnrs.fr">LIRIS</a>, a CNRS laboratory, since sept. 2005.
        He is interested in machine learning and computer vision, especially large-scale learning of the capacity to perform high-level reasoning from visual observations, and more recently the connections between machine learning and control.
       	</p>
       	<p>
        He is the head of the <a href="https://chriswolfvision.github.io/remember">AI chair / chair in Artificial Intelligence</a> at INSA-Lyon, and the national coordinator of project <a href="https://projet.liris.cnrs.fr/delicio/">ANR Delicio</a> "Data and Prior, Machine Learning and Control" (2019-2023).        
        He is a member of the directing committee of <a href="http://www.gdr-isis.fr/">GDR ISIS</a> and co-leader of it's topic "<em>Machine Learning</em>"; co-leader of the topic "<em>Machine Learning and Robotics</em>" at <a href="https://www.gdr-robotique.org/">GDR Robotique</a>; member of the scientific committee of <a href="https://www.gdria.fr/">GDR IA</a>; member of the board of AI experts at the French national supercomputing cluster <a href="https://www.genci.fr">GENCI</a>; member of evaluation <a href="https://anr.fr">ANR</a> committee "<em>Artificial Intelligence</em>" from 2019-2021.
        He has supervised <a href="#phdstudents">13 defended PhD theses</a>, is an associate editor of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE-T on PAMI</a> and area chair of <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>, <a href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>, <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a>, <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>, <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="https://nips.cc/Conferences/2021">NeurIPS 2021</a>, <a href="https://icml.cc/Conferences/2022">ICML 2022</a>.
		    </p>

		    <p>
		    He received his MSc in computer science from
				<a href="http://www.tuwien.ac.at">TU Vienna</a>, Austria,
				in 2000, and a PhD in computer science from
				<a href="http://www.insa-lyon.fr">INSA de Lyon</a>,
				France, in 2003. In 2012 he obtained the habilitation diploma, also from
				<a href="http://www.insa-lyon.fr">INSA de Lyon</a>. Between September 2017 and August 2019 he was on leave at <a href="https://www.inria.fr/">INRIA</a>, at the <a href="https://team.inria.fr/chroma/en">chroma work group</a>
                at the <a href="http://www.citi-lab.fr/">CITI laboratory</a>.
            </p>

			<ul class="logo-bar">
			<li><a href="http://liris.cnrs.fr/">
			    <img height=25 alt="" src="graphics/logo_liris.jpg"></a>
			</li>
			<li><a href="http://www.cnrs.fr/">
			    <img height=25 alt="" src="graphics/logo_cnrs.jpg"></a>
			</li>
			<li><a href="http://www.insa-lyon.fr">
			    <img height=25 alt="" src="graphics/logo_insa.png"></a>
			</li>
			</ul>
		</td>
		</tr>
		</table>

		Twitter: <a href="https://twitter.com/chriswolfvision">@chriswolfvision</a><br>
		Blog/Medium: <a href="https://medium.com/@chriswolfvision">@chriswolfvision</a>

		<br>
		<br>
		
		<!--
		<div class="hiringbox">
		<h4>We are hiring! Funded positions:</h4>

		
    	<ul class="sparselist" style="margin-bottom: 0.5ex">
			<li> <a href="openpositions/phd-remember/index.html">Funded PhD position: (Deep Learning and Robotics)</a>.
			</li>
		</ul>

		</div>
	-->
	


		<h4>Research interests:</h4>
    <!-- Computer Vision, Machine Learning, Deep Learning, Artificial Intelligence</h4> -->
    <ul>
      <li>Machine Learning, Deep Learning, AI: trying to make machines learn to reason</li>
      <li>Computer Vision and understanding humans</li>
      <li>Robotics and learning to control</li>
    </ul>

    <!-- <center> -->
    	<div style="width: 70%">
    	<center>
    	<img width="100%" src="miscphotos/wolfpack_oct2021.jpg">
    	<br>
    	<i>The group in October 2021 at our "ML + Control workshop": <a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny</a>, <a href="https://edbeeching.github.io/">Edward Beeching</a>, <a href="https://theo-jaunet.github.io/">Théo Jaunet</a>, <a href="https://fr.linkedin.com/in/quentin-possama%C3%AF-425420142">Quentin Possamaï</a>, <a href="https://pierremarza.github.io/">Pierre Marza</a>, <a href="https://europe.naverlabs.com/people_user/assem-sadek">Assem Sadek</a>, <a href="https://fr.linkedin.com/in/olivier-serris-4575631b9">Olivier Serris</a>, <a href="">Guillaume Bono</a>, Aurélien Bénéteau, <a href=""></a><a href="https://corentinkervadec.github.io/">Corentin Kervadec</a> (not shown). <a href="miscphotos/wolfpack_feb2020.jpg">[The group in feb. 2020]</a>.</i>
    	</center>
    	</div>
    
    <!-- </center> -->
    <!--
		<ul class="sparselist" style="margin-bottom: 0.5ex">
			<li>Gesture recognition and pose estimation (<a href="research/gesturerec.html">read more...</a>)</li>
			<li>Activity recognition (<a href="research/activityrec.html">read more...</a>)</li>
			<li>Deep learning for computer vision (<a href="research/deeplearning.html">read more...</a>)</li>
			<li>Document image analysis (<a href="research/documents.html">read more...</a>)</li>
			<li>Image and video segmentation, scene labelling (<a href="research/segmentation.html">read more...</a>)</li>
			<li>Computer vision for robotics (<a href="research/robotvision.html">read more ...</a>)</li>
		</ul>
  -->

		<h4>Recent news ( <a href="publications/index.html">All publications</a> | <a href="http://scholar.google.com/citations?user=idYS1AIAAAAJ&hl=en">Google citations page</a> )</h4>
		<ul class="sparselist" style="margin-bottom: 0.5ex">
			<li>08.12.2021: Paper accepted at the AAAI 2022 Workshop on RL for games: "Godot Reinforcement Learning Agents", work by E. Beeching et al., <a href="https://arxiv.org/abs/2112.03636">[arxiv]</a>.</li>
			<li>30.11.2021: New paper: "An in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments", work by A. Sadek et al. <a href="https://arxiv.org/abs/2111.14666">[arxiv]</a>.</li>
			<li> 25.11.2020: I am <b>outstanding BMVC 2021</b> <a href="https://twitter.com/BMVCconf/status/1463481756586283013?s=20">reviewer</a>.</li>
			<li>28.09.2021: NeurIPS 2021 paper accepted: "Supervising the Transfer of Reasoning Patterns in VQA" <a href="https://arxiv.org/abs/2106.05597">[arxiv]</a>.</li>
			<li>27.07.2021: CDC 2021 paper accepted: "Deep KKL: Data-driven Output Prediction for Non-Linear Systems", work by <a href="https://scholar.google.com/citations?user=IC0ceIgAAAAJ&hl=en">Steeven Janny</a>, <a href="https://arxiv.org/abs/2103.12443">[arxiv]</a>.</li>			
			<li>16.07.2021: VIS 2021 paper accepted (journal published in IEEE-T on Visualization and Computer Graphics): "VisQA: X-raying Vision and Language Reasoning in Transformers"; work by <a href="https://theo-jaunet.github.io/">Théo Jaunet</a>, <a href="https://arxiv.org/abs/2104.00926">[arxiv]</a>.</li>
			<li>30.06.2021: New paper: "Universal Domain Adaptation in Ordinal Regression" <a href="https://arxiv.org/abs/2106.11576">[arxiv]</a>.</li>
			<li>20.06.2021: My student <a href="https://scholar.google.com/citations?user=NAI5mi4AAAAJ&hl=fr">Pierre Marza</a> won the <a href="http://multion-challenge.cs.sfu.ca/">Multi Object Navigation Challenge</a> at CVPR 2021! <a href="https://arxiv.org/abs/2107.06011">[arxiv]</a>.</li>		
			<li> 19.05.2021: I am <b>outstanding CVPR 2021</b> <a href="http://cvpr2021.thecvf.com/node/184">reviewer</a>.</li>			
			<li>07.04.2021: <a href="https://fabienbaradel.github.io/">Fabien Baradel</a>, former PhD student at our group, received the runner-up (2nd place) thesis prize  by <a href="http://afrif.irisa.fr">AFRIF</a> (French association of pattern recognition). Congrats! <a href="http://www.afrif.asso.fr/?p=737">[Prize-page]</a>.</li>			
			<li> 28.01.2021: 3 papers accepted at CVPR 2021:
				<ul class="square">
					<li>"How Transferrable are Reasoning Patterns in VQA?" <a href="https://openreview.net/pdf?id=hARFKEECBD_">[open-review]</a>.</li>
					<li>"Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?" <a href="https://arxiv.org/abs/2006.05121">[arxiv]</a>.</li>
					<li>"SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation" <a href="https://arxiv.org/abs/2101.08833">[arxiv]</a>.</li>
				</ul>
			</li>
			<li> 24.09.2020: Paper accepted at <a href="https://visxai.io/">Vis 2020 workshop on Visualization for AI Explainability</a> (work by T. Jaunet): <a href="">[Visualization submission link]</a>
			<li> 28.08.2020: I am <b>outstanding BMVC 2020</b> <a href="https://bmvc2020.github.io/people/reviewers/">reviewer</a>.</li>
			<li>16.07.2020: CDC 2020 paper accepted: "Data-driven multi-model control for a waste heat recovery system" <a href="https://ieeexplore.ieee.org/document/9304418">[IEEE]</a>.</li>
			<li>02.07.2020: ECCV 2020 paper accepted: "Learning to plan with uncertain topological maps" <a href="https://arxiv.org/abs/2007.05270">[arxiv]</a>.</li>			
			<li>11.06.2020: New paper: "Estimating semantic structure for the VQA answer space" <a href="https://arxiv.org/abs/2006.05726">[arxiv]</a>.</li>
			<li>05.06.2020: ECML-PKDD paper accepted: "EgoMap: Projective mapping and structured egocentric memory for Deep RL" <a href="https://arxiv.org/abs/2002.02286">[arxiv]</a>.</li>
			<li>11.02.2020: Computer Graphics Forum / Eurovis paper accepted: "DRLViz: Understanding Decisions and Memory in Deep Reinforcement Learning" <a href="https://arxiv.org/abs/1909.02982">[arxiv]</a>.</li>		
			<li>14.01.2020: ECAI 2020 paper accepted: "Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks" <a href="https://arxiv.org/abs/1912.03063">[arxiv]</a>.</li>
			<li>20.12.2019: ICLR 2020 paper accepted: "CoPhy: Counterfactual Learning of Physical Dynamics"  <a href="https://arxiv.org/abs/1909.12000">[arxiv]</a>, <a href="https://projet.liris.cnrs.fr/cophy/">[Project + Benchmark data]</a>.</li>
			<li>12.12.2019: My <b>AI Chair position</b> has been accepted, titled: <a href="https://chriswolfvision.github.io/remember/">"REMEMBER - Learning Reasoning, Memory and Behavior"</a>, which will provide funding for the next years of our research! The chair is co-financed by ANR, Naver Labs Europe and INSA-Lyon. Team members are  <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>, <a href="http://perso.citi-lab.fr/jdibangoy/">Jilles Dibangoye</a>, <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetitia Matignon</a> and <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii</a>.</li>
			<li>09.12.2019: Journal paper accepted: "Machine Learning-Based Classification of the Health State of Mice Colon in Cancer Study from Confocal Laser Endomicroscopy", Nature Scientifc Reports (Rasti et al).</li>
			<li>26.11.2019: I joined <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE-Transactions on PAMI</a> as an <b>Associate Editor</b>.</li>
			<li>19.11.2019: Journal paper accepted: "An anticipation experiment for plate tectonics", to appear in Tectonics (Gillooly et al).</li>
		
			<li>12.07.2019: <b>ANR grant</b> "<a href="https://projet.liris.cnrs.fr/delicio/">Delicio</a>" accepted, addressing stable and robust control in complex environments combining machine learning and control theory. Partners:
				<ul style="list-style-type: square;">
                    <li> LIRIS/INSA-Lyon (<a href="index.html">Christian Wolf</a> (Project Leader), <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetitia Matignon</a>) </li>
                    <li> CITI/INSA-Lyon (<a href="http://perso.citi-lab.fr/jdibangoy/#/">Jilles Dibangoye</a>, <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>, <a href="https://ievred.github.io/">Ievgen Redko</a> (LHC Laboratory)) </li>
                    <li> LAGEPP/Lyon 1 University (<a href="https://lagepp.univ-lyon1.fr/membre/nadri-madiha/">Madiha Nadri</a>, <a href="https://sites.google.com/site/vincentandrieu/">Vincent Andrieu</a>, <a href="https://sites.google.com/site/astolfidaniele/">Daniele Astolfi</a> <a href="https://www.ec-lyon.fr/contacts/laurent-bako">Laurent Bako</a>(AMPERE Laboratory), <a href="https://www.ec-lyon.fr/contacts/giacomo-casadei">Giacomo Casadei</a>(AMPERE Laboratory)) </li>
                    <li> Onera (<a href="https://www.onera.fr/en/staff/sylvain-bertrand">Sylvain Bertrand</a>, <a href="http://julien.marzat.free.fr/">Julien Marzat</a>, <a href="https://scholar.google.com/citations?user=xg8ezOQAAAAJ&hl=en">Hélène Piet-Lahanier</a>) </li>
                </ul>
			</li>
			<li>09.06.2019: ECML-PKDD 2019 paper accepted: "Learning 3D Navigation Protocols on Touch Interfaces with Cooperative Multi-Agent Reinforcement Learning" <a href="https://arxiv.org/abs/1904.07802">[arxiv]</a>.</li>
			<li> 29.05.2019: I am <b>outstanding CVPR 2019</b> <a href="http://cvpr2019.thecvf.com/files/CVPR_2019_Program_Guide.pdf">reviewer</a>.</li>			
      		<li>10.09.2018: We released <a href="https://fabienbaradel.github.io/masks_data">complementary mask prediction</a> on VLOG and EPIC KITCHEN datasets (saves ~2 months of calculation on a single GPU).
			<li>03.07.2018: ECCV 2018 paper accepted: "Object Level Visual Reasoning in Videos" <a href="https://arxiv.org/abs/1806.06157">[arxiv]</a>.</li>
			<li>02.07.2018: BMVC 2018 paper accepted: "Human Activity Recognition by attending to RGB frames from deep pose features" <a href="https://arxiv.org/abs/1703.10106">[arxiv]</a>.</li>
			<li>29.06.2018: IROS 2018 paper accepted: "Semantic Grid Estimation with a Hybrid Bayesian and Deep Neural Network Approach" (PDF will come soon).</li>
            <li>19.02.2018: CVPR 2018 paper accepted: "Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points" <a href="https://arxiv.org/abs/1802.07898">[arxiv]</a>.</li>
            <li>29.01.2018: FG 2018 paper accepted: "Learning to recognize touch gestures: recurrent vs. convolutional features and dynamic sampling" (arxiv coming) <a href="../papers/fg2018.pdf">[PDF]</a><a href="https://arxiv.org/abs/1802.09901">[arxiv]<a>.</li>
            <li>15.10.2017: CVIU paper accepted: Deep hand pose estimation with semi/weakly-supervised learning; <a href="../papers/cviu2017.pdf">[pdf]</a>,
			<a href="http://www.sciencedirect.com/science/article/pii/S1077314217301686">[sciencedirect</a>],
			<a href="http://arxiv.org/abs/1511.06728">[arxiv]</a>,
			<a href="https://www.youtube.com/watch?v=7GMiExWKM8c">[video]</a>.
			</li>
			<li> 31.08.2017: SIGGRAPH Asia 2017 (ToG) paper accepted: <a href="publications/index_bydate.html#siggraphasia2017">
            Interactive Example-Based Terrain Authoring with Conditional Generative Adversarial Networks</a>.
            <li>
                16.05.2017: My former PhD student <a href="https://nneverova.github.io/">Natalia Neverova</a>
                won the 2017 French PhD thesis prize of Club EEA/GDR ISIS for work on Deep Learning of Human Motion. Congratulations!
            </li>
            <li> 01.07.2017: BMVC 2017 paper accepted: <a href="publications/index_bydate.html#bmvc2017">Residual Conv-Deconv Grid Network for Semantic Segmentation</a>.            
			<li> 17.11.2016: New paper out: <a href="https://arxiv.org/abs/1611.05664">arxiv:1611.05664</a> on object localization with fully convolutional networks and context from spatial 2D-LSTMs.</li>
			<li> 01.05.2016: I am <b>outstanding CVPR 2016</b> <a href="http://cvpr2016.thecvf.com/">reviewer</a>.</li>
			<li> 05.04.2016: IEEE Access paper accepted: learning human identity from motion patterns: <a href="http://arxiv.org/abs/1511.03908">ArXiv pre-print 1511.03908</a>.</li>
			<li> 12.02.2016: French-Canadian <a href="https://projet.liris.cnrs.fr/deepvis/">ANR/NSERC project "Deepvision"</a> has been accepted. It involves 4 partners: LIRIS/INSA-Lyon, LIP6/UPMC, University of Guelph, Simon Fraser University.</li>
			<li> 04.02.2016: <a href="papers/prl2016.pdf">PRL 2016 paper accepted: </a> social behavior modeling in face-to face interaction with dynamic Bayesian networks.
			</li>
			<li> 20.07.2015: <a href="papers/pami2015.pdf">IEEE-T-PAMI 2016 paper</a> accepted: multi-modal gesture recognition and a modality dropout learning algorithm for deep models [<a href="https://www.youtube.com/watch?v=yuT8i8eiUXU">Video/youtube</a>].
			</li>
			<li>
				16.06.2015: Winners of the CVPR 2015 - OpenCV Vision Challenge (<a href="http://code.opencv.org/projects/opencv/wiki/VisionChallenge">First in category "gesture recognition"</a>)
			</li>
			<li>
				09.06.2014: Winners of the ECCV 2014 - Chalearn competition: work of <a href="https://nneverova.github.io/">Natalia Neverova</a> on gesture recognition (<a href="http://gesture.chalearn.org/challenge-results">Results</a>; More in the <a href="papers/pami2015.pdf">PAMI paper</a>).</td></tr>
			</li>			
		</ul>

		<h4>In the press:</h4>
		<ul class="sparselist">
			<li>23.09.2020: in "Libération" on Twitters biased image cropping algorithm <a href="https://www.liberation.fr/checknews/2020/09/22/l-algorithme-de-twitter-choisit-il-toujours-une-personne-blanche-dans-ses-apercus-d-image_1800209">[Web]</a>
			<li>01.11.2019: In "Usine nouvelle" on Deepmind's Starcraft algorithm <a href="https://www.usinenouvelle.com/editorial/comment-deepmind-est-devenu-l-un-des-meilleurs-joueurs-de-starcraft-ii.N899894">[Web]</a>
			<li>19.06.2018: Student Fabien Baradel is interviewed for the "CVPR Daily" online journal <a href="https://www.rsipvision.com/CVPR2018-Tuesday/14">[Web]</a><a href="inthepress/cvpr2018daily.pdf">[PDF]</a>.</li>
			<li>05.04.2016: In the CNRS journal on automatic authentification of smartphone users with Deep Learning
				<a href="inthepress/press-cnrsjournal-160901.pdf">[PDF]</a>
				<a href="https://lejournal.cnrs.fr/articles/bouger-pour-sidentifier">[web-version]</a>
				<a href="https://news.cnrs.fr/articles/biometrics-identification-in-action">[english-version]</a>
			</li>
			<li>11.03.2016: In "Industrie & Technologies" on AI and Deep Learning <a href="http://www.industrie-techno.com/les-retombees-industrielles-de-l-intelligence-artificielle.43218">[direct link]</a></li>
			<li>25.07.2015: In "Le monde" on Deep Learning <a href="http://www.lemonde.fr/pixels/article/2015/07/24/comment-le-deep-learning-revolutionne-l-intelligence-artificielle_4695929_4408996.html">[direct link]</a><a href="papers/press-lemonde-2015.pdf">[saved-pdf]</a></li>
			<li>28.04.2011: In "Usine Nouvelle" on Kinect <a href="inthepress/usinenouvelle20110428.pdf">[pdf]</a></li>
			<li>28.01.2011: In "Millenaire 3" on robotics <a href="papers/interview_millenaire3_2011.pdf">[pdf]</a></li>
			<li>10.03.2010: In "Le monde" on video surveillance <a href="inthepress/lemonde20120310.pdf">[pdf]</a></li>
		</ul>

    <h3>Current students</h3>
    <a id="phdstudents"></a>
    <div style="margin-left: 1em;">

		<h4>PhD Students</h4>
		<ul class="sparselist">
			<li><a href="https://scholar.google.com/citations?user=NAI5mi4AAAAJ&hl=fr">Pierre Marza</a> (11/2020-);  (co-supervised with <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetitia Matignon</a> (LIRIS); <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>  ( <em>INRIA/CITI/INSA-Lyon</em>)).</li>
			<li><a href="https://www.researchgate.net/profile/Steeven_Janny">Steeven Janny</a> (09/2020-);  Deep Learning and physical models (co-supervised with <a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri</a> (<em>LAGEPP/Univ Lyon 1</em>)).</li>
			<li><a href="https://europe.naverlabs.com/people_user/assem-sadek/">Assem Sadek</a> (06/2020-); Situation Awareness with Geometry and Self-Supervised Learning (co-supervised with <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii">Boris Chidlovsky</a> (<em>Naver Labs Europe)</em>.</li>
			<li><a href="https://fr.linkedin.com/in/quentin-possama%C3%AF-425420142">Quentin Possamaï</a> (01/2020-);  Stable and robust control with AI and control-theory (co-supervised with <a href="https://scholar.google.com/citations?user=KOXeslUAAAAJ&hl=en">Madiha Nadri</a> (<em>LAGEPP/Univ Lyon 1)</em> and <a href="https://sites.google.com/site/laurentbako">Laurent Bako</a> (<em>AMPERE / Ecole Centrale de Lyon</em>).</li>
      		<li><a href="https://theo-jaunet.github.io/">Théo Jaunet</a> (10/2018-);  Transparency and Explainability of Machine Learning (co-supervised with <a href="https://romain.vuillemot.net/">Romain Vuillemot</a> (<em>LIRIS/Ecole Centrale de Lyon</em>)).</li>      		
      		<li><a href="https://edbeeching.github.io">Edward Beeching</a> (10/2018-);  Large-scale automatic learning of autonomous agent behavior with structured deep reinforcement learning (co-supervised with <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a> and <a href="http://perso.citi-lab.fr/jdibangoy/#/">Jilles Dibangoye</a> (both <em>INRIA/CITI/INSA-Lyon</em>)).</li>
    	</ul>

    	<!-- <h4>MSc Students</h4> -->

    	<h4>Engineer</h4>
		<ul class="sparselist">
			<li><a href="https://twitter.com/_wgw101">Guillaume Bono</a> (11/2020-).</li>
		
  </div>

    <h3>Former students (alumni)</h3>
    <div style="margin-left: 1em;">

    <h4>Former PhD Students</h4>
    <ul class="sparselist">
    		<li><a href="https://corentinkervadec.github.io">Dr. Corentin Kervadec</a> (10/2018-12/2021);  Vision and Language for Scene Comprehension (co-supervised with <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl=en">Moez Baccouche</a> and <a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&hl=en">Grigory Antipov</a> (both <em>Orange Labs R&D</em>)).</li>
    		<li><a href="https://fabienbaradel.github.io">Dr. Fabien Baradel</a> (10/2016-06/2020); Structured Deep Learning for Video Analysis (co-supervised with <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a> (<em>LI/INSA Val de Loire</em>)); <a href="papers/phdthesis-baradel2020.pdf">[pdf-thesis]</a><a href="http://www.afrif.asso.fr/?p=737">[thesis-prize]</a>; Current position: research scientist at Naver Labs Europe.</li>
			<li><a href="https://perso.liris.cnrs.fr/quentin.debard">Dr. Quentin Debard</a> (12/2016-05/2020); Learning to collaboratively interact with big touch tables  (co-supervised with <a href="http://asi.insa-rouen.fr/enseignants/~scanu/">Stéphane Canu</a> (<em>LITIS/INSA Rouen</em>)).</li>
			<li><a href="https://liris.cnrs.fr/membres?idn=bmoysset">Dr. Bastien Moysset</a> (10/2014-05/2018); Document analysis by deep learning (co-supervised with <a href="https://www.linkedin.com/in/christopher-kermorvant-87158b2">Christopher Kermorvant</a> (<em><a href="http://www.a2ia.com/en">A2IA</a>/<a href="http://www.teklia.com">Teklia</a></em>)); <a href="papers/phdthesis-moysset2018.pdf">[pdf-thesis]</a>.</li>

			<li><a href="http://liris.cnrs.fr/membres?idn=dfourure">Dr. Damien Fourure</a> (10/2014-12/2017); Learning deep representations of videos (co-supervised with <a href="http://portail.univ-st-etienne.fr/bienvenue/utilitaires/m-tremeau-alain-1543.kjsp">Alain Tremeau</a>, <a href="http://home.heeere.com/">Rémi Emonet</a>, <a href="http://perso.univ-st-etienne.fr/frel9915/">Elisa Fromont</a>, <a href="http://perso.univ-st-etienne.fr/muda8804/">Damien Muselet</a> (all <em>LHC,Saint-Etienne</em>)); <a href="papers/phdthesis-fourure2017.pdf">[pdf-thesis]</a>.</li>

            <li><a href="https://liris.cnrs.fr/membres?idn=edogan">Dr. Emre Dogan</a> (01/2013-07/2017); Joint recognition of human activities by multiple robots (co-supervised with <a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> (<em>LIRIS/INSA-Lyon</em>) and <a href="http://www.goneneren.com/">Gönen Eren</a> (<em>Galatasaray University, Istanbul</em>)); <a href="papers/phdthesis-dogan2017.pdf">[pdf-thesis]</a></li>

			<li><a href="https://nneverova.github.io/">Dr. Natalia Neverova</a> (10/2012-04/2016); Deep Learning for Human Motion Analysis (co-supervised with <a href="http://www.uoguelph.ca/~gwtaylor/">Graham W. Taylor</a> (<em> University of Guelph, Canada</em>)); <a href="papers/phdthesis-neverova2016.pdf">[pdf-thesis]</a>; [<a href="http://www.gdr-isis.fr/news/4602/121/Prix-de-these-ISIS-GRETSI-Club-EEA.html">thesis prize</a>]; current position: research lead at <a href="https://research.facebook.com/ai/">Facebook AI Research</a>.</li>
			<li><a href="http://www.gipsa-lab.grenoble-inp.fr/page_pro.php?vid=1752">Dr. Alaeddine Mihoub</a> (1.10.2012-8.10.2015); Attention et communication homme-robot dans des tâches de co-manipulation
 (co-supervised with <a href="https://www.gipsa-lab.grenoble-inp.fr/~gerard.bailly/">Gerard Bailly</a> (<em>Gipsalab, Grenoble</em>)); <a href="papers/phdthesis-mihoub2015.pdf">[pdf-thesis]</a>; current position: assistant professor - college of business and economics at Qassim University.</li>
		    <li><a href="http://liris.cnrs.fr/membres?idn=mjiu">Dr. Jiu Mingyuan</a> (01.10.2010-03.04.2014); Spatial information and end-to-end training for visual recognition (co-supervised with <a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> (<em>LIRIS/INSA-Lyon</em>)); <a href="http://liris.cnrs.fr/Documents/Liris-7054.pdf">[pdf-thesis]</a>; current position: assistant professor at Zhengzhou University.</li>
			<li><a href="http://www.busim.ee.boun.edu.tr/~oya/">Dr. Oya Celiktutan</a> (01.01.2011-06.09.2013); Action recognition in videos (co-supervised with <a href="http://www.ee.boun.edu.tr/busim/B%C3%BClentSankur/tabid/956/language/en-US/Default.aspx">Bülent Sankur</a> (<em>Bogazici University, Istanbul</em>)); current position: lecturer in engineering (robotics), Kings College, London, UK.</li>
			<li><a href="https://scholar.google.com/citations?user=olfpe-kAAAAJ&hl=en">Dr. Moez Baccouche</a> (01.10.2009-15.07.2012);
			Video indexation taking into account human behavior (co-supervised with Frank Mamalet (<em>Orange Labs</em>), Christophe Garcia and <a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> (both </em>LIRIS/INSA-Lyon</em>)); <a href="http://liris.cnrs.fr/Documents/Liris-6240.pdf">[pdf-thesis]</a>; current position: researcher at Orange Labs R&D.</li>
			<li><a href="http://liris.cnrs.fr/vincent.vidal">Dr. Vincent Vidal</a> (01.10.2008-09.12.2011); Remeshing and mesh simplification with probabilistic graphical models (co-supervised with Florent Dupont (<em>LIRIS/University Lyon 1</em>)); current position: assistant professor (MCF) at LIRIS laboratory; <a href="http://liris.cnrs.fr/Documents/Liris-5345.pdf">[pdf-thesis]</a></li>
			<li><a href="http://liris.cnrs.fr/anh-phuong.ta">Dr. Anh-Phuong TA</a> (01.02.2008-26.11.2010);
			Inexact graph matching and it's application to objet detection and action recognition (co-supervised with <a href="http://liris.cnrs.fr/guillaume.lavoue/">Guillaume Lavoué</a> and <a href="http://liris.cnrs.fr/atilla.baskurt">Atilla Baskurt</a> (both <em>LIRIS/INSA-Lyon</em>)); <a href="papers/phdthesis-ta2010.pdf">[pdf-thesis]</a></li>
		</ul>		

		<h4>Contractual researcher</h4>
    <ul class="sparselist">
        <li><a href="">Tom Gillooly</a> (12/2018-); Learning Robot navigation through language (co-supervised with <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a> and <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>).</li>
    </ul>

    <h4>Former post-doctoral Students</h4>
  		<ul class="sparselist">
			<li><a href="https://sites.google.com/site/drkhanrizwan17/">Dr. Rizwan Khan</a></span> (12/2013-11/2014); Semantic labelling of HD videos taken from occulometric eye-glasses
			(co-supervised with <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>)</li>
		</ul>
  </div>

		<h4>Ongoing research projects</h4>
		<ul class="sparselist">
			<li>
				I hold the AI Chair in research and teaching <a href="https://chriswolfvision.github.io/remember/">"REMEMBER - Learning Reasoning, Memory and Behavior"</a> at INSA-Lyon, co-financed by ANR, Naver Labs Europe and INSA-Lyon. Team members are  <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>, <a href="http://perso.citi-lab.fr/jdibangoy/">Jilles Dibangoye</a>, <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetitia Matignon</a> and <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii</a>.
			</li>
			<li>
				I am the project leader of ANR project "DeLiCo"</a> (stable and robust control in complex environments combining machine learning and control theory), involving 4 partners:
                <ul style="list-style-type: square;">
                    <li> LIRIS/INSA-Lyon (<a href="index.html">Christian Wolf</a>, <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetitia Matignon</a>) </li>
                    <li> CITI/INSA-Lyon (<a href="http://perso.citi-lab.fr/jdibangoy/#/">Jilles Dibangoye</a>, <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a>, <a href="https://ievred.github.io/">Ievgen Redko</a> (LHC Laboratory)) </li>
                    <li> LAGEPP/Lyon 1 University (<a href="https://lagepp.univ-lyon1.fr/membre/nadri-madiha/">Madiha Nadri</a>, <a href="https://sites.google.com/site/vincentandrieu/">Vincent Andrieu</a>, <a href="https://sites.google.com/site/astolfidaniele/">Daniele Astolfi</a> <a href="https://www.ec-lyon.fr/contacts/laurent-bako">Laurent Bako</a>(AMPERE Laboratory), <a href="https://www.ec-lyon.fr/contacts/giacomo-casadei">Giacomo Casadei</a>(AMPERE Laboratory)) </li>
                    <li> Onera (<a href="https://www.onera.fr/en/staff/sylvain-bertrand">Sylvain Bertrand</a>, <a href="http://julien.marzat.free.fr/">Julien Marzat</a>, <a href="https://scholar.google.com/citations?user=xg8ezOQAAAAJ&hl=en">Hélène Piet-Lahanier</a>) </li>
                </ul>
            </li>
		</ul>

		<h4>Completed research projects</h4>
		<ul class="sparselist">
		<li>
				I was the French leader for French-Canadian <a href="https://projet.liris.cnrs.fr/deepvis/">ANR/NSERC project "Deepvision"</a>, involving 4 partners:
                <ul style="list-style-type: square;">
                    <li> LIRIS/INSA-Lyon (<a href="index.html">Christian Wolf</a> - French Leader; <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille//">Julien Mille</a>) </li>
                    <li> LIP6/UPMC (<a href="http://webia.lip6.fr/~cord">Matthieu Cord</a>, <a href="http://webia.lip6.fr/~thomen/">Nicolas Thome</a>) </li>
                    <li> University of Guelph (<a href="http://www.uoguelph.ca/~gwtaylor/">Graham W. Taylor</a>; Canadian leader)</li>
                    <li> Simon Fraser University (<a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>) </li>
                </ul>
            </li>
      	<li>
				<a href="https://solstice.univ-st-etienne.fr/">ANR blanc "Solstice"</a>; graphes and structured models for computer vision (2014 - 2018).
			</li>
            <li>
                <a href="http://www.minalogic.com/TPL_CODE/TPL_PROJET/PAR_TPL_IDENTIFIANT/3276/15-annuaire-innovations-technologiques-nanotechnologie-systeme-embarque.htm#.VUhr7Gb8-n8">Investissements d'Avenir: "INTERABOT"</a>; gesture and object recognition in mobile robotics environment (5/2012-5/2016; leader of "INSA" partner).
            </li>
			<li>
				<a href="http://imu.universite-lyon.fr/bilan-appels-internes-de-recherche-2013/projet-riviere-209479.kjsp">Labex IMU "RIVIERE"</a>; semantic labeling of HD videos acquire from portable eyetracking glasses  (2013 - 2015).
			</li>
			<li>
				BQR INSA "CROME"; Multi-view multi-robot scene understanding and fleet coordination (2014 - 2016).
			</li>
			<li>
				<a href="">ANR Canada</a> (2007-2010) - Comportements Anormaux : Analyse, Détection, Alerte; ; leader of "INSA" partner.
			</li>
			<li>
				<a href="http://www-rech.telecom-lille1.eu/madras">ANR Madras</a> (2008-2011) - 3D Models And Dynamic models Representation And Segmentation
			</li>
			<li>
				<a href="http://labh-curien.univ-st-etienne.fr/wiki-sattic/index.php/Main_Page">ANR Sattic</a> (2007-2011) - Strings and Trees for Thumbnail Images Classification.
			</li>
		</ul>

		<h4>Responsibilities (National, French)</h4>
        <ul class="sparselist" style="margin-bottom: 0.5ex">
            <li>2017-now: Membre du comité de direction du <a href="http://gdr-isis.fr">GDR ISIS (Information Signal Image Vision)</a> du CNRS</li>
            <li>2016-now: Membre du comité d'animation du <a href="http://www.gdria.fr/">GDR Intelligence Artificielle</a> du CNRS</li>
            <li>2018-now: Co-animation du GT "Apprentissage et Robotique" du <a href="https://www.gdr-robotique.org/">GDR Robotique</a> du CNRS</li>
            <li>2019/2020: Membre du comité d'évaluation CES 23 de l'ANR</a> (Intelligence artificielle)</li>
            <li>2016-2018: Membre du comité d'évaluation <a href="http://www.agence-nationale-recherche.fr/fileadmin/comites/2017/CES-aapg2017.pdf">CES 33 de l'ANR</a> (Robotique et Interactions, 2 ans)</li>
            <li>2021-now: Membre elu du conseil du <a href="http://liris.cnrs.fr/">laboratoire LIRIS</a></li>
            <li>2017-2020: Membre elu du conseil de la <a href="http://fil.cnrs.fr/">Fédération Informatique de Lyon</a></li>
            <li>2009-2017: Responsable de l'enseignement de l'informatique en deuxième année au premier cycle de l'INSA de Lyon</li>
        </ul>

        <h4>Responsibilities (International)</h4>
        <ul>
        	<li>Associate editor for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE-Transactions on Pattern Analysis and Machine Intelligence (PAMI)</a></li>
        	<li>List of <a href="research/jurys.html">PhD and HDR defense committees</a></li>
        </ul>

    		<h4>Recent committee participations (area chair, reviewer)</h4>
    		<ul class="sparselist" style="margin-bottom: 0.5ex">    			
    			<li> Reviewer for <a href="https://cvpr2022.thecvf.com">CVPR 2022</a>.</li>
    			<li> Area chair for <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a>, <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>, <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="https://nips.cc/Conferences/2021">NeurIPS 2021</a>.</li>
    			<li> Reviewer for <a href="http://cvpr2021.thecvf.com">CVPR 2021</a>, <a href="https://www.bmvc2021.com/">BMVC 2021</a>.</li>    			
    			<li> Area chair for <a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a> and
    			<a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</li>
    			<li> Reviewer for
                    <a href="https://openreview.net/group?id=ICLR.cc/2020/Conference">ICLR 2020</a>,                    
                    <a href="https://openreview.net/group?id=thecvf.com/ECCV/2020/Conference">ECCV 2020</a>,
                    <a href="https://bmvc2020.github.io/">BMVC 2020</a> (<a href="https://bmvc2020.github.io/people/reviewers/">outstanding reviewer</a>).
                </li>
    			<li> Area chair for FG 2019</li>
          		<li> Reviewer for
                    <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a> (<a href="http://cvpr2019.thecvf.com/files/CVPR_2019_Program_Guide.pdf">outstanding reviewer</a>),
                    <a href="https://iclr.cc/Conferences/2019">ICLR 2019</a>,
                    <a href="https://icml.cc/Conferences/2019">ICML 2019</a>,
                    <a href="https://nips.cc/">NeurIPS 2019</a>,
                    <a href="https://bmvc2019.org/">BMVC 2019</a>,
                    <a href="http://www.ecmlpkdd2019.org/">ECML-PKDD 2019</a>.
          		</li>
    			<li> PC member for <a href="http://www.ijcai-18.org/">IJCAI 2018</a>,
    			<a href="https://project.inria.fr/humans2018/">CVPR 2018 Workshop on Human Pose, Motion, Activities and Shape in 3D</a>,
    			<a href="https://sites.google.com/view/hands2018/">ECCV 2018 Workshop on Hands in Action</a>,
    			<a href="https://rfiap2018.ign.fr">RFIAP 2018</a>.</li>
    			</li>
    			<li> Reviewer for
                    <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a>,
                    <a href="https://nips.cc/Conferences/2018">NIPS 2018</a>,
                    <a href="http://www.iclr.cc/doku.php?id=ICLR2018:main&redirect=1">ICLR 2018</a>,
                    <a href="https://icml.cc/">ICML 2018</a>,
                    <a href="https://www.ijcai-18.org/">IJCAI 2018</a>,
                    <a href="http://bmvc2018.org/">BMVC 2018</a>
                </li>    			
    			<li>PC member of <a href="https://bmvc2017.london/">BMVC 2017</a>,
                <a href="http://icvl.ee.ic.ac.uk/hands17/dates/">ICCV 2017 Workshop on hands in action</a>,
    			<a href="http://u-pat.org/ICDAR2017/">ICDAR 2017</a>, <a href="http://www.cvl.isy.liu.se/CAIP2017.html">CAIP 2017</a> and <a href="http://caip.eu.org/caip2015">CAIP 2015</a></li>    			
    			<li> Reviewer for
                    <a href="http://cvpr2017.thecvf.com">CVPR 2017</a>,
                    <a href="http://iccv2017.thecvf.com/">ICCV 2017</a>,
                    <a href="https://nips.cc/Conferences/2016">NIPS 2017</a>,
    				<a href="http://www.iclr.cc/doku.php?id=ICLR2017:main&redirect=1">ICLR 2017</a>
                </li>
    			<li>PC member of <a href="http://www-rech.telecom-lille.fr/uha3ds2016">UHA3DS’16"</a></li>
    			<li> Reviewer for
    				<a href="http://www.pamitc.org/cvpr16">CVPR 2016</a>
                    (<a href="http://cvpr2016.thecvf.com/">outstanding reviewer</a>),
                    <a href="http://www.eccv2016.org">ECCV 2016</a>,
    				<a href="https://nips.cc/Conferences/2016">NIPS 2016</a>
                </li>
                <li> Reviewer for
    				<a href="http://www.pamitc.org/cvpr15">CVPR 2015</a> and
    				<a href="http://pamitc.org/iccv15/">ICCV 2015</a></li>
    			<li>PC member of <a href="http://www.deep-vision.net">CVPR 2016, 2015 and 2014 - Deep Vision Workshop</a></li>
    			<li>PC member of <a href="http://gesture.chalearn.org">CVPR 2015 ChaLearn workshop on Looking at people</a>,
    			 <a href="http://www-rech.telecom-lille.fr/uha3ds2015/">FG 2015 - Human activities workshop</a>.</li>
    		    <li> Area chair for <a href="http://www.avss2014.org">AVSS 2014</a> and <a href="http://www.avss2013.org">AVSS 2013</a></li>
    		    <li>PC member of <a href="http://icmi.acm.org/2014">ICMI 2014</a></li>
    		</ul>
    		    		

		<h4>Miscellaneous:</h4>
		<ul class="sparselist">
			<li>My <a href="contribution/index.html">most important contribution</a></li>
			<li><a href="motivation/index.html">Passion</a> (other then ML/AI/CV)</li>			
			<li>A short list of some <a href="journalif.html">journal impact factors</a></li>			
			<li><a href="computers/index.html">My personal computing history</a></li>
			
			<li><a href="blue_tiger.html">Blue tiger</a></li>
			<li>My Erdös-Number is 4 (<a href="https://mathscinet.ams.org/mathscinet/freeTools.html?version=2">compute yours</a>), through <a href="https://scholar.google.fr/citations?user=z9FUD8QAAAAJ&hl=fr">Bülent Sankur</a>:<br>
				<div style="margin-left: 4em; margin-top: 1em; font-style: italic;">
				Christian Wolf = 4<br>
				Bülent Sankur = 3<br>
				C. Sinan Güntürk = 2<br>
				Melvyn B. Nathanson = 1<br>
				Paul Erdös = 0
				<div>
			</li>
		</ul>



		<h4>Contact:</h4>

			<p> Email: christian.wolf (at) insa-lyon.fr<br>
			Tel: +33 4 72 43 63 08            
			</p>

			<p>
            Christian Wolf<br>
            INSA de Lyon<br>
            Laboratoire LIRIS<br>
            Batiment Blaise Pascal<br>
            20 av. Albert Einstein<br>
            69621 Villeurbanne            
			</p>

			<p style="border-top: 1px solid #bbbbbb;">When your only tool is a hammer, every problem looks like a nail</p>

	</div>

	<div class="breaker">
	</div>

</p>

<a href="contribution/yosemite.html">
<img style="max-width: 100%; height: auto; width: auto\9;" src="miscphotos/chris_banner_usa2014_footer.jpg">
</a>



</body>
</html>
